# 02.爬虫类

`Spider`是定义某个站点或某一组站点抓取的类，其中包括如何执行抓取以及如何提取数据的功能，`Spider`的抓取周期为，首先定义初始需要访问的`url`，引擎将把初始`url`序列化为初始请求并发送。其次获取来自引擎返回的对应初始`url`的响应结果，解析响应获取新的可迭代的数据结构或者构建新的可迭代的请求对象返回给引擎，引擎将把新的请求发送给下载器(`download`)，把解析后的数据发送给管道(`pipelines`)。

## 一、爬虫基类Spider

`scrapy.Spider`是`scrapy`的爬虫基类，其中封装了一系列常用的属性与方法。

### 1.常用属性

| 基本对象属性      | 说明                                                         |
| ----------------- | ------------------------------------------------------------ |
| `name`            | 定义蜘蛛名称的字符串，`spider.name`是`scrapy`定位和实例化的方式，因此它必须是唯一的，建议以域名的形式定义`name`属性。 |
| `allowed_domain`  | 包含允许此蜘蛛爬行的域的字符串的可选列表。当`setting`中 `OffsiteMiddleware=True`时，不属于以上列表中的请求不被允许。 |
| `start_urls`      | 引擎默认用以构建请求的`url`列表，后续请求将根据对应的响应结果生成。 |
| `custom_settings` | 类属性，对框架中`setting`的设置进行覆盖，假设想要覆盖`setting`中的`headers`内容，则需要将`DEFAULT_REQUEST_HEADERS`内容写入`custom_settings`，具体内置设置可以参考[内置设置参考](https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-ref)。 |
| `crawler`         | 在`spider`类初始化后由类方法`from_crawler() `设置，它是框架中` Crawler`实例的拷贝引用，常用于配置爬虫[组件api](https://docs.scrapy.org/en/latest/topics/api.html#topics-api-crawler)，（例如扩展、中间件、信号管理器等），暂时了解，详细使用将在后续组件中介绍。 |
| `settings`        | 它是框架中`setting`实例的拷贝引用，参见[设置API](https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings)了解更多。 |
| `logger`          | 运行`Spider`的日志记录器，它是框架中`Python logger`实例以`name`为名的拷贝引用，可以用于发送日志消息，参见[日志API](https://docs.scrapy.org/en/latest/topics/logging.html#topics-logging-from-spiders)了解更多。 |
| `state`           | 一个用以保存`batches`之间爬行状态的字典，参考[batches 之间的状态保持](https://docs.scrapy.org/en/latest/topics/jobs.html#topics-keeping-persistent-state-between-batches)以了解更多。 |

### 2.常用方法

#### （1）`start_requests()`方法

`start_requests`是`scrapy.Spider`父类中的方法，在没有重写的情况下，`Scrapy`框架默认自动调用一次该方法，用于提取`start_urls`列表中的地址并构建请求对象`Request(url, dont_filter=True) `，如果需要自定义最初请求，应该重写该方法，注意要保证该方法的返回值必须是可迭代的。

> 当第一次请求不是一般的基于`start_urls`列表生成的`get`请求的情况下，都应该重写该函数，如在请求发送前需要获取`cookie`、首次请求为`Post`请求...

#### （2）`parse(response)`方法

当请求对象未指定回调时用来处理响应的默认回调函数， `parse` 方法负责处理响应，并返回 爬取 的数据或更多的`URL`/请求对象。`parse()`函数及任何其他请求回调函数中一般使用`yield`而不是`return`返回数据，返回的对象必须只能是`BaseItem`,`Request`, `dict`和`None`。

`Spider`还包含一个`_parse(self, response, **kwargs)`方法，用于间接调用`parse()`。

> `parse`函数必须被重写，否则将会报错。

#### （3）`log`方法

其调用等价于`self.logger.log(level, message, **kw)`，可以方便的在`Spider`内部进行日志输出，保持向后兼容性。

```python
self.log(level=10, '这时一句level=10级别的log')
```

`logger`对象通过`property`属性附加在`Spider`中作为实例属性使用。

```python
@property
def logger(self):
    logger = logging.getLogger(self.name)
    return logging.LoggerAdapter(logger, {"spider": self})
```

#### （4）静态方法`closed(reason)`

蜘蛛关闭时调用，此方法为 `spider_closed` 信号。

#### （5）类方法`update_settings`

将`custom_settings`内的字段覆盖到`settings`中。

#### （6）`_set_crawler(self, crawler: Crawler)`方法

用于为当前的`Spider`绑定一个`crawler`对象，详细使用后续进一步了解，参阅核心`API`。

#### （7）类方法`from_crawler`

用来创建蜘蛛的类方法，一般不需要直接重写，其默认实现了以`args`和`kwargs`作为参数的`__init__()`方法，该方法将设置`crawler`和`settings`新实例中的属性。

```python
@classmethod
def from_crawler(cls, crawler, *args, **kwargs):
    spider = cls(*args, **kwargs)
    spider._set_crawler(crawler)
    return spider

def _set_crawler(self, crawler: Crawler):
    self.crawler = crawler
    self.settings = crawler.settings
    crawler.signals.connect(self.close, signals.spider_closed)
```

#### （8）类方法`handles_request`

用于确定某个请求的`url`是否符合当前`Spider`的`domain`设置。

### 3.Spider参数

`Spider`可以为为其定制化` __init__(self, category, *args, **kwargs)`

```python
def __init__(self, category=None, *args, **kwargs):
    super(MySpider, self).__init__(*args, **kwargs)
    self.start_urls = [f'http://www.example.com/categories/{category}']
    # ...
```

这些参数可以在 命令行 或 代码直接调用中添加：

```shell
# 定义配置URL
scrapy crawl myspider -a category=electronics
# 配置其他功能，设置 HttpAuthMiddleware 或用户代理 UserAgentMiddleware 
scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot
```

```shell
# 实现基本的外部调用
process = CrawlerProcess()
process.crawl(MySpider, category="electronics")
```

> 蜘蛛参数也可以通过`scrapyD`传递 `schedule.json` 应用程序编程接口，详见 [Scrapyd documentation](https://scrapyd.readthedocs.io/en/latest/) 。

## 二、`CrawlSpider`类

`CrawlSpider`是最常用的爬行常规网站的`Spider`，它通过定义一组规则为跟踪链接提供了一种方便的机制，除了从`spider`基类继承的属性，这个类支持一个新属性`rules`，并重写了一个新方法`parse_start_url(response, **kwargs)`。

> `crawlspider`爬虫可以按照规则自动获取连接。

### 1.创建命令

```shell
scrapy genspider -t crawl job 163.com
```

执行命令后` spider`中默认生成的内容如下：

```python
class JobSpider(CrawlSpider):
    name = 'job'
    allowed_domains = ['163.com']
    start_urls = ['https://hr.163.com/position/list.do']

    rules = (
        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),
    )

    def parse_item(self, response):
        i = {}
        #i['domain_id'] = response.xpath('//input[@id="sid"]/@value').extract()
        #i['name'] = response.xpath('//div[@id="name"]').extract()
        #i['description'] = response.xpath('//div[@id="description"]').extract()
        return i
```

在`crawlspider`爬虫中，没有`parse`函数，并增加类属性`rules`。

### 2.源码分析

规则爬虫首先需要定义爬行规则`Rule`对象，关键就是定义参数链接提取器和默认解析函数`parse_item`。

这时不做任何操作，直接运行爬虫，`CrawlSpider`将依次向`start_urls`中的每个`url`发送请求，获取的响应在引擎处会首先调用`_requests_to_follow`，按照用户定义规则从相应中提取链接。可以看到该函数遍历我们定义的所有`rule`，依次执行其`link_extractor.extract_links`，只要通过任意一个'规则'，即表示合法，将提取结果放入`link`中，并执行`process_links`预处理链接，将连接存入集合中构造请求并返回，实际上引擎将获取的可迭代请求序列存在了爬虫类属性`self._follow_links`中。

```python
def _requests_to_follow(self, response):
    """从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回"""
    if not isinstance(response, HtmlResponse):
        return
    seen = set()
    #抽取之内的所有链接，只要通过任意一个'规则'，即表示合法
    for n, rule in enumerate(self._rules):
        links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]
        #使用用户指定的process_links处理每个连接
        if links and rule.process_links:
            links = rule.process_links(links)
        #将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()
        for link in links:
            seen.add(link)
            #构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数
            r = Request(url=link.url, callback=self._response_downloaded)
            r.meta.update(rule=n, link_text=link.text)
            #对每个Request调用process_request()函数。该函数默认为indentify，即不做任何处理，直接返回该Request.
            yield rule.process_request(r)
```

提取规则后，引擎后续使用默认的`_parse`再次处理返回`response`，该函数将默认调用`_parse_response`并将回调函数设置为`parse_start_url`，并设置自动跟进。`_parse_response`中首先尝试使用`callback`解析出`item`或`request`，其次如果设置了自动跟进，则依次从类属性中读取`request`返回给引擎。

```python
def _parse(self, response, **kwargs):
    return self._parse_response(
        response=response,
        callback=self.parse_start_url,
        cb_kwargs=kwargs,
        follow=True
    )
    
async def _parse_response(self, response, callback, cb_kwargs, follow=True):
    """解析response对象，会用callback解析处理他，并返回request或Item对象"""
    if callback:
        cb_res = callback(response, **cb_kwargs) or ()
        if isinstance(cb_res, AsyncIterable):
            cb_res = await collect_asyncgen(cb_res)
        elif isinstance(cb_res, Awaitable):
            cb_res = await cb_res
        cb_res = self.process_results(response, cb_res)
        for request_or_item in iterate_spider_output(cb_res):
            yield request_or_item
	# 如果自动跟进，则
    if follow and self._follow_links:
        for request_or_item in self._requests_to_follow(response):
            yield request_or_item

def parse_start_url(self, response):
    """对于`Spider`根据其属性`start_urls`生成的每个响应都会调用该方法，用于解析响应并返回`None`、字典、`item`、`Request`或者包含任一的迭代器，如果需要解析 start_urls 需要重写该函数。"""
    return []
```

除此之外，该爬虫还定义了`process_results`、`_build_request`、`_compile_rules`等方法，用于构建请求、规则等。

```python
class CrawlSpider(Spider):
    rules: Sequence[Rule] = ()

    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)
        self._compile_rules()

    def process_results(self, response: Response, results: list):
        return results

    def _build_request(self, rule_index, link):
        return Request(
            url=link.url,
            callback=self._callback,
            errback=self._errback,
            meta=dict(rule=rule_index, link_text=link.text),
        )

    def _callback(self, response, **cb_kwargs):
        rule = self._rules[response.meta["rule"]]
        return self._parse_response(
            response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow
        )

    def _errback(self, failure):
        rule = self._rules[failure.request.meta["rule"]]
        return self._handle_failure(failure, rule.errback)

    def _handle_failure(self, failure, errback):
        if errback:
            results = errback(failure) or ()
            for request_or_item in iterate_spider_output(results):
                yield request_or_item

    def _compile_rules(self):
        self._rules = []
        for rule in self.rules:
            self._rules.append(copy.copy(rule))
            self._rules[-1]._compile(self)

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider._follow_links = crawler.settings.getbool(
            "CRAWLSPIDER_FOLLOW_LINKS", True
        )
        return spider
```

### 3.扩展内容

#### （1）爬行规则

`Rule `定义对网站进行爬行的特定行为，规则对象如下所述，如果多个规则与同一链接匹配，则根据属性定义顺序使用第一个规则。

```python
class scrapy.spiders.Rule(
    link_extractor=None,
    callback=None, 
    cb_kwargs=None, 
    follow=None, 
    process_links=None, 
    process_request=None, 
    errback=None
):
```

| 参数名            | 说明                                                         |
| ----------------- | ------------------------------------------------------------ |
| `link_extractor`  | 直接传递一个 `Link Extractor` 对象，该对象定义如何从每个网页面提取链接，每个生成的链接将被引擎用于生成 `Request` 对象，默认将使用不带参数创建的默认链接提取器，提取所有链接。 |
| `callback`        | 可以传递一个字符串或回调函数名，将后续的`response`传递给该回调函数（回调函数应该类似于`parse`接受`response`参数）， |
| `cb_kwargs`       | 包含传递给回调函数的关键字参数`dict`。                       |
| `follow`          | 布尔值，指定是否对每个响应返回的结果继续应用规则。           |
| `process_links`   | 传入一个字符串（自于其他`spider`中定义的方法）或函数名，该函数用于对从`link_extractor`中提取的链接进行过滤或者预处理。 |
| `process_request` | 传入一个字符串（自于其他`spider`中定义的方法）或函数名，该函数必须以`request`和`response`作为参数，返回`None`或经过处理后的`request`对象，该请求从`response`发出。 |
| `errback`         | 在处理规则生成的请求时引发任何异常时都会调用该回调函数。     |

#### （2）链接提取器

链接提取器是从响应中提取链接的对象，主要用于 借助`Rule`实例用于`CrawlSpider`，比较常用的链接提取器是`LxmlLinkExtractor`，其调用有点复杂，为方便起见，可以使用如下两种调用。

```python
from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor
from scrapy.linkextractors import LinkExtractor
```

`LXMLlinkextractor`是带有便捷过滤选项的链接提取程序，其基于`lxml`的`HTMLParser`实现。

```python
class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(
    allow=(),  # 可传入正则表达式str或正则表达式list，匹配符合规则的链接，如果为空则，默认匹配所有链接。
    deny=(),   # 可传入正则表达式str或正则表达式list，排除不符合规则的链接，其优先级大于allow，如果为空则不会排除任何链接。
    allow_domains=(),   # 接收一个域名或一个域名列表，提取到指定域的链接，默认为全部。
    deny_domains=(),  # 和allow_doains相反，拒绝一个域名或一个域名列表，提取除被deny掉的所有匹配url。
    deny_extensions=None,  # 提取链接时应忽略的包含扩展名的字符串的单个值或列表。如果未给出，则默认为 7z ， 7zip ， apk ， bz2 ， cdr ， dmg ， ico ， iso ， tar ， tar.gz ， webm 和 xz
    restrict_xpaths=(),  #  一个xpath或xpath列表，定义应该从响应中提取链接的区域，只扫描由这些xpath选择的文本中的链接，未给定时，默认提取所有链接。
    restrict_css=(),  #  一个CSS选择器或选择器列表，行为类似于restrict_xpaths。
    restrict_text(),  # 一个正则表达式或正则列表，行为类似于restrict_xpaths。
    tags=('a', 'area'),   # 接收一个标签（字符串）或一个标签列表，仅提取指定标签内的链接，默认为('a', 'area')。
    attrs=('href',),  # 接收一个属性（字符串）或者一个属性列表，提取指定的属性内（且属于指定标签）的链接。默认为 ('href',)
    canonicalize=False,  # 规范化每个提取的URL（使用w3lib.url.canonicalize_url），默认为 False ，规范化URL用于重复检查，规范化后的URL可能与原始URL的请求获得的响应不同，保持默认链接更为可靠。
    unique=True, # 是否对提取的链接应用重复筛选。
    process_value=None,  # 一个接受从扫描的标记和属性中提取值的函数，修改该值并返回新值，返回None时完全忽略链接。
    # 例如：从链接x = <a href=“javascript:gotopage（'../other/page.html'）；return false“>link text.<a>中提取链接
    # process_value=lambda x :  m=re.search（"javascript:gotopage（'（.*？）'"，x），若m为真将返回 m.group（1）。
    strip=True  # 是否从提取的属性中删除空白，根据HTML5标准， href 属性 <a> ， <area> 还有许多其他元素，src 属性 <img> ， <iframe> 元素等属性内不允许存在空白，默认删除空白。
):pass

# 使用案例
if __name__ == '__main__':
	from scrapy.linkextractor import LinkExtractor

	def parse(self, response):
	    # 创建链接提取器
	    link = LinkExtractor(restrict_xpaths='//ul[@class="note-list"]/li')
	    # 传入response对象，返回link对象
	    links = link.extract_links(response)
	    if links:
	        for link_one in links:
	            print(link_one)
```

`LxmlLinkExtractor`包含一个非私有方法`extract_links(response)`，该方法可以传入一个`response`对象返回`Link`列表。

#### （3）`Link`链接对象

`Link`对象表示`LinkExtractor`提取的链接实例，可以将其理解为一个字典包含`url`、`text`、`fragment`、`nofollow`对象。

```python
class scrapy.link.Link(
    url,   # 在定位标记中链接到的绝对url
    text='',  # 锚定标记中的文本。
    fragment='',   # url中#符号后面的部分，fragment。
    nofollow=False  # 是否存在nofollow指示。
):pass

# test
from scrapy.link import Link

# <a href="https://example.com/nofollow.html#foo" rel="nofollow">Dont follow this one</a>
s = Link(url='https://example.com/nofollow.html#foo',text='Dont follow this one',fragment='foo',nofollow=True)
print(s)  
# Link(url='https://example.com/nofollow.html#foo', text='Dont follow this one', fragment='foo', nofollow=True)
```

### 4.案例-使用`CrawlSpider`类爬取豆瓣电影

这里不直接爬取豆瓣了，选择网站`https://ssr1.scrape.center/`进行模拟爬取。

- 首先创建项目与爬虫

```shell
scrapy startproject movies
scrapy genspider -t crawl movie_spider ssr1.scrape.center
```

- 其次设计需要的字段

这里的字段分别指电影名称、封面、类别、上映时间、剧情简介和评分。

```python
import scrapy

class MoviesItem(scrapy.Item):
    # define the fields for your item here like:
    cover = scrapy.Field()
    categories = scrapy.Field()
    published_at = scrapy.Field()
    drama = scrapy.Field()
    score = scrapy.Field()
```

- 最后撰写爬虫类

首先当前网站第一个网页就是列表页，所以`start_urls`则无需更改，网页首先访问列表首页，需要定义前往详情页的规则和翻页规则。

详情页链接处于提取`class`为`name`的`<a>`节点中，取其`href`属性即可，为了更准确，可以考虑根据其父节点`item`定位，如下通过`restrict_css`限制定位区域，使用默认的`tags=('a', 'area')`和`attrs=('href',)`，限制提取`a`标签的`href`属性。

```python
# <a data-v-7f856186="" href="/detail/1" class="name">
# 	<h2 data-v-7f856186="" class="m-b-sm">霸王别姬 - Farewell My Concubine</h2>
# </a>
# 构建链接提取器类
rules = Rule(LinkExtractor(restrict_css='.item .name'), follow=True, callback=self.parse_item)
```

翻页链接处于如下结构中，同样思路定义链接提取器，这次使用`restrict_xpaths`，但因为这次无需从列表页提取 `Item`，所以无须额外指定 `callback`。

```python
# <ul class="el-pager">
# 	<li class="number active"><a href="/page/1">1</a></li>
# 	...
# </ul>
# <a href="/page/2" class="next">
# 	<button type="button" class="btn-next"><i class="el-icon el-icon-arrow-right"></i></button>
# </a>
rules = Rule(LinkExtractor(restrict_xpaths='//div[@class="el-row"]//a[@class="next"]')
```

开发完毕后大致`spider`如下：

```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from movies.items import MoviesItem


class MovieSpiderSpider(CrawlSpider):
    name = "movie_spider"
    allowed_domains = ["ssr1.scrape.center"]
    start_urls = ["https://ssr1.scrape.center"]

    rules = (
        Rule(LinkExtractor(restrict_css='.item .name'), follow=True, callback="parse_item"),
        Rule(LinkExtractor(restrict_xpaths='//div[@class="el-row"]//a[@class="next"]'), follow=True),
        # 提取所有指定域的链接，但是不能匹配部分域的链接，并且会递归爬取(如果没有定义callback，默认follow=True)
        # Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),
        # 提取匹配'/article/\d+/\d+.html'的链接，并使用parse_item来解析它们下载后的内容，不递归
        # Rule(LinkExtractor(allow=('/article/\d+/\d+\.html', )), callback='parse_item'),
    )

    def parse_item(self, response):

        item = MoviesItem()
        item["name"] = response.css(".item h2::text").get()
        item["categories"] = response.css('.categories button span::text').getall()
        item["cover"] = response.css('.cover::attr(src)').get()
        item["published_at"] = response.css('.info span::text').re_first(r'(\d{4}-\d{2}-\d{2})\s?上映')
        item["score"] = response.xpath("//p[contains(@class, 'score')]/text()").get().strip()
        item["drama"] = response.xpath('//div[@class="drama"]/p/text()').get().strip()
        return item

# scrapy crawl movie_spider
```

> 链接只要满足其中一个`rule`，就会被提取出来，`crawlspider`中不能有以`parse`为名的数据提取方法，传入`callback`的应该是一个字符串。

扩展阅读：[scrapy链接提取器](https://docs.scrapy.org/en/latest/topics/link-extractors.html)

## 三、XMLFeedSpider类

`XMLFeedSpider`是为解析`XML`而设计的，它通过使用特定的节点名对这些提要进行迭代，内置迭代器包括`iternodes` ，`xml`和 `html`三种形式，其中`iternodes`可以迭代部分， `xml` 和 `html` 一次生成整个`DOM`以便解析它。不过当内容比较多的时候推荐使用`iternodes`（默认），可以节省内存提升性能，不需要将整个DOM加载到内存中再解析。`html`迭代器在分析带有错误标记的`XML`时更好，处理XML的时候最好先[Removing namespaces](http://doc.scrapy.org/en/1.0/topics/selectors.html#removing-namespaces)。

```python
class scrapy.spiders.XMLFeedSpider
```

### 1.属性或方法

| 属性或方法                          | 说明                                                         |
| ----------------------------------- | ------------------------------------------------------------ |
| `itertag`                           | 具有要迭代的节点（或元素）名称的字符串。                     |
| `namespaces`                        | 一个 用于定义该文档中可用的名称空间的(`prefix`, `uri`) 元组，这个 `prefix` 和 `uri` 属性自动注册命名空间。 |
| `adapt_response(response)`          | 当`response`从中间件到达时，在`spider`开始解析它之前，立即接收、处理并返回响应。 |
| `parse_node(response, selector)`    | 将为匹配的节点调用此方法，接收响应，并引发 `Selector` 匹配每个节点，`selector`默认我i上面的`itertag`，重写此方法是必需的。此方法是一个类`parse`方法。 |
| `process_results(response, result)` | 这个方法是为`spider`返回的每个结果（项或请求）调用的，它用于在将结果返回到框架核心之前执行所需的任何最后一次处理，例如设置项`id`。它接收结果列表和产生这些结果的响应。它必须返回结果列表（项或请求）。 |

### 2.案例

```python
from scrapy.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = 'xml'
    namespaces = [('atom', 'http://www.w3.org/2005/Atom')]
    allowed_domains = ["github.io"]
    start_urls = [
        "http://www.pycoding.com/atom.xml"
    ]
    iterator = 'xml'  # 缺省的iternodes，貌似对于有namespace的xml不行，默认为'iternodes'
    itertag = 'atom:entry'  # 'item'

    def parse_node(self, response, node):
        self.logger.info('Hi, this is a <%s> node!: %s', self.itertag, ''.join(node.getall()))
        item = BlogItem()
        item['title'] = node.xpath('atom:title/text()')[0].extract()
        item['link'] = node.xpath('atom:link/@href')[0].extract()
        item['id'] = node.xpath('atom:id/text()')[0].extract()
        item['published'] = node.xpath('atom:published/text()')[0].extract()
        item['updated'] = node.xpath('atom:updated/text()')[0].extract()
        self.logger.info('|'.join([item['title'],item['link'],item['id'],item['published']]))
        return item
```

## 四、CSVFeedSpider类

与`xmlFeedSpider`非常相似，只是它迭代行，而不是节点，在每次迭代中被调用的方法是 `parse_row()`。

### 1.属性方法

| 属性方法                   | 说明                                                         |
| -------------------------- | ------------------------------------------------------------ |
| `delimiter`                | 带有`csv`文件中每个字段分隔符的字符串默认为 `,`'（逗号）。   |
| `quotechar`                | 带有`csv`文件中每个字段的外壳字符的字符串默认为`'"'`（引号）。 |
| `headers`                  | csv文件中的列名列表。                                        |
| `parse_row(response, row)` | 接收响应和`dict`（代表每一行），其中为csv文件的每个提供的（或检测到的）头文件都有一个键。该`spider`也提供了`adapt_response`和`process_results`用于预处理和后处理目的的方法。 |

### 2.案例

```python
from scrapy.spiders import CSVFeedSpider
from myproject.items import TestItem

class MySpider(CSVFeedSpider):
    name = "csv"
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.csv']
    delimiter = ';'
    quotechar = "'"
    headers = ['id', 'name', 'description']

    def parse_row(self, response, row):
        self.logger.info('Hi, this is a row!: %r', row)

        item = TestItem()
        item['id'] = row['id']
        item['name'] = row['name']
        item['description'] = row['description']
        return item
```

## 五、SitemapSpider

`SiteMapSpider`允许通过使用`Sitemaps`从持嵌套的站点地图和从中发现站点地图`URL`。

```
class scrapy.spiders.SitemapSpider
```

### 1.属性或方法

| 属性或方法                | 说明                                                         |
| ------------------------- | ------------------------------------------------------------ |
| `sitemap_urls`            | 指向要爬网其URL的网站地图的URL列表。                         |
| `sitemap_rules`           | 元组列表 `(regex, callback)`，其中`regex` 是一个正则表达式，用于匹配从站点地图中提取的URL。 `regex` 可以是str或已编译的regex对象。回调是用于处理与正则表达式匹配的URL的回调。 `callback` 可以是字符串（来自于其他`spider`）或函数名。<br>规则按顺序应用，只使用第一个匹配的规则。如果省略此属性，则在站点地图中找到的所有URL都将使用 `parse` 回调。 |
| `sitemap_follow`          | 应遵循的站点地图正则表达式列表，默认遵循所有站点地图。       |
| `sitemap_alternate_links` | 指定是否为一个 `url` 应该遵循。这些是同一网站的链接，使用同一网站内传递的另一种语言 `url` 块。 |
| `sitemap_filter(entries)` | 这是一个过滤器函数，可以重写该函数以根据其属性选择站点地图条目。 |

详情参阅：https://docs.scrapy.org/en/latest/topics/spiders.html#sitemapspider

### 2.案例

```python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ['http://www.example.com/robots.txt']
    sitemap_rules = [
        ('/shop/', 'parse_shop'),
    ]

    other_urls = ['http://www.example.com/about']

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass # ... scrape shop here ...

    def parse_other(self, response):
        pass # ... scrape other here ..
```

## 六、爬虫启动方法

### 1.`cmd`命令运行

#### （1）启动爬虫

```shell
scrapy crawl <spider_name>
```

#### （2）启动需要暂停的爬虫

想要实现暂停，`Scrapy`代码不用修改，只需要在启动时修改运行命令即可：

```python
# scrapy crawl 爬虫名称 -s JOBDIR=缓存scrapy信息的路径

scrapy crawl MySpider -s JOBDIR=crawls/my_spider-1
```

在终端启动爬虫之后，只需要按下`ctrl + c`就可以让爬虫暂停，**`ctrl + c`不能执行两次，只需一次即可**。

```python
# 恢复爬虫
# scrapy crawl 爬虫名称 -s JOBDIR=缓存scrapy信息的路径

scrapy crawl MySpider -s JOBDIR=crawls/my_spider-1
```

#### （3）运行含有参数的爬虫



### 2.脚本启动

#### （1）调用`cmd`模块

在项目根目录创建`run.py`脚本，以启动`scrapy`爬虫。

```python
from scrapy import cmdline

cmdline.execute("scrapy crawl <spider_name>".split())
```

#### （2）`Twisted reactor`

- `CrawlerProcess`类

可以在`spider`文件夹中添加`main`代码：

```python
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings


class MySpider(scrapy.Spider):
    # Your spider definition
    ...
 
process = CrawlerProcess(get_project_settings())
# `get_project_settings()`应该是一个类似字典的`setting`对象，也可以直接传入一个字典
# process = CrawlerProcess(settings={
#     "FEEDS": {
#         "items.json": {"format": "json"},
#     },
# })

process.crawl(MySpider)
# 如果存在多个Spider还可以继续运行
process.crawl(MySpider2) # ...
process.start()
```

也可已将如上代码放置在项目根目录创建`run.py`脚本中，通过`spider_name`启动爬虫：

```python
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings

process = CrawlerProcess(get_project_settings())

# 'followall' is the name of one of the spiders of the project.
process.crawl('followall', domain='scrapy.org')
process.start()
```

- `CrawlerRunner`类

一般更建议使用`CrawlerRunner`而不是`CrawlerProcess`，不过要注意，`spider`爬取完毕后，应该手动关闭`reactor`，也可以像如下一样将关闭代码写入回调函数。

```python
from twisted.internet import reactor
import scrapy
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

class MySpider(scrapy.Spider):
    # Your spider definition
    ...

configure_logging({'LOG_FORMAT': '%(levelname)s: %(message)s'})

settings = get_project_settings()
runner = CrawlerRunner()

# 将手动关闭代码写入callback中
runner.crawl(MySpider)
runner.crawl(MySpider2)
d = runner.join()

d.addBoth(lambda _: reactor.stop())
reactor.run()
```

或者像如下这样，按顺序启动爬虫。

```python
from twisted.internet import reactor, defer
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings

configure_logging()
settings = get_project_settings()
runner = CrawlerRunner(settings)

@defer.inlineCallbacks
def crawl():
    yield runner.crawl(MySpider1)
    yield runner.crawl(MySpider2)
    reactor.stop()
crawl()
reactor.run()
```

### 3.扩展：通用配置爬虫

可以将一个`spider`中的所需要的所有的基本的配置（不同部分）都抽离成一个`json`文件，当需要启动爬虫时，从该配置文件中读取然后动态加载到 Spider 中即可。例如：

```json
{
  "spider": "universal",
  "website": "中华网科技",
  "type": "新闻",
  "index": "http://tech.china.com/",
  "settings": {"USER_AGENT": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36"
  },
  "start_urls": ["http://tech.china.com/articles/"],
  "allowed_domains": ["tech.china.com"],
  "rules": {
      "china": (Rule(LinkExtractor(allow="article\/.*\.html", restrict_xpaths="//div[@id="left_side"]//div[@class="con_item"]"),
             callback='parse_item'),
        Rule(LinkExtractor(restrict_xpaths='//div[@id="pageStyle"]//a[contains(., "下一页")]'))
    )
}
}
```

进一步的，在`spider`文件中动态生成`spider`：

```python
import json
from os.path import realpath, dirname
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from scrapyuniversal.rules import rules

class UniversalSpider(CrawlSpider):
    name = 'universal'
    def __init__(self, name, *args, **kwargs):
        # 读取配置文件
        path = dirname(realpath(__file__)) + '/configs/' + name + '.json'
    	with open(path, 'r', encoding='utf-8') as f:
    	    self.config = json.loads(f.read())
		# 配置爬虫属性
        self.rules = rules.get(config.get('rules'))
        self.start_urls = config.get('start_urls')
        self.allowed_domains = config.get('allowed_domains')
        super(UniversalSpider, self).__init__(*args, **kwargs)
        
if __name__ == '__main__':
    import sys
    from scrapy.utils.project import get_project_settings
    from scrapy.crawler import CrawlerProcess
    
     # 读取配置文件
    path = dirname(realpath(__file__)) + '/configs/' + name + '.json'
    with open(path, 'r', encoding='utf-8') as f:
        custom_settings = json.loads(f.read()).get('settings')
        
    # 读取爬取使用的 Spider 名称
    spider = custom_settings.get('spider', 'universal')
    
    # 合并setting配置
    settings = dict(get_project_settings().copy())
    settings.update(custom_settings)
    process = CrawlerProcess(settings)
    # 启动爬虫
    process.crawl(spider, **{'name': sys.argv[1]})
    process.start()
```

程序会首先读取 `JSON` 配置文件，将配置中的一些属性赋值给 `Spider`，然后启动爬取。

同样，也可将解析部分配置化， 通过定义`class` 和` loader` 属性来配置 `Item` 和 `Item Loader `所使用的类。定义了`attrs`  属性来定义每个字段的提取规则

```python
"item": {
  "class": "NewsItem",
  "loader": "ChinaLoader",
  "attrs": {
    "title": [
      {
        "method": "xpath",
        "args": ["//h1[@id='chan_newsTitle']/text()"]
      }
    ],
    "url": [
      {
        "method": "attr",
        "args": ["url"]
      }
    ],
    "text": [
      {
        "method": "xpath",
        "args": ["//div[@id='chan_newsDetail']//text()"]
      }
    ],
    "datetime": [
      {
        "method": "xpath",
        "args": ["//div[@id='chan_newsInfo']/text()"],
        "re": "(\\d+-\\d+-\\d+\\s\\d+:\\d+:\\d+)"
      }
    ],
    "source": [
      {
        "method": "xpath",
        "args": ["//div[@id='chan_newsInfo']/text()"],
        "re": "来源：(.*)"
      }
    ],
    "website": [
      {
        "method": "value",
        "args": ["中华网"]
      }
    ]
  }
}
```

进一步将其动态加载到`parse_item`函数：

```python
def parse_item(self, response):
    item = self.config.get('item')
    if item:
        cls = eval(item.get('class'))()
        loader = eval(item.get('loader'))(cls, response=response)
        # 动态获取属性配置
        for key, value in item.get('attrs').items():
            for extractor in value:
                if extractor.get('method') == 'xpath':
                    loader.add_xpath(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'css':
                    loader.add_css(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'value':
                    loader.add_value(key, *extractor.get('args'), **{'re': extractor.get('re')})
                if extractor.get('method') == 'attr':
                    loader.add_value(key, getattr(response, *extractor.get('args')))
        yield loader.load_item()
```

首先获取` Item` 的配置信息，然后获取` class` 的配置，将其初始化，初始化 `Item Loader`，遍历` Item `的各个属性依次进行提取。判断` method` 字段，调用对应的处理方法进行处理。如 `method `为 `css`，就调用 `Item` `Loader` 的 `add_css() `方法进行提取。所有配置动态加载完毕之后，调用` load_item() `方法将 `Item `提取出来。

了解更多可以参考：https://github.com/Python3WebSpider/ScrapyUniversal。
