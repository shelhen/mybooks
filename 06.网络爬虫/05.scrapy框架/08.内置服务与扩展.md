



# 08.内置服务与扩展

## 一、扩展

扩展提供了一种将自定义功能插入`scrapy`的机制，利用 `Extension` 可以注册一些处理方法并监听 `Scrapy`运行过程中的各个信号，做到在发生某个事件时执行自定义的方法，扩展实际上只是常规的`python`类。

### 1.扩展设置与激活

#### 1）扩展设置

扩展使用Scrapy设置来管理公共配置项，就像`scrapy`其他组件一样，推荐在扩展设置项前面增加自己的名称，以避免与现有（和未来）的扩展冲突。

#### 2）扩展激活

扩展通过为每个正在运行的蜘蛛实例化扩展类的单个实例来加载和激活，因此所有扩展初始化代码都必须在类的`__init__`方法中执行。扩展的激活也像是`middlewares`设置，再设置中添加如下类似的路径字符串与优先级映射来激活，其值也规定了扩展的启用顺序，`EXTENSIONS`设置与`Scrapy`中定义的`EXTENSIONS_BASE`设置合并（并不意味着被覆盖），然后按顺序排序，以获得已启用扩展的最终排序列表。**由于扩展通常不相互依赖，因此在大多数情况下，它们的加载顺序无关紧要。**

```python
EXTENSIONS = {
    "scrapy.extensions.corestats.CoreStats": 500,
    "scrapy.extensions.telnet.TelnetConsole": 500,
    # 关闭某些扩展
    "scrapy.extensions.corestats.CoreStats": None,
}

EXTENSIONS_BASE = {
    "scrapy.extensions.corestats.CoreStats": 0,
    "scrapy.extensions.telnet.TelnetConsole": 0,
    "scrapy.extensions.memusage.MemoryUsage": 0,
    "scrapy.extensions.memdebug.MemoryDebugger": 0,
    "scrapy.extensions.closespider.CloseSpider": 0,
    "scrapy.extensions.feedexport.FeedExporter": 0,
    "scrapy.extensions.logstats.LogStats": 0,
    "scrapy.extensions.spiderstate.SpiderState": 0,
    "scrapy.extensions.throttle.AutoThrottle": 0,
}
```

`EXTENSIONS_BASE`中的设置均是以相同的顺序执行，如果需要添加依赖于其他扩展的扩展，可以利用优先级来设置。

### 2.自定义扩展对象

其实，中间件和管道就是一种特殊的扩展，其主要入口点时接收`crawler`实例的`from_crawler`类方法，通过`Crawler`对象，可以访问设置、信号、统计数据以及控制`spider`行为。通常，扩展通过连接信号并执行由信号触发的任务，注意如果`from_crawler`方法引发了`NotConfigured`，响应的扩展将被禁用。

如下，我们实现了一个消息记录扩展，在每次`spider`开启或关闭时，记录抓取`item`的数目，我们自定义了如下设置项用于配置扩展：

```python
MYEXT_ENABLED = True  # 是否启用扩展
MYEXT_ITEMCOUNT = # 指定项目的数量
```

```python
import logging
from scrapy import signals
from scrapy.exceptions import NotConfigured

logger = logging.getLogger(__name__)


class SpiderOpenCloseLogging:
    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # 首先检查是否应启用扩展，如果不应启用扩展就抛出 NotConfigured 异常
        if not crawler.settings.getbool("MYEXT_ENABLED"):
            raise NotConfigured

        # 从设置中获取项目数
        item_count = crawler.settings.getint("MYEXT_ITEMCOUNT", 1000)
        # 实例化扩展对象
        ext = cls(item_count)

        # 链接扩展对象的 handle 和 信号
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # 返回扩展实例
        return ext

    def spider_opened(self, spider):
        logger.info("opened spider %s", spider.name)

    def spider_closed(self, spider):
        logger.info("closed spider %s", spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped % self.item_count == 0:
            logger.info("scraped %d items", self.items_scraped)
```

### 3.内置扩展

#### 1）日志记录扩展

```python
class scrapy.extensions.logstats.LogStats
```

#### 2）信息统计扩展

```python
class scrapy.extensions.corestats.CoreStats
```

#### 3）Telnet控制台扩展

```python
class scrapy.extensions.telnet.TelnetConsole
```

#### 4）内存使用扩展

监控运行`spider`的`Scrapy`进程使用的内存，当内存超过特定值（`MEMUSAGE_warning_MB`）时发送通知电子邮件，当内存超过某个值（`MEMUSAGE_LIMIT_MB`）时关闭`spider`并终止`scrapy`进程。

> 此扩展在Windows中不起作用。

```python
class scrapy.extensions.memusage.MemoryUsage
```

配置项

```python
MEMUSAGE_ENABLED  # 是否启用该扩展
MEMUSAGE_LIMIT_MB  # 内存最大限度值，超出此值将关闭爬虫
MEMUSAGE_WARNING_MB  # 内存警告限度值，超出此值将发送电子邮件。
MEMUSAGE_NOTIFY_MAIL  # 内存警告限制时通知的电子邮件列表[]
MEMUSAGE_CHECK_INTERVAL_SECONDS  # 以固定的时间间隔检查当前内存使用情况是否达到以上限制，单位为秒。
```

#### 5）内存调试扩展

```python
class scrapy.extensions.memdebug.MemoryDebugger
```

用于调试内存使用情况的扩展，启用此扩展请设置`MEMDEBUG_ENABLED=True`，它收集有关以下对象的信息，信息将存储在统计数据中：

- Python垃圾收集器未收集的对象；
- 不应该存活的对象，了解更多请参阅 [trackref调试内存泄漏](https://docs.scrapy.org/en/latest/topics/leaks.html#topics-leaks-trackrefs)。

#### 6）`spider`关闭扩展

```python
class scrapy.extensions.closespider.CloseSpider
```

当满足某些条件时，使用每个条件的特定关闭原因自动关闭`spider`，关闭蜘蛛的条件可以通过以下设置进行配置：

```python
# 当spider 开启指定时间后自动关闭，关闭原因 closespider_timeout，为零表示不会超时关闭，单位为秒
CLOSESPIDER_TIMEOUT = 0 
# 当spider 在 指定时间内没有返回item则自动关闭，关闭原因closespider_timeout_no_item，为零表示不会关闭，单位为秒
CLOSESPIDER_TIMEOUT_NO_ITEM = 0
# 当spider抓取（并通过管道存储）的item超过指定数目时自动关闭，关闭原因closespider_itemcount,为零表示无限，
CLOSESPIDER_ITEMCOUNT = 0
# 当spider爬取的页面数达到指定数目时自动关闭,原因 closespider_pagecount，如果为0表示无限制。
CLOSESPIDER_PAGECOUNT = 0
# 当spider遇到的错误达到指定数目时自动关闭，原因 closespider_errorcount, 设置为0代表无限制。
CLOSESPIDER_ERRORCOUNT = 0
# 当满足某个关闭条件时，当前在下载器队列中的请求（最多为CONCURRENT_requests请求）仍将被处理。
CONCURRENT_REQUESTS
```

#### 7）StatsMailer扩展

可用于在每次域完成抓取时发送通知电子邮件，包括收集的抓取统计数据。邮件将发送给`STATSMAILER_RCPTS`指定的所有收件人。

```python
class scrapy.extensions.statsmailer.StatsMailer
```

#### 8）定期日志扩展

此扩展定期将丰富的统计数据记录为JSON对象.

```python
class scrapy.extensions.periodic_log.PeriodicLog
```

```python
2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {
    "delta": {
        "downloader/request_bytes": 55582,
        "downloader/request_count": 162,
        "downloader/request_method_count/GET": 162,
        "downloader/response_bytes": 618133,
        "downloader/response_count": 162,
        "downloader/response_status_count/200": 162,
        "item_scraped_count": 161
    },
    "stats": {
        "downloader/request_bytes": 338243,
        "downloader/request_count": 992,
        "downloader/request_method_count/GET": 992,
        "downloader/response_bytes": 3836736,
        "downloader/response_count": 976,
        "downloader/response_status_count/200": 976,
        "item_scraped_count": 925,
        "log_count/INFO": 21,
        "log_count/WARNING": 1,
        "scheduler/dequeued": 992,
        "scheduler/dequeued/memory": 992,
        "scheduler/enqueued": 1050,
        "scheduler/enqueued/memory": 1050
    },
    "time": {
        "elapsed": 360.008903,
        "log_interval": 60.0,
        "log_interval_real": 60.006694,
        "start_time": "2023-08-03 23:24:57",
        "utcnow": "2023-08-03 23:30:57"
    }
}
```

此扩展记录了以下可配置部分:

- “time”显示详细的计时数据，可通过设置项`PERIODIC_LOG_TIMING_ENABLED`决定是否开启。
  - 默认为`True`，即允许显示详细的计时数据。

- “delta”显示了自上次统计日志消息以来一些数字统计数据的变化情况，`PERIODIC_LOG_DELTA`设置。

  - 默认为`None`；

  - 当其值为`True`，则显示所有类型为`int`和`float`统计值的增量；
  - 当其值为字典:`{"include":['scheduler/',],"exclude":['downloader/',]}`时，显示所有名称处于`include`关键字下的统计数据增量，但是排除所有名称处于`exclude`关键字统计数据的增量。

- “stats”显示了一些统计数据的当前值，通过`PERIODIC_LOG_STATS`设置确定的目标统计信息。

  - 默认为`None`;
  - 当其值为`True`时，显示所有统计数据的当前值。
  - 当其值为字典`{"include":['scheduler/',],"exclude":['downloader/',]}`时，显示`include`关键字下所有子字符串的名称的统计数据的当前值，并排除所有处于`exclude`关键字下所有子字符串的所有统计信息的当前值。

参考配置

```python
custom_settings = {
    "LOG_LEVEL": "INFO",
    "PERIODIC_LOG_STATS": {
        "include": ["downloader/", "scheduler/", "log_count/", "item_scraped_count/"],
    },
    "PERIODIC_LOG_DELTA": {"include": ["downloader/"]},
    "PERIODIC_LOG_TIMING_ENABLED": True,
    "EXTENSIONS": {
        "scrapy.extensions.periodic_log.PeriodicLog": 0,
    },
}
```

#### （9）堆栈跟踪转储扩展

```python
class scrapy.extensions.periodic_log.StackTraceDump
```

当接收到`SIGQUIT`或`SIGUSR2`信号时，转储有关正在运行的进程的信息。转储的信息如下：

- 引擎状态，`scrapy.utils.engine.get_engine_status`；
- 实时引用，请参阅[trackref调试内存泄漏](https://doc.scrapy.org/en/latest/topics/leaks.html#topics-leaks-trackrefs)了解更多。

所有线程的堆栈跟踪转储堆栈跟踪和引擎状态后，scrapy进程继续正常运行。

> **此扩展仅适用于POSIX兼容平台（即非Windows）**，因为SIGQUIT和SIGUSR2信号在Windows上不可用。

至少有两种方法可以向Scrapy发送SIGQUIT信号：

- 在Scrapy进程运行时按Ctrl键（仅限Linux）
- 通过运行命令`kill -QUIT <pid>`，（<pid>是Scrapy过程的进程id）

#### （10）调试扩展

当收到`SIGUSR2`信号时，在正在运行的`Scrapy`进程中调用`Python`调试器，退出调试器后，Scrapy进程继续正常运行。

```python
class scrapy.extensions.periodic_log.Debugger
```

> 此扩展仅适用于POSIX兼容平台（即不适用于Windows）。

## 二、插件`add-on`

插件是一个统一管理和配置扩展Scrapy核心功能的组件框架，它为用户提供了Scrapy扩展管理的即插即用体验，并为开发人员提供了广泛的配置控制。

### 1.激活附加系统

在`spider`初始化期间，将从`ADDONS`设置中读取已启用的加载项列表，`ADDONS`设置是一个字典，其中每个键都是一个附加类或其导入路径，值是它的优先级。

```python
ADDONS = {
    'path.to.someaddon': 0,
    SomeAddonClass: 1,
}
```

### 2.插件的继承扩展

`scrapy`插件(`add-on`)是实现利润如下方法的`python`标准类：

#### （1）`update_settings(settings)`

该方法在`spider`初始化期间调用，应该在该方法中执行依赖性检查，并根据需要更新`Settings`对象，例如为此插件启用组件或设置其他扩展的所需配置。如果`update_settings`方法引发`scrapy.exceptions.NotConfigured`，则将跳过该插件。

```python
def update_settings(self, settings):
    # 基于add-on 更新 设置应该使用 `"addon"`优先级
    settings.set("DNSCACHE_ENABLED", True, "addon")

def update_settings(self, settings):
    # 插件允许用户在项目或spider设置中覆盖初始设置。
    # 但是对于可变类型而言，往往是无法覆盖修改而是直接修改的，这时可以提供一个特定的附加组件设置，以控制该附加组件是否会修改ITEM_PIPELINES
    if settings.getbool("MYADDON_ENABLE_PIPELINE"):
        settings["ITEM_PIPELINES"]["path.to.mypipeline"] = 200
```

#### （2）`classmethod from_crawler(cls, crawler)`

如果存在，则调用此类方法以从`spider`中创建**附加组件**实例。它须返回插件的新实例，该方法可以提供包括设置在内的所有核心组件的访问，这时附加组件访问它们并将其功能挂接到Scrapy的一种方式。

### 3.回退

插件提供的一些组件有时需要回退到“默认”实现，例如自定义下载处理程序需要通过默认下载处理程序发送它不处理的请求，一个包含一些额外处理但使用默认统计数据收集器的统计数据收集器，或者一个项目可能需要使用多个相同类型的自定义组件。为了使此类用例更易于配置，建议此类自定义组件应按以下方式编写：

- 自定义附加组件不应该继承自`scrapy`默认附加组件，例如`scrapy.core.downloader.handlers.http.HTTPDownloadHandler`，而是应该从一个特殊的设置`MY_FALLBACK_DOWNLOAD_HANDLER`中加载回退组件的类`Scrapy.core.downloader.HANDLER.http.HTTPDownloadHandler`。
- 包含这些组件的插件应在其`update_settings`方法中读取默认设置的当前值（如`OWNLOAD_HANDLERS`），将该值保存到回退设置`MY_fallback_DOWNLOAD_HANDLER`中，并将默认设置设置为插件提供的组件（如`MyDownloadHandler`），如果用户已经设置了回退设置，则不应更改它。
- 如果有多个插件想要修改相同的设置，它们都将回退到上一个组件中的组件，然后回退到Scrapy默认值。其顺序取决于ADDONS设置中的优先级顺序。

#### 4.附件组件案例

```python
from scrapy.core.downloader.handlers.http import HTTPDownloadHandler


FALLBACK_SETTING = "MY_FALLBACK_DOWNLOAD_HANDLER"


class MyHandler:
    lazy = False

    def __init__(self, settings, crawler):
        dhcls = load_object(settings.get(FALLBACK_SETTING))
        self._fallback_handler = create_instance(
            dhcls,
            settings=None,
            crawler=crawler,
        )

    def download_request(self, request, spider):
        if request.meta.get("my_params"):
            # handle the request
            ...
        else:
            return self._fallback_handler.download_request(request, spider)


class MyAddon:
    def __init__(self, crawler) -> None:
        super().__init__()
        self.crawler = crawler

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)
    
    def update_settings(self, settings):
        # 检测依赖并进行基本设置
        if not settings.get(FALLBACK_SETTING):
            settings.set(
                FALLBACK_SETTING,
                settings.getwithbase("DOWNLOAD_HANDLERS")["https"],
                "addon",
            )
        settings["DOWNLOAD_HANDLERS"]["https"] = MyHandler
```

## 三、日志记录

日志记录是一种跟踪某些软件运行时发生的事件的方法。这 软件的开发人员将日志记录调用添加到他们的代码中，以指示某些事件已经发生。`Scrapy`使用`Python`内置的的日志系统来记录事件日志，如需了解更多细节，建议阅读[logging-python的日志记录工具](https://docs.python.org/zh-cn/3/library/logging.html)。

### 1.日志级别

python的内置日志记录定义了5个不同的级别，以指示给定日志消息的严重性。以下是标准的，按降序排列：

| 严重级别           | 说明                                     | 对应方法     |
| ------------------ | ---------------------------------------- | ------------ |
| `logging.CRITICAL` | 程序无法继续运行的严重错误（严重性最高） | `critical()` |
| `logging.ERROR`    | 程序无法执行某些功能的常规错误           | `error()`    |
| `logging.WARNING`  | 出现意想不到的事情的警告消息             | `warning()`  |
| `logging.INFO`     | 确认正常的预期信息                       | `info()`     |
| `logging.DEBUG`    | 令人感兴趣的调试消息（最低严重性）       | `debug()`    |

### 2.基本使用

```python
import logging

# 直接输出错误信息
logging.warning("This is a warning")
logging.log(logging.WARNING, "This is a warning")
# 可变数据
logging.warning('%s before you %s', 'Look', 'leap!')

# 创建“日志实例”来封装消息
logger = logging.getLogger('simple_example')
# 设置日志级别
logger.setLevel(logging.DEBUG)

# 创建日志话柄
ch = logging.StreamHandler()
# 创建日志格式
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# 设置日志格式
ch.setFormatter(formatter)
logger.addHandler(ch)
# 直接输出信息
logger.warning("This is a warning")
```

### 3.`scrapy`内置日志

`scrapy`在每个`spider`中提供了日志接口：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['https://scrapy.org']
    def parse(self, response):
        self.logger.info('Parse function called on %s', response.url)
```

### 4.日志记录配置

#### （1）日志设置

```python
LOG_ENABLED = True  # 是否启用scrapy日志
LOG_FILE_APPEND=True  # 如果为False，日志存储将基于`w`（覆写）模式，否则基于`a`(续写)模式。
LOG_ENCODING = "utf-8"  # 日志文本编码
LOG_LEVEL = logging.INFO  # 要显示的最低严重性级别
LOG_FILE = "log/spider.log"  # 存储路径
LOG_STDOUT = False  # 如果为True，则所有的标准输出都将转化为log模式，比如print也会转化为log。
LOG_FORMAT = "%(asctime)s [%(name)s] %(levelname)s: %(message)s"  # 所有消息布局的格式字符串
LOG_DATEFORMAT = "%Y-%m-%d %H:%M:%S"  # 日志时间格式

LOG_SHORT_NAMES = False # 如果为True，则仅仅将root path置入日志，否则则会详细报告到产生日志的组件
```

#### （2）日志命令

```python
--logfile FILE  # 加入此参数将 覆盖设置中的  LOG_FIL
--loglevel/-L LEVEL  # 加入此参数将 覆盖设置中的  LOG_LEVEL
--nolog  # 加入此参数相当于 设置 LOG_ENABLED = False
```

### 5.自定义日志格式

如果需要自定义日志格式，可以继承或重写如下类：

```python
class scrapy.logformatter.LogFormatter
```

该类提供日志输出的接口函数包括如下六个方法，所有方法都必须返回一个列出参数的字典 `level` 、 `msg` 和 `args` 调用时将用于构造日志消息 `logging.log`，如果希望彻底忽略该类消息，可以将函数返回值设置为`None`。

```python
{
    'level': logging.INFO, # 该操作的日志级别
    'msg': "Dropped: %(exception)s" + os.linesep + "%(item)s",  # 包含不同格式占位符的字符串，对应的参数在于args
    'args': {
        'exception': exception,
        'item': item
    }  # 应该是一个tuple或dict，其中的格式占位符为 msg
}
```

日志格式化类的继承扩展案例：

```python
class PoliteLogFormatter(logformatter.LogFormatter):
    
    def crawled(self, request: Request, response: Response, spider: Spider):
        """当爬虫找到网页时记录一条消息。"""
        pass
    
    def download_error(self, failure: Failure, request: Request, spider: Spider, errmsg: Optional[str] = None)->dict:
        """记录来自spider的下载错误消息（通常来自引擎）。"""
        pass
    
    def dropped(self, item: Any, exception: BaseException, response: Response, spider: Spider)->dict
    	"""当某个项在通过项管道时被丢弃时，记录该消息。"""
        pass
    
    def item_error(self, item: Any, exception: BaseException, response: Response, spider: Spider)->dict:
        """当一个项目在通过项目管道时出错时，记录消息。"""
        pass
    
    def scraped(self, item: Any, response: Union[Response, Failure], spider: Spider)->dict:
        """当一个项目被蜘蛛抓取时记录一条消息。"""
        pass
    
    def spider_error(failure: Failure, request: Request, response: Union[Response, Failure], spider: Spider)->dict:
        """记录来自spider的错误消息。"""
        pass
        
```

> 上面的方式是`scrapy`提供的接口自定义日志的输出，还有部分日志输出来源于框架本身，如`2016-12-16 22:00:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring
> response <500 http://quotes.toscrape.com/page/1-34/>: HTTP status code
> is not handled or not allowed`，这些日志信息来源于框架本身，无法定制化，但是可以选择忽略。

- 隐藏所有`info`级别的信息

只需要在设置中增加 `LOG_LEVEL = logging.WARNIN`就可以忽略所有`WARNIN`级别以下的日志信息，上述信息也可以被忽略。

- 定制化忽略

可以看到，该日志的名称为`scrapy.spidermiddlewares.httperror`，不妨设置名称为`scrapy.spidermiddlewares.httperror`的日志输出级别。

```python
class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        logger = logging.getLogger('scrapy.spidermiddlewares.httperror')
        logger.setLevel(logging.WARNING)
        super().__init__(*args, **kwargs)
```

- 批量忽略

创建日志过滤器，基于字串或正则表达式按照消息内容进行过滤。

```python
import logging
import re
import scrapy

class ContentFilter(logging.Filter):
    def filter(self, record):
        match = re.search(r'\d{3} [Ee]rror, retrying', record.message)
        if match:
            return False

class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        for handler in logging.root.handlers:
            handler.addFilter(ContentFilter())
# 也可以选择特定的记录器并将其隐藏，而不会影响其他记录器
class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        logger = logging.getLogger('my_logger')
        logger.addFilter(ContentFilter())
```

> 了解更多可参考[stdlib模块]()。

## 四、信息统计

`Scrapy`提供了一种方便的工具，可以以`键-值`的形式收集统计信息，其中值通常是计数器。该工具称为`stats collector`，可以通过`Crawler API`访问此类属性，`stats collector`为每个打开的`spider`保留一个`stats`表，该表在`spider`打开时自动打开，在`spider`关闭时关闭。

### 1.基本使用

在爬虫类中使用`stats collector`，通过`stats`属性访问统计数据收集器。

```python
class ExtensionThatAccessStats:
    def __init__(self, stats):
        self.stats = stats

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats)

	def parse(self, response):
        # 这里不处理响应数据，尝试读取和设置 统计信息
        
        # 设置统计 键的值
        stats.set_value("hostname", socket.gethostname())
        # 增加统计的键
        stats.inc_value("custom_count")
        # 仅当大于之前的值时才设置统计值
        stats.max_value("max_items_scraped", value)
        # 仅当低于之前的值时才设置统计值
        stats.min_value("min_free_memory_percent", value)
        # 读取统计值
        print(stats.get_value("custom_count"))
        # 读取所有统计信息
        print(stats.get_stats())
```

### 2.`StatsCollector`基类

```python
class scrapy.statscollectors.StatsCollector
```

`StatsCollector`本质是一个类字典类型，其中实现了一些方法：

| 方法名称                           | 说明                                                     |
| ---------------------------------- | -------------------------------------------------------- |
| `get_value(key, default=None)`     | 返回给定统计键的值，如果不存在，则返回默认值。           |
| `get_stats()`                      | 获取当前正在运行的蜘蛛的所有统计数据。                   |
| `set_value(key, value)`            | 为给定的统计键设置给定值。                               |
| `set_stats(stats)`                 | 用`stats`参数中传递的`dict`覆盖当前统计数据。            |
| `inc_value(key, count=1, start=0)` | 假设给定起始值（未设置时），按给定的计数递增统计键的值。 |
| `max_value(key, value)`            | 仅当大于之前的值时才设置统计值                           |
| `min_value(key, value)`            | 仅当低于之前的值时才设置统计值                           |
| `clear_stats()`                    | 清除所有统计数据                                         |
| `open_spider(spider)`              | 打开给定的蜘蛛以收集统计数据。                           |
| `close_spider(spider)`             | 调用此命令后，将无法访问或收集更具体的统计数据。         |

### 3.统计信息收集器子类

#### （1）MemoryStatsCollector

继承自`StatsCollector`，是`spider`默认的统计信息收集器，它在关闭后将上次（每个蜘蛛）抓取运行的统计信息保存在内存中，其扩展了`spider_stats`作为一个字典存储统计数据。

#### （2）DummyStatsCollector

一个关闭统计信息收集器的类，他什么也不做，当在`settings`中将参数`STATS_CLASS`指定为该类时，将禁用统计信息收集以提高性能。

```python
class scrapy.statscollectors.DummyStatsCollector
```

> 与其他零碎的工作负载（如解析页面）相比，统计数据收集的性能损失通常是微乎其微的。

## 五、发送电子邮件

`Scrapy`提供了发送电子邮件的工具，该工具基于[Twisted non-blocking IO](https://docs.twisted.org/en/stable/core/howto/defer-intro.html)实现，并提供了简单易用的`api`来发送网络邮件，只需要增加一些配置项即可。

### 1.MailSender 类

```python
class scrapy.mail.MailSender(
    smtphost=None,  # 用于发送电子邮件的SMTP主机，默认为  MAIL_HOST
    mailfrom=None,   # 用于发送电子邮件的地址，默认为 MAIL_FROM
    smtpuser=None,  # SMTP用户， 默认为  MAIL_USER
    smtppass=None, # 用于身份验证的SMTP通行证。
    smtpport=None,  # 要连接到的SMTP端口
    smtptls=False,  # 强制使用 smtp starttls
    smtpssl=False # 强制使用安全的SSL连接
):
```

#### （1）类方法`from_settings(settings)`

```python
@classmethod
def from_settings(settings):
    # 获取设置中的参数构建类
    pass
```

#### （2）send方法

```python
def send(
    to,  # 以字符串或字符串列表的形式显示电子邮件收件人，是主要接收邮件的人或团体
    subject,  # 电子邮件的主题，主题是对邮件内容简短而准确的描述，帮助收件人在查看邮件之前了解邮件的主要内容。
    body:str,  # 电子邮件正文，包含邮件的详细内容
    cc=None,  # 以字符串或字符串列表的形式向CC发送电子邮件，cc是邮件的抄送对象，可以看到邮件内容和对话，但他们不一定是邮件的主要行动对象。
    attachs=[()],  # 一个可迭代的元组， (attach_name, mimetype, file_object) 分别对应(附件名称，附件mime类型，可读的附件二进制内容)
    mimetype='text/plain',  # 电子邮件的mime类型
    charset=None  # 用于电子邮件内容的字符编码
):
    pass
```

### 2.基本使用案例

```python
from scrapy.mail import MailSender

# mailer = MailSender()
mailer = MailSender.from_settings(settings)

with open('report.pdf', 'rb') as file:
    content = file.read()

mailer.send(
    to='recipient@example.com',
    subject='会议提醒',
    body='亲爱的约翰，请记住我们明天上午10点的会议。...',
    cc='cc@example.com',
    attachs=[('report.pdf', 'application/pdf', content)],
    mimetype='text/plain',
    charset='utf-8'
)
```

**基本设置**

```python
MAIL_FROM = 'scrapy@localhost'  # 要使用的发件人电子邮件
MAIL_HOST = 'localhost'  # 用于发送电子邮件的SMTP主机。
MAIL_PORT = 25  # 用于发送电子邮件的SMTP端口。
MAIL_USER = ""  # 用于SMTP身份验证的用户。如果禁用，将不执行任何SMTP身份验证。
MAIL_PASS = ""  # 用于SMTP身份验证的密码
MAIL_TLS = False  # 使用starttls强制。starttls是一种获取现有不安全连接并使用ssl/tls将其升级为安全连接的方法。
MAIL_SSL = False  # 使用SSL加密连接强制连接
```

## 六、远程链接控制台

`Scrapy`附带了一个内置的`telnet`控制台，用于检查和控制`Scrapy`运行过程。`telnet`控制台只是`Scrapy`进程中运行的一个常规`python shell`，因此可以从中执行任何操作。`telnet`控制台是一个内置的Scrapy扩展，默认是启用的`TELNETCONSOLE_ABLED=True`。

> 通过公共网络使用`telnet`控制台是不安全的，因为telnet不提供任何传输层安全性。用户名/密码身份验证不会改变这一点。实际`telnet`控制台上的预期用途是在本地或通过安全连接（VPN、SSH隧道）连接到正在运行的Scrapy蜘蛛。

### 1.访问telnet控制台

`telnet`控制台监听中定义在`setting`中的`TCP`的主机IP-`TELNETCONSOLE_HOST`和端口`TELNETCONSOLE_PORT`，默认为 `6023`：

```shell
telnet localhost 6023
Trying localhost...
Connected to localhost.
Escape character is '^]'.
Username:
Password:
```

默认情况下，用户名是`scrapy`，密码可以在Scrapy日志上看到，也可以在设置中通过配置`TELNETCONSOLE_USERNAME`和`TELNETCONSOLE_PASSWORD`自定义。

### 2.可用变量

`telnet`控制台就像一个运行在`scrappy`进程内部的常规`python shell`，可以在其中做任何事，另外，telnet控制台附带一些为方便起见而定义的默认变量：

| 捷径         | 描述                                                         |
| ------------ | ------------------------------------------------------------ |
| `crawler`    | 残废的爬虫 ([`scrapy.crawler.Crawler`](https://www.osgeo.cn/scrapy/topics/api.html#scrapy.crawler.Crawler) 对象） |
| `engine`     | crawler.engine属性                                           |
| `spider`     | 主动蜘蛛                                                     |
| `slot`       | 发动机槽                                                     |
| `extensions` | 扩展管理器（crawler.extensions属性）                         |
| `stats`      | stats收集器（crawler.stats属性）                             |
| `settings`   | Scrapy设置对象（crawler.settings属性）                       |
| `est`        | 打印发动机状态报告                                           |
| `prefs`      | 内存调试（请参见 [调试内存泄漏](https://www.osgeo.cn/scrapy/topics/leaks.html#topics-leaks) ） |
| `p`          | 到的快捷方式 [`pprint.pprint()`](https://docs.python.org/3/library/pprint.html#pprint.pprint) 功能 |
| `hpy`        | 内存调试（请参见 [调试内存泄漏](https://www.osgeo.cn/scrapy/topics/leaks.html#topics-leaks) ） |

### 3.使用示例

```python
telnet localhost 6023
# 检查引擎状态
est()
# 暂停引擎
engine.pause()
# 恢复引擎
engine.unpause()
# 停止引擎
engine.stop()
```

### 4.telnet信号

```python
scrapy.extensions.telnet.update_telnet_vars(telnet_vars)
```

在telnet控制台打开之前发送。可以连接到该信号来添加、删除或更新telnet本地命名空间中可用的变量。为此，需要更新 `telnet_vars` 监听程序，参数`telnet_vars：dict`telnet变量的字典。

## 七、RPC服务

https://github.com/scrapy-plugins/scrapy-jsonrpc

