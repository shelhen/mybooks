## 10.图片爬取

### 一、基础的图片爬虫

http://md.itlun.cn/a/nhtp/

#### 1.抓包分析

访问首页后，发现只有第一条数据载html中，其他数据由js动态生成，首页的图片较小，跳转到详情页后，图片变大了。

我们需求就是简单的拿图片，拿前5页吧，没必要拿更大的图片，因为差不多。

总体思路即访问首页，获取图片链接，下载图片，翻页，下载图片....

获取图片链接是本次练习的关键所在：

**数据存在于js中，不能使用json解析、也不能使用dom树，因此只能用正则表达式。**

或者使用无头浏览器来加载/获取script中的内容使用js库解析，动态执行加载获取。

#### 2.下载和访问链接部分

```python
import requests
import re


def get_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    # response.text  # 出现乱码
    # response.content.decode() # 报错
    return requests.get(url, headers=headers,timeout=10).content


def img_download(base_path, img_src_list):
    def save_img(path, datas):
        with open(path, 'wb') as f:
            f.write(datas)
    for img_src in img_src_list:
        img_src = 'http:' + img_src
        img_name = base_path + img_src.split('/')[-1]
        content = requests.get(img_src)
        save_img(img_name, content.content)
        print(f'{img_name}下载成功！')

for i in range(5):
    url = f"http://md.itlun.cn/a/nhtp/list_2_{i+1}.html"
    response = get_content(url).decode('gbk')
    # img_src_list = use_re_parse(response)
    img_src_list = use_js_parse(response)
    # 解析出的图片地址，是不完整的，缺少http:
    img_download('./datas/imgs1/', img_src_list)
```

#### 2.基于re来爬取

正则表达式计算

```
正常应该是
'<li>.*?<img.*?src="(.*?)" style.*?</li>'
但是首页<LI>是大写的：
 '<LI>.*?<IMG.*?src="(.*?)" style.*?</LI>'
又发现链接由js动态加载
ex_ = '<script.*?src = "(.*?)"; </script>'
```

```python
def use_re_parse(response):
    # ex = '<LI>.*?<IMG.*?src="(.*?)" style.*?</LI>'
    # match_ = re.findall(ex, response.content.decode('gbk'), re.S)
    # 真正的图片地址是有js动态加载出来的
    ex_ = '<script.*?src = "(.*?)"; </script>'
    img_src_list = re.findall(ex_, response, re.S)
    return img_src_list
```

#### 3.基于js解析库来解析

```python
 def use_js_parse(response):
    import js2py
    script = """
    var result=[]
    var imgs={}
    var document={
        getElementById:function(name){  
            imgs[name]={}
            return imgs[name]
        }
    }
    function summery(obj){
        for(let i in obj){
            result.push(obj[i].src)
        }
    }
    """
    ex = '<script type="text/javascript">(.*?)</script>'
    match_ = re.findall(ex, response, re.S)
    context = js2py.EvalJs(enable_require=True)
    context.execute(script)
    for ma in match_:
        if 'document.getElementById' in ma:
            context.execute(ma)
    context.summery(context.imgs)
    return list(context.result)
```

### 二、带有防盗链的图片爬虫

现在很多网站启用了防盗链反爬，防止服务器上的资源被人恶意盗取。什么是防盗链呢？

就是在请求头中加入`referce`字段，访问图片要使用正确的`referce`字段才可以，一般该字段指向自身的网站链接，直接访问图片地址得不到图片。

#### 1.微博照片爬取分析

爬取三里屯时尚女郎：http://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1

将页面中某一组系列详情页的图片进行抓取保存

在解析图片地址的时候，定位src的属性值，返回的内容和开发工具Element中看到的不一样，通过network查看网页源码发现需要解析real_src的值。

直接请求real_src请求到的图片不显示，加上Refere请求头即可

抓包工具定位到某一张图片数据包，在其requests headers中获取

#### 2.代码

```python
import requests
from parsel import Selector

def get_content(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
        'Referer':'https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1'
    }
    return requests.get(url, headers=headers,timeout=10).content

def save_img(path, datas):
    with open(path, 'wb') as f:
        f.write(datas)

html = get_content("https://blog.sina.com.cn/s/blog_01ebcb8a0102zi2o.html?tj=1")
sel = Selector(html.decode())
Nodes = sel.xpath('//div[@id="sina_keyword_ad_area2"]/div/a/img')
url_list = [node.attrib['real_src'] for node in Nodes]
for url in url_list:
    img_name = './datas/imgs2/' + url.split('/')[-1] + '.jpg'
    content = get_content(url)
    save_img(img_name, content)
    print(f"{img_name}下载成功！")
```

### 三、爬取桌面壁纸

将爬取到的图片存储到指定的文件夹中，要求爬取5页。

http://pic.netbian.com/4kmeinv/

```python
import requests
from parsel import Selector
import os

headers = {
    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'
}
def save_img(path, datas):
    with open(path, 'wb') as f:
        f.write(datas)

base_url = 'https://pic.netbian.com'
for i in range(10):
    api = '/4kmeinv/index.html' if i ==0 else f"/4kmeinv/index_{i+1}.html"
    lists = requests.get(base_url+ api, headers=headers, timeout=10).content
    sel = Selector(lists.decode('gbk'))
    # /uploads/allimg/230619/002708-1687105628cd1d.jpg
    imgs_links = sel.xpath('//div[@class="slist"]/ul/li/a/img/@src').getall()
    for url in imgs_links:
        img_name = './datas/imgs3/' + url.split('/')[-1]
        real_url = base_url+url
        # headers[] = ''
        content = requests.get(real_url, headers=headers, timeout=10).content
        save_img(img_name, content)
        print(f"{img_name}下载成功！")
```

###  四、懒加载图片爬取

url：https://sc.chinaz.com/tupian/meinvtupian.html

爬取上述链接中前5页所有的图片数据

图片懒加载是应用在展示图片的网页中的一种技术，该技术是指当网页刷新后，先加载局部的几张图片数据即可，随着用户滑动滚轮，当图片被显示在浏览器的可视化区域范围的话，在动态将其图片请求加载出来即可。（图片数据是动态加载出来）。

**如何实现？**

使用img标签的伪属性（指的是自定义的一种属性）。在网页中，为了防止图片马上加载出来，则在img标签中可以使用一种伪属性来存储图片的链接，而不是使用真正的src属性值来存储图片链接。（图片链接一旦给了src属性，则图片会被立即加载出来）。只有当图片被滑动到浏览器可视化区域范围的时候，在通过js将img的伪属性修改为真正的src属性，则图片就会被加载出来。

**爬取思路**：

只需要在解析图片的时候，定位伪属性（src2）的属性值即可。

**代码**

```python
import requests
from parsel import Selector



headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
}
session = requests.Session()

def download(imgs_links):
    # //scpic.chinaz.net/files/default/imgs/2023-06-21/e06f91c412a69b3c_s.jpg
    def save_img(path, datas):
        with open(path, 'wb') as f:
            f.write(datas)
    for url in imgs_links:
        img_name = './datas/imgs4/' + url.split('/')[-1]
        real_url = 'https:' + url
        content = session.get(real_url, headers=headers, timeout=10).content
        save_img(img_name, content)
        print(f"{img_name}下载成功！")

for i in range(10):
    url = 'https://sc.chinaz.com/tupian/meinvtupian.html' if i==0 else f'https://sc.chinaz.com/tupian/meinvtupian_{i+1}.html'
    print(url)
    lists = session.get(url, headers=headers, timeout=10).content
    sel = Selector(lists.decode())
    imgs_links = sel.xpath('//div[has-class("item")]/img/@data-original').getall()
    download(imgs_links)
```

