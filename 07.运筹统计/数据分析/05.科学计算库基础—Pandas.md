## 05.科学计算库—Pandas

Pandas是Wes McKinney在2008年开发的一个强大的**分析结构化数据**的工具集。Pandas以NumPy为基础（数据表示和运算），提供了用于数据处理的函数和方法，对数据分析和数据挖掘提供了很好的支持；同时Pandas还可以跟数据可视化工具Matplotlib很好的整合在一起，非常轻松愉快的实现数据的可视化展示。

Pandas核心的数据类型是`Series`（数据系列）、`DataFrame`（数据表/数据框），分别用于处理一维和二维的数据，除此之外还有一个名为`Index`的类型及其子类型，它为`Series`和`DataFrame`提供了索引功能。

### 一、Series的应用

Pandas库中的`Series`对象可以用来表示一维数据结构，跟数组非常类似，但是多了一些额外的功能。`Series`的内部结构包含了两个数组，其中一个用来保存数据，另一个用来保存数据的索引。

#### 1.Series对象的创建

```python
# 1.通过列表或数组创建Series对象
ser1 = pd.Series(
    data=[320, 180, 300, 405],
    index=['一季度', '二季度', '三季度', '四季度']
)
# data参数表示数据，index参数表示数据的索引（标签）,若没指定index属性，默认使用数字索引

# 2.通过字典创建Series对象
ser2 = pd.Series(
    {'一季度': 320, '二季度': 180, '三季度': 300, '四季度': 405}
)
# 字典中的键就是数据的索引（标签），字典中的值就是数据
```

#### 2.索引和切片

跟数组一样，`Series`对象也可以进行索引和切片操作，不同的是Series对象因为内部维护了一个保存索引的数组，所以除了可以使用整数索引通过位置检索数据外，还可以通过自己设置的索引标签获取对应的数据。

```python
# 使用整数索引
print(ser2[0], ser[1], ser[2], ser[3])
ser2[0], ser2[3] = 350, 360  # 修改
# 使用自定义的标签索引
print(ser2['一季度'], ser2['三季度'])
ser2['一季度'] = 380  # 修改

# 切片操作
print(ser2[1:3])
print(ser2['二季度':'四季度'])

# 花式索引
print(ser2[['二季度', '四季度']])
ser2[['二季度', '四季度']] = 500, 520

# 布尔索引
ser2[ser2 >= 500]
```

> **提示**：如果要使用负向索引，必须在创建`Series`对象时通过`index`属性指定非数值类型的标签。

#### 3.常用的属性与方法

| 属性                      | 说明                                    |
| ------------------------- | --------------------------------------- |
| `dtype` / `dtypes`        | 返回`Series`对象的数据类型              |
| `hasnans`                 | 判断`Series`对象中有没有空值            |
| `at` / `iat`              | 通过索引访问`Series`对象中的单个值      |
| `loc` / `iloc`            | 通过一组索引访问`Series`对象中的一组值  |
| `index`                   | 返回`Series`对象的索引                  |
| `is_monotonic`            | 判断`Series`对象中的数据是否单调        |
| `is_monotonic_increasing` | 判断`Series`对象中的数据是否单调递增    |
| `is_monotonic_decreasing` | 判断`Series`对象中的数据是否单调递减    |
| `is_unique`               | 判断`Series`对象中的数据是否独一无二    |
| `size`                    | 返回`Series`对象中元素的个数            |
| `values`                  | 以`ndarray`的方式返回`Series`对象中的值 |

|    统计相关的方法    | 说明                                                         |
| :------------------: | ------------------------------------------------------------ |
|     `ser.sum()`      | 求和                                                         |
|     `ser.mean()`     | 求平均值                                                     |
|     `ser.max()`      | 求最大值                                                     |
|     `ser.min()`      | 求最小值                                                     |
|    `ser.count()`     | 求总数                                                       |
|     `ser.std()`      | 求标准差                                                     |
|     `ser.var()`      | 求方差                                                       |
|    `ser.median()`    | 求中位数                                                     |
|   `ser.describe()`   | 求以上所有                                                   |
|    `ser.unique()`    | 去重                                                         |
|   `ser.nunique()`    | 统计不重复值的数量                                           |
| `ser.value_counts()` | 统计每个值重复的次数,返回一个`Series`对象，它的索引就是原来的`Series`对象中的值，而每个值出现的次数就是返回的`Series`对象中的数据，在默认情况下会按照出现次数做降序排列。 |

> `describe()`返回的也是一个`Series`对象，所以可以用`ser2.describe()['mean']`获取平均值。

#### 4.数据处理方法

| 方法                                         | 功能                                                         |
| -------------------------------------------- | ------------------------------------------------------------ |
| `ser.isnull()`                               | 分别对数组中各元素进行空值的判断，为空返回True               |
| `ser.notnull()`                              | 分别对数组中各元素进行空值的判断，非空返回True               |
| `ser.dropna()`                               | 删除空值                                                     |
| `ser.fillna(value=,method='ffill')`          | 填充空值，其中method参数：bfill或backfill表示用后一个元素的值填充空值；ffill或pad表示用前一个元素的值填充空值。 |
| `mask(条件判断,替换值)`                      | 将满足条件的值进行替换。                                     |
| `where(条件,替换值)`                         | 将不满足条件的值进行替换                                     |
| `duplicated()`                               | 找出重复的数据                                               |
| `drop_duplicates()`                          | 删除重复数据。                                               |
| `sort_index(ascending=False,kind=quicksort)` | 对索引排序，ascending=False为从大到小排序，kind参数是排序算法，可选为quicksort/heapsort，如果存在空值，那么可以用`na_position`参数空值放在最前还是最后，默认是`last`。 |
| `sort_values(ascending=False)`               | 对数据排序。                                                 |
| `nlargest()`                                 | 从`Series`对象中找出元素中最大。                             |
| `nsmallest()`                                | 从`Series`对象中找出元素中最小。                             |

> `dropna()`和`fillna()`方法都有一个名为`inplace`的参数，它的默认值是`False`，表示删除空值或填充空值不会修改原来的`Series`对象，而是返回一个新的`Series`对象。如果将`inplace`参数的值修改为`True`，那么删除或填充空值会就会直接修改原来的`Series`对象，那么方法的返回值是`None`。

```python
# 两个重要方法 map();apply()

ser1 = pd.Series(['cat', 'dog', np.nan, 'rabbit'])
# 这里用kitten替换原本的cat,用pippy替换原本的dog,未被设置的将返回NaN，并且最后结果将返回新数组
ser1.map({'cat': 'kitten', 'dog': 'puppy'})
# 这里将用 I am a {}'.format('cat','dog', np.nan, 'rabbit') 替换原本的cat,dog,rubbit，并且忽略NaN,同样返回新数组。
ser1.map('I am a {}'.format, na_action='ignore')

# 
ser2 = pd.Series([20, 21, 12],  index=['London', 'New York', 'Helsinki'])
ser2.apply(np.square)  # 取值的平方
# 返回x-value,其中value=5,x取series中每个数
ser2.apply(lambda x, value: x - value, args=(5, ))
```

#### 5.绘制图表

Series对象有一个名为`plot`的方法可以用来生成图表，如果选择生成折线图、饼图、柱状图等，默认会使用Series对象的索引作为横坐标，使用Series对象的数据作为纵坐标。

```python
import matplotlib.pyplot as plt

# 配置支持中文的非衬线字体（默认的字体无法显示中文）
plt.rcParams['font.sans-serif'] = ['SimHei', ]
# 使用指定的中文字体时需要下面的配置来避免负号无法显示
plt.rcParams['axes.unicode_minus'] = False

# 创建`Series`对象
ser = pd.Series({'一季度': 400, '二季度': 520, '三季度': 180, '四季度': 380})
# 通过Series对象的plot方法绘图（kind='bar'表示绘制柱状图）
ser.plot(kind='bar', color=['r', 'g', 'b', 'y'])
# x轴的坐标旋转到0度（中文水平显示）
plt.xticks(rotation=0)
# 在柱状图的柱子上绘制数字
for i in range(4):
    plt.text(i, ser9[i] + 5, ser9[i], ha='center')
# 显示图像
plt.show()

# autopct参数可以配置在饼图上显示每块饼的占比
ser9.plot(kind='pie', autopct='%.1f%%')
# 设置y轴的标签（显示在饼图左侧的文字）
plt.ylabel('各季度占比')
plt.show()
```

### 二、DataFrame的应用

#### 1.创建DataFrame对象

数据大部分存在于文件当中，所以pandas会支持复杂的IO操作，pandas的API支持众多的文件格式，如CSV、SQL、XLS、JSON、HDF5。

|  Type  | Data Description | Reader         | Writer       |
| :----: | ---------------- | -------------- | ------------ |
|  text  | CSV              | read_csv       | to_csv       |
|  text  | JSON             | read_json      | to_json      |
|  text  | HTML             | read_html      | to_html      |
|  text  | Local clipboard  | read_clipboard | to_clipboard |
| binary | Excel            | read_excel     | to_excel     |
| binary | HDF5 Format      | read_hdf       | to_hdf       |
| binary | Stata            | read_stata     | to_stata     |
| binary | Python Pickle    | read_pickle    | to_pickle    |
|  SQL   | SQL              | read_sql       | to_sql       |
|  SQL   | Google Big Query | read_gbq       | to_gbq       |

##### (1)通过二维数组创建`DataFrame`对象

```python
scores = np.random.randint(60, 101, (5, 3))
courses = ['语文', '数学', '英语']
ids = [1001, 1002, 1003, 1004, 1005]
df1 = pd.DataFrame(data=scores, columns=courses, index=ids)

# 1.直接传入numpy数组
stock_day_rise = pd.DataFrame(stock_change)

# 2.下面添加行索引
stock_code = ['股票' + str(i) for i in range(stock_day_rise.shape[0])]
data = pd.DataFrame(stock_change, index=stock_code)

# 3.下面增加列索引， date_range()：用于生成一组连续的时间序列
# date_range( 
#    start=None,  # 开始时间
#    end=None,   # 结束时间
#    periods=None,  # periods:时间天数
#    freq='B'  # freq:递进单位，默认1天,'B'默认略过周末
#)
# 生成一个时间的序列，长度等于=stock_day_rise的长度，略过周末非交易日
date = pd.date_range('2022-09-20', periods=stock_day_rise.shape[1], freq='B')
# index代表行索引，columns代表列索引
data = pd.DataFrame(stock_change, index=stock_code, columns=date)
```

```
		语文	数学	英语
1001    69    80	79
1002    71	  60	100
1003    94    81	93
1004    88	  88	67
1005    82	  66    60
```

##### (2)通过字典创建`DataFrame`对象

```python
scores = {
    '语文': [62, 72, 93, 88, 93],
    '数学': [95, 65, 86, 66, 87],
    '英语': [66, 75, 82, 69, 82],
}
ids = [1001, 1002, 1003, 1004, 1005]
df2 = pd.DataFrame(data=scores, index=ids)
```

##### (3)读取 CSV 文件创建`DataFrame`对象

```python
def read_csv(
    sep,delimiter, # 这俩都是分隔符，默认是, 
    header,  # 表头（列索引）的位置，默认值是infer，用第一行的内容作为表头（列索引）。
    index_col,  # 用作行索引（标签）的列。
    usecols,  # 需要加载的列，可以使用序号或者列名。
    true_values,false_values,  # 被视为布尔值True / False的条件
    na_values,  # 被视为空值的条件
    skiprows,   # 通过行号、索引或函数指定需要跳过的行。
    skipfooter,  # 要跳过的末尾行数。
    nrows,  # 需要读取的行数
)
# 示例
df3 = pd.read_csv('2018年北京积分落户数据.csv', index_col='id')
```

##### (4)读取Excel文件创建`DataFrame`对象

```python
def read_excel(
    # ...其他参数与csv一致
    # 无sep或delimiter参数
    sheet_name,  # 指定数据表的名称
    skiprows,  # 传入一个lambda函数
)
# 示例
df4 = pd.read_excel(
    io='小宝剑大药房2018年销售数据.xlsx',
    usecols=['购药时间', '社保卡号', '商品名称', '销售数量', '应收金额', '实收金额'],
    skiprows=lambda x: x > 0 and random.random() > 0.1
    # 该 Lambda 函数作用是只读取 Excel 文件的表头和其中10%的数据，跳过其他数据。
)
```

##### (5)读取txt文档创建`DataFrame`对象

```python
def read_table(
    filepath_or_buffer, # 文件路径，可以是制定储存数据的网站链接
    sep, # 分隔符，默认是tab制表符, 
    header,  # 表头（列索引）的位置，默认值是infer，用第一行的内容作为表头（列索引）。
    names,  # 若原数据集中无变量名，可以用name来给数据添加具体名称，即列名
    index_col,  # 制定某列为数据行索引（标签）的列。
    usecols,  # 指定需要加载的列，可以使用序号或者列名。
    dtype,  # 为不同的列设置不同的数据类型
    converters,  # 通过字典格式，微数据集中某些变量设置转换函数
    skiprows,   # 通过行号、索引或函数指定需要跳过的行。
    skipfooter,  # 要跳过的末尾行数。
    skip_blank_lines,  # 跳过空白行，默认为True
    nrows,  # 需要读取的行数
    na_values,  # 被视为空值的条件，制定哪些特征值为缺失值
    
    true_values,false_values,  # 被视为布尔值True / False的条件 
)
```

> 可以用read_csv代替，只需要将read_csv中的sep参数改变即可。

##### (6)读取json文件创建`DataFrame`对象

```python
def read_json(  # 将JSON格式准换成默认的Pandas DataFrame格式
    path_or_buf=None,
    orient=None,
    # orient :传入一个字符串字典，其中包括以下内容{
    # ‘split’,  # split 将索引总结到索引，列名到列名，数据到数据。将三部分都分开了
    # ’records’,  # 以columns：values的形式输出
    # ’index’,  #  以index：{columns：values}...的形式输出
    # ’columns’,  # 以columns:{index:values}的形式输出
    # ’values’}  # values 直接输出值
    typ='frame',  # 指定转换成的对象类型series或者dataframe
    lines=False)  # 按照每行读取json对象


def to_json(  # 将Pandas 对象存储为json格式
    path_or_buf=None,  # 文件地址
    orient=None,  # 存储的json形式，{‘split’,’records’,’index’,’columns’,’values’}
    lines=False)  # 一个对象存储为一行

# 这里使用一个新闻标题讽刺数据集，格式为json。is_sarcastic：1讽刺的，否则为0；headline：新闻报道的标题；article_link：链接到原始新闻文章。
'''{"article_link": "https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5", "headline": "former versace store clerk sues over secret 'black code' for minority shoppers", "is_sarcastic": 0}
{"article_link": "https://www.huffingtonpost.com/entry/roseanne-revival-review_us_5ab3a497e4b054d118e04365", "headline": "the 'roseanne' revival catches up to our thorny political mood, for better and worse", "is_sarcastic": 0'''

json_read = pd.read_json("./data/Sarcasm_Headlines_Dataset.json", orient="records", lines=True)
```

##### (7)读取HDF5文件创建`DataFrame`对象

```python
def read_hdf(
    path_or_buf， # 文件路径
    key=None，  # 读取的键
    **kwargs)
def to_hdf(
    path_or_buf,  #  
    key,   # 
    **kwargs)  # 
day_eps_ttm = pd.read_hdf("./data/stock_data/day/day_eps_ttm.h5")
new_eps = pd.read_hdf("./data/test.h5", key="day_eps_ttm")
day_eps_ttm.to_hdf("./data/test.h5", key="day_eps_ttm")
```

> **优先选择使用HDF5文件存储**
>
> - HDF5在存储的时候支持压缩，**使用的方式是blosc，这个是速度最快**的也是pandas默认支持的
> - 使用压缩可以**提磁盘利用率，节省空间**
> - HDF5还是跨平台的，可以轻松迁移到hadoop 上面

##### (8)通过SQL从数据库读取数据创建`DataFrame`对象

```python
import pymysql

def read_sql('sql语句','链接对象',index_col='eno')

# 创建一个MySQL数据库的连接对象
conn = pymysql.connect(
    host='47.104.31.138', port=3306,
    user='guest', password='Guest.618',
    database='hrs', charset='utf8mb4'
)
# 通过SQL从数据库读取数据创建DataFrame
df5 = pd.read_sql('select * from tb_emp', conn, index_col='eno')
```

#### 2.索引与切片

```python
# 使用属性方法/字典法 获取某一列
emp_df.ename
emp_df['ename']
# 执行代码可发现，得出一列Series对象,事实上，DataFrame对象是由多个Series对象组合而成

# 使用整数索引/设置索引  获取某一行 
emp_df.iloc[1]
# 执行代码可发现，得出一行Series对象,事实上，单独取DataFrame的某一行或某一列得到的都是Series对象。

# 花式索引
emp_df[['ename', 'job']]  # 获取多个列
emp_df.loc[[2056, 7800, 3344]]  # 获取多个行

# 需要修改DataFrame中某个单元格数据，需要同时指定行和列的索引
emp_df['job'][2056]
emp_df.loc[2056]['job']
emp_df.loc[2056, 'job'] # 推荐使用，只做了一次索引运算
emp_df.loc[2056, 'job'] = '架构师'

# 还可以使用切片获取多行多列
emp_df.loc[2056:3344]

# 布尔索引进行条件获取
emp_df[emp_df.sal > 3500]  # 从 emp_df 中筛选出月薪超过 3500 的员工

# 多条件组合数据筛选
# 从emp_df中筛选出月薪超过3500且部门编号为20的员工
emp_df[(emp_df.sal > 3500) & (emp_df.dno == 20)]

# DataFrame对象的query方法也可以实现数据筛选，query方法的参数是一个字符串，它代表了筛选数据使用的表达式
emp_df.query('sal > 3500 and dno == 20')
```

#### 3.基本属性和方法

为方便演示，先读取三张表的数据，创建三个`DataFrame`对象：

```python
import pymysql

conn = pymysql.connect(
    host='47.104.31.138', port=3306, 
    user='guest', password='Guest.618', 
    database='hrs', charset='utf8mb4'
)
dept_df = pd.read_sql('select * from tb_dept', conn, index_col='dno')
emp_df = pd.read_sql('select * from tb_emp', conn, index_col='eno')
emp2_df = pd.read_sql('select * from tb_emp2', conn, index_col='eno')
```

```python
# 部门表（dept_df），其中dno是部门的编号，dname和dloc分别是部门的名称和所在地。
dno dname  dloc
10	会计部	北京
20	研发部	成都
30	销售部	重庆
40	运维部	天津

# 员工表（emp_df），其中eno是员工编号，ename、job、mgr、sal、comm和dno分别代表员工的姓名、职位、主管编号、月薪、补贴和部门编号。
eno    ename    job        mgr      sal     comm    dno
1359	胡一刀    销售员	   3344.0	1800	200.0	30
2056	乔峰	    分析师	    7800.0	 5000	 1500.0	 20
3088	李莫愁	   设计师	   2056.0	3500	800.0	20
3211	张无忌	   程序员	   2056.0	3200	NaN     20
3233	丘处机	   程序员	   2056.0	3400	NaN	    20
3244	欧阳锋	   程序员	   3088.0	3200	NaN     20
3251	张翠山	   程序员	   2056.0	4000	NaN	    20
3344	黄蓉	    销售主管   7800.0	3000	800.0	30
3577	杨过	    会计	     5566.0	  2200	  NaN	  10
3588	朱九真	   会计	    5566.0	 2500	 NaN	 10
4466	苗人凤	   销售员	   3344.0	2500	NaN	    30
5234	郭靖	    出纳	     5566.0	  2000	  NaN	  10
5566	宋远桥	   会计师	   7800.0	4000	1000.0	10
7800	张三丰	   总裁	    NaN      9000	 1200.0	 20

# 员工表（emp2_df），跟上面的员工表结构相同，但是保存了不同的员工数据。
eno    ename    job    mgr     sal      comm    dno
9800	骆昊	   架构师	7800	30000	 5000	 20
9900	王小刀	  程序员  9800	   10000	1200	20
9700	王大锤	  程序员  9800    8000 	600	    20
```

> 在数据库中`mgr`和`comm`两个列的数据类型是`int`，但是因为有缺失值（空值），读取到`DataFrame`之后，列的数据类型变成了`float`，因为我们通常会用`float`类型的`NaN`来表示空值。

| 属性名         | 说明                                |
| -------------- | ----------------------------------- |
| `at` / `iat`   | 通过标签获取`DataFrame`中的单个值。 |
| `columns`      | `DataFrame`对象的列索引列表         |
| `index`        | `DataFrame`对象的行索引列表         |
| `dtypes`       | `DataFrame`对象每一列的数据类型     |
| `empty`        | `DataFrame`对象是否为空             |
| `loc` / `iloc` | 通过标签获取`DataFrame`中的一组值。 |
| `ix`           |                                     |
| `ndim`         | `DataFrame`对象的维度               |
| `shape`        | `DataFrame`对象的形状（行数和列数） |
| `size`         | `DataFrame`对象中元素的个数         |
| `values`       | `DataFrame`对象的数据对应的二维数组 |
| `T`            | 转置                                |

```python
# loc / iloc
# 使用loc:只能指定行列索引的名字
data.loc['2018-02-27':'2018-02-22', 'open']
data.loc[data.index[0:4], ['open', 'close', 'high', 'low']]
# 使用iloc可以通过索引的下标去获取
data.iloc[0:4, data.columns.get_indexer(['open', 'close', 'high', 'low'])]
# 获取前100天数据的'open'列的结果
data.iloc[0:100, 0:2].head()

# 使用ix进行下表和名称组合做引
data.ix[0:4, ['open', 'close', 'high', 'low']]

# 修改行列索引值
stock_code = ["股票_" + str(i) for i in range(stock_day_rise.shape[0])]
data.index = stock_code  # 必须整体全部修改

#  重设索引
data.reset_index(drop=False)  # 设置新的下标索引,drop:默认为False，不删除原来索引，如果为True,删除原来的索引值

```

| 方法                                   | 说明                                                         |
| -------------------------------------- | ------------------------------------------------------------ |
| `info()`                               | 输出`DataFrame`的相关信息。                                  |
| `head(n)`                              | 获取前n行的数据，默认为5。                                   |
| `tail(n)`                              | 获取后n行的数据，默认为5。                                   |
| `reset_index(inplace=True,drop=False)` | 重置索引，drop=False表示不删除原本的索引。<br>参数`inplace`默认为False，表示所有操作不会修改原来`DataFrame`对象，而是将结果以一个新的`DataFrame`对象返回。若设置为True时，处理结果直接作用于原DataFrame，并返回空。 |
| `set_index(keys, drop=True)`           | 将某列设置为新索引，keys传入列索引名或列索引名的列表，drop可选传入boolean/True，默认为true设置为新索引，无视原索引。 |
| `sort_values(by=key, ascending=True)`  | 按照单个键或者多个键进行排序，by传单个键或键列表，ascending=False:降序 |
| `sort_index()`                         | 给索引进行排序。                                             |
| `concat([DataFrame1, DataFrame2,..])`  | 实现两个或多个纵向数据表拼接                                 |
| `merge()`                              | 实现两个或多个数据的横向合并                                 |

```python

# concat([DataFrame1, DataFrame2,..]) 的使用
all_emp_df = pd.concat([emp_df, emp2_df])

# 实现横向扩展需要先使用  reset_index()方法  重置索引列，
# 只需将其中参数inplace设置为True,注意充值操作直接作用于本表而不是返回新表  

all_emp_df.reset_index(inplace=True)  # 这样eno不再是索引而是一个普通列

# 然后才能使用merge()函数合并数据
pd.merge(dept_df, all_emp_df, how='inner', on='dno')
# 也可以直接调用DataFrame对象的merge方法

# 如果将how参数修改为left,运行结果比之前的输出多出了如下所示的一行
pd.merge(dept_df, all_emp_df, how='left', on='dno')
# 17  40  运维部  天津  NaN  NaN  NaN  NaN  NaN  NaN
# 因为 left 代表左外连接，也就意味着左表 dept_df 中的数据会被完整的查出来，但是在 all_emp_df 中又没有编号为 40 部门的员工，所以对应的位置都被填入了空值。
```

> `merge`函数的一个参数代表合并的左表、第二个参数代表合并的右表。合并结果与数据库中的表连接非常类似，所以上面代码中的`how`代表了合并两张表的方式，有`left`、`right`、`inner`、`outer`四个选项；而`on`则代表了基于哪个列实现表的合并，相当于 SQL 表连接中的连表条件，如果左右两表对应的列列名不同，可以用`left_on`和`right_on`参数取代`on`参数分别进行指定。

如果一个DataFrame具有多个索引列，这时DataFrame就变成了一个具有MultiIndex的DataFrame。

### 三、Index的类型





### 四、DataFrame运算

#### 1.算术运算

```python
# add(other)  加运算
data['open'].add(1)  # 该行全加1
# sub(other)  减运算
close = data['close']
open1 = data['open']
data['m_price_change'] = close.sub(open1)  # close-open1
```

#### 2.逻辑运算

```python
print(data['p_change'] > 2)  # 输出该列大于2的逻辑结果
data[data['p_change'] > 2]  # # 逻辑判断的结果可以作为筛选的依据
# 完成一个多个逻辑判断， 筛选p_change > 2并且open > 15
data[(data['p_change'] > 2) & (data['open'] > 15)]

# 还可以使用逻辑运算函数 
# query(expr) :expr:为 查询字符串
data.query("p_change > 2 & turnover > 15")
# isin(values)  判断是否在values中，values可以为序列或单个元素
# 可以指定值进行一个判断，从而进行筛选操作
data[data['turnover'].isin([4.19, 2.39])]
```

#### 3.统计运算

##### (1)综合分析

```python
data.describe()  # 能够直接得出很多统计结果,count, mean, std, min, max 等
```

![describe结果](imgs/describe结果.png)

##### (2)统计函数

| 函数     | 功能              |
| -------- | ----------------- |
| `count`  | 计数              |
| `sum`    | 求和              |
| `mean`   | 求平均值          |
| `median` | 求平均数          |
| `min`    | 求最小值          |
| `max`    | 求最大值          |
| `mode`   |                   |
| `abs`    | 求绝对值          |
| `prod`   | Product of values |
| `std`    | 求标准差          |
| `var`    | 求方差            |
| `idxmax` | 最大值索引        |
| `idxmin` | 最小值索引        |

#### 4.累计统计函数

| 函数      | 作用                        |
| --------- | --------------------------- |
| `cumsum`  | **计算前1/2/3/…/n个数的和** |
| `cummax`  | 计算前1/2/3/…/n个数的最大值 |
| `cummin`  | 计算前1/2/3/…/n个数的最小值 |
| `cumprod` | 计算前1/2/3/…/n个数的积     |

#### 5.自定义运算

```python
def apply(
    func,  # 自定义函数
    axis=0)# axis=0:默认是列，axis=1为行进行运算

data[['open', 'close']].apply(lambda x: x.max() - x.min(), axis=0)

open     22.74
close    22.85
dtype: float64
```

