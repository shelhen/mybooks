# 03.请求与响应对象

 `Request`和 `Response` 用于对网站进行爬网的对象，通常，`Request`对象自`spider`中生成并在整个框架中传递，直到其被传送到`download`，`download`执行发送命令并异步接收`response`对象，进一步在框架中传递。二者均有子类。

## 一、请求对象

### 1.请求基类`Request`

表示`HTTP`请求，该请求通常在框架中生成并由下载程序（`download`）执行，从而生成 `Response`。

```python
class scrapy.http.Request(*args, **kwargs)
```

#### （1）基本参数

| 参数       | 类型             | 说明                                                         |
| ---------- | ---------------- | ------------------------------------------------------------ |
| `url`      | `str`            | 该请求的URL，如果该URL无效，则将引发`ValueError`异常。       |
| `callback` | 函数对象或函数名 | 对应处理此请求对应的响应（存在）的类`parse`函数。            |
| `errback`  | 函数对象或函数名 | 处理过程中如果引发任何异常则调用该函数，包括`404 HTTP`错误等失败情形，它将`Failure` 作为第一个参数，有关详细信息可进一步阅读[errorback 的使用](https://docs.scrapy.org/en/latest/topics/request-response.html#topics-request-response-ref-errbacks)。 |
| `method`   | 可选自查         | 该请求对应的`HTTP`方法，默认为`GET`，其余还有如`POST`、`PUT`等。 |
| `meta`     | `dict`           | 该字典是`Request.meta` 的浅拷贝，对于新请求，此`dict`是空的，通常由不同的组件（扩展、中间件等）填充，详细可参阅[meta 参数](https://docs.scrapy.org/en/latest/topics/request-response.html#topics-request-meta)。 |
| `body`     | `bytes or str`   | 请求体，如果未给定，则存储空字节对象，如果传递了一个字符串，则将对其`encoding(utf-8)`。 |
| `headers`  | `dict or None`   | 字典的值可以是字符串(对于单值标头)或列表(对于多值标头)，如果传入`None`，则根本不会发送`HTTP`标头，注意通过`header`传入的`cookie`不会被`CookiesMiddleware`处理，如果需要传递`cookie`尽量通过`cookie`参数。 |
| `cookies`  | 字典或列表       | 请求的`cookies`，使用字典列表允许设置 `domain` 和 `path`属性。 |

```python
# 使用DICT
Request(url="http://www.example.com", cookies={'currency': 'USD', 'country': 'UY'})
# 使用字典列表
Request(url="http://www.example.com", cookies=[{'name': 'currency', 'value': 'USD', 'domain': 'example.com', 'path': '/currency'}])
```

**补充：`scrapy`的`cookie`管理机制**

在`Scrapy`中，添加`cookie`的方式共有三种，`Requests` 类可以直接使用 `cookies`参数 和 `headers.cookie` 两种方法添加 `cookie` ，同时 `CookiesMiddleware` 可以捕获来自`response.headers.set-cookie`中的`cookie`设置。

`settings`中存在设置`COOKIES_ENABLED`参数，其默认值为`True`（注释掉），该参数用于设置是否启用`CookiesMiddleware`。`CookiesMiddleware`用于整个框架的`cookie`管理，`CookiesMiddleware`的作用主要有，清空`headers.cookie`设置的`cookie`，捕获和管理来自`cookies`参数 和`set-cookie`中的`cookie`。

- 当设置`COOKIES_ENABLED=False`时，将关闭`CookiesMiddleware`，此时`response` 设置的 `cookie`和`cookies `参数设置的 `cookie`都将失效，由于不再受中间件影响，通过`headers`设置的`cookie`将生效。

- 当设置`COOKIES_ENABLED=True`时，启动 `CookiesMiddleware` 中间件，通过`headers`设置的`cookie`永远被屏蔽。此时中间件会自动捕获和管理来自于`set-cookie`和手动传递的`cookies`参数。
- `meta`参数中的`dont_merge_cookies`是请求级别的`COOKIES_ENABLED=False`，假如为一个请求传递了`dont_merge_cookies`，该请求将略过`CookiesMiddleware`，因此`cookies`参数和该请求对应的响应`cookie`都将失效，而通过`headers`设置的`cookie`将生效。

- `CookiesMiddleware`捕获的所有`cookie`将会以键值对的方式存储在框架`CookieJar`中，`CookieJar`的键来自于`meta`参数中的`cookiejar`参数。
  - 如果不在请求中传入`meta.cookiejar`，新请求无法找到同系列请求保存的来自于`response`的``cookie`信息，此时手动传入的`cookies`参数，相当于每个请求使用独立的`cookie`，这时如果需要获取来自`response`中的`cookie`，则需要手动捕获添加。
  - 如果在请求中传入`meta.cookiejar`，新请求在被发送之前会被`CookieMiddleWare`自动添加已保存的`cookie`信息，如果希望手动更新`cookie`信息，可以通过传递`cookies`参数实现，注意，后添加的`cookie`会自动覆盖先前添加的`cookie`，前提是`cookie`字段的`name`和`path`字段都相等时才会被替代，否则可能会创建两个同名不同`path`的`cookie`导致替代失效。

> 如果不希望本次请求的`cookie`受到到`CookieMiddleWare`干扰，则可以传递`dont_merge_cookies`参数，此后的`cookie`设置只能通过`headers`传递。

| 参数          | 类型   | 说明                                                         |
| ------------- | ------ | ------------------------------------------------------------ |
| `encoding`    | `str`  | 该请求对应的编码，默认为`utf-8`，该编码用于对`URL`和`body`编码。 |
| `priority`    | `int`  | 该请求的优先级，默认为`0`，调度程序使用优先级定义用于处理请求的顺序。 |
| `dont_filter` | `bool` | 调度程序`scheduler`不在筛选该请求，用于可能多次执行的请求，忽略重复（可能会导致循环），默认为`False`。 |
| `flags`       | `list` | 发送请求的标志，多用于日志。                                 |
| `cb_kwargs`   | `dict` | 具有任意数据的`dict`，将作为关键字参数传递到请求的回调。对于新请求，它为空，即默认情况下，回调只获取 `Response` 对象作为参数，该字典是`response.kw_kwargs`的浅拷贝。 |

#### （2）属性或方法

部分参数会被框架加载为属性，如`url`、`method`、`headers`、`bady`、`meta`、`cb_kwargs`，除此之外还有：

| 属性或方法           | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| `attributes`         | 类的所有公共属性的名称列表，主要用于`Request.replace()`、`Request.to_dict()`和`request_from_dict()`方法。 |
| `copy()`             | 返回原请求的潜拷贝副本。                                     |
| `replace()`          | 对原请求对象的部分属性进行直接修改，以请求属性为参数，注意，传入的参数是原本参数的浅拷贝。 |
| `from_curl(cur_str)` | 将一个`cURL`命令文本转换为`Scrapy`请求`Request`对象，[源码](https://www.osgeo.cn/scrapy/_modules/scrapy/http/request.html#Request.from_curl)。 |
| `to_dict(spider)`    | 返回包含请求数据的字典，它可以以某个`spider`为参数，此方法尝试找出用作回调和错误回调的爬行器方法的名称，并将它们包括在输出字典中，如果找不到它们，则引发异常。同理可以使用`request_from_dict()`将一个字典转换为`Request `对象。 |

#### （3）与请求有关的 api

`scrapy.utils.request`中包含了一系列用于处理请求的方法，可进一步详细了解。

```python
from scrapy.utils.request import (
    request_from_dict, # 将dict转化为request
    request_to_curl, # 将request 转化为 curl
    referer_str, # 提取request中headers的referer字段
    request_httprepr, # 获取raw HTTP 请求报文 (bytes)
    request_authenticate, # 为请求添加注册信息
    fingerprint, # 对请求进行hash 处理
    _serialize_headers, # 从请求中提取指定的值
)
```

`fingerprint(request, include_headers, keep_fragments)`方法：该方法用于根据`request`生成唯一的指纹，该方法在对`reuqest`进行哈希运算时并未考虑`URL fragments`和`URL query parameters`，并且部分特殊的请求头也可能不会影响`hash`指纹的生成，因此有时候需要扩展生成请求指纹的对比算法。

该函数包含三个参数，`include_headers`用于添加需要考虑的`headers`列表，`keep_fragments`用于指定是否考虑`URL query parameters`，也可以自定义`fingerprint`函数，如果想要自定义请求过滤算法，需要返回一个`WeakKeyDictionary`类。

```python
from hashlib import sha1
from weakref import WeakKeyDictionary
from scrapy.utils.python import to_bytes


def fingerprint(request) -> bytes:
    cache = WeakKeyDictionary()
    if request not in self.cache:
        fp = sha1()
        fp.update(to_bytes(request.method))
        fp.update(to_bytes(canonicalize_url(request.url)))
        fp.update(request.body or b"")
        cache[request] = fp.digest()
    return cache[request]
```

#### （4）请求指纹

请求指纹是对请求对象进行哈希生成的唯一的指纹，主要目的是过滤请求对象以避免重复，在绝大多数项目中我们无需考虑请求指纹，默认的过滤请求指纹的类能够适配大多数场景，但是，并不存在通用的生成请求指纹的方法，因为不同情形下对比请求的方法往往不同，因此有时需要自定义 请求指纹算法、自定义算法类等。

在`scrapy`中，可以通过`REQUEST_FINGERPRINTER_CLASS`指定用于生成请求指纹的算法，默认为`scrapy.utils.request`中的`RequestFingerprinter`类，它提取`url`、`method`和`body`内容生成`sha1`哈希值。

```python
class scrapy.utils.request.RequestFingerprinter(crawler: Optional[Crawler] = None)
```

除此外，通过`REQUEST_FINGERPRINTER_IMPLEMENTATION=2.7`设置算法版本，代表调用的函数为`scrapy.utils.request.fingerprint`，`2.6`目前已经废弃（代表调用函数为`scrapy.utils.request._request_fingerprint_as_bytes`），继续使用可能导致未知的问题。

```python
# 定义自己的请求指纹类：请求指纹类应该实现如下方法
 
class CustomRequestFingerprinter:

    @classmethod
    def from_crawler(cls, crawler):
        """
        该方法可选被设置，它允许你从 Crawler 配置文件中获取参数 来初始化一个 请求指纹生成类，
        用于为 请求指纹生成类 提供访问 scrapy 其他组件的接口，例如设置或信号。
        参数 Crawler 是 使用此请求指纹识别器的爬虫，
        """
        return cls(crawler)
    
	@classmethod
    def from_settings(cls, settings):
        """
        该方法可选被设置，该方法允许你从 setting 配置文件中获取参数 来初始化一个 请求指纹生成类
        用于为 请求指纹生成类 提供访问 setting 的接口，该方法必须返回一个
        """
         # 从 settings 中获取参数
        custom_param = settings.get('CUSTOM_PARAM', 'default_value')
        # 返回类的实例
        return cls(custom_param)
    
    def __init__(self, custom_param=None, crawler=None):
        custom_param = custom_param
        implementation = crawler.settings.get("REQUEST_FINGERPRINTER_IMPLEMENTATION")
        ...
    
    def fingerprint(self, request: Request):
        """
        返回一个 用于识别请求的 bytes object————该方法必须被实现。
        如下是默认的生成方法，来自于 scrapy.utils.request 模块。
        """
        fingerprint_byte = scrapy.utils.request.fingerprint(request)
        return fingerprint_byte
```

> 还可以通过`setting`中的`DUPEFILTER_CLASS`指定用于请求过滤的类，默认为`scrapy.dupefilters.RFPDupeFilter`，默认情况下`log`日志仅会输出第一次的重复过滤请求，如果需要每次都输出，可以增加参数`DUPEFILTER_DEBUG=True`

使用 `request fingerprints` 时还应注意，Scrapy中的一些组件要求请求指纹（request fingerprint）满足特定的格式要求：

- `scrapy.extensions.httpcache.FilesystemCacheStorage`（`HTTPCACHE_STORAGE`的默认值）

  - 哈希指纹的长度应至少为1 byte；

  - 文件系统的路径和文件名长度限制也适用。在`HTTPCACHE_DIR`目录下，会创建以下目录结构：

    - 以爬虫的名称命名的一级目录。
    - 请求指纹的第一个字节，以十六进制表示，作为二级目录。
    - 请求指纹的完整值，以十六进制表示，作为三级目录。
    - 文件名最长为16个字符。

    ```python
    # 假设一个请求指纹由20个字节组成（默认值），HTTPCACHE_DIR设置
    /home/user/project/.scrapy/httpcache/my_spider/01/0123456789abcdef0123456789abcdef01234567/response_headers
    ```

- `scrapy.extensions.httpcache.DbmCacheStorage`

  - 底层的`DBM`实现必须支持长度为请求指纹字节数两倍再加5的键。

  ```python
  # 例如，如果请求指纹由20个字节组成（默认值），则必须支持45字符长的键。
  20字节 * 2 + 5 = 45字符
  ```

#### （5）meta参数中特殊的键

| 键名                        | 说明                                                         |
| --------------------------- | ------------------------------------------------------------ |
| `bindaddress`               | 用于伪装成特定的IP地址进行爬取。                             |
| `cookiejar`                 | 指定请求应该使用哪个`cookiejar`。                            |
| `dont_merge_cookies`        | 请求级别`cookie`设置，控制请求是否经过`cookie`中间件处理。   |
| `dont_cache`                | 是否缓存响应。                                               |
| `dont_obey_robotstxt`       | 是否遵循`robot`协议。                                        |
| `dont_redirect`             | 是否支持重定向。                                             |
| `dont_retry`                | 是否重试。                                                   |
| `download_fail_on_dataloss` | 遇到数据丢失的情况时处理下载的响应的策略，值为`True`时将会将对应的请求标记为失败。 |
| `download_latency`          | 从请求开始到响应下载完成所花费的时间，即HTTP消息在网络上的传输时间，该键是只读的。 |
| `download_maxsize`          | 响应下载的最大大小（以字节为单位），若响应超过这个大小，会被丢弃。 |
| `download_warnsize`         | 响应下载的警告大小（以字节为单位），若响应超过这个大小，会记录一条警告。 |
| `download_timeout`          | 定义下载器在超时之前等待时间（以秒为单位）                   |
| `ftp_password`              | 用于FTP连接的密码。与全局设置`FTP_PASSWORD`相关。            |
| `ftp_user`                  | 用于FTP连接的用户名，与全局设置`FTP_USER`相关。              |
| `handle_httpstatus_all`     | 如果设置为`True`，则所有的HTTP状态码都会被视为有效的响应，并且会被处理。 |
| `handle_httpstatus_list`    | 一个包含HTTP状态码的列表，列表中的状态码会被视为有效的响应，并且会被处理。 |
| `max_retry_times`           | 设置每个请求的最大重试次数。其优先级高于全局设置`RETRY_TIMES`。 |
| `proxy`                     | 用于指定请求的代理服务器。                                   |
| `redirect_reasons`          | 一个包含重定向原因的列表，这些原因记录了请求重定向的原因。   |
| `redirect_urls`             | 一个包含请求重定向到的URL的列表。                            |
| `referrer_policy`           | 设置HTTP请求的`Referer`头的策略。这可以用来控制如何发送`Referer`信息。 |

> 所有请求级的参数设置优先级均高于 设置中

### 2.请求子类 FormRequest

该对象除了继承自`Request`的参数外，默认其提交`method`为`post`，增加了部分参数和属性。

```python
class scrapy.http.request.form.FormRequest
class scrapy.http.FormRequest
class scrapy.FormRequest
```

> 使用 `FormRequest`将自动设置请求头域 `Content-Type` 报头 `application/x-www-form-urlencoded` 。

#### （1）`formdata`参数

类型一般为字典或可为（键、值）元组，`formdata`中的数据将作为HTML表单根据请求方法的不同被URL编码为`queryString`或分配给请求`body`。

#### （2）类方法`from_response(cls, response,...)`

自定义一定的规则，从`response`中读取HTML表单字段，解析数据生成新的`FormRequest`对象。

```python
@classmethod
def from_response(
    response  # 包含用于预填充表单字段的HTML表单的响应对象
    [, 
     formname=None, formid=None, formxpath=None, formcss=None,  # str，若给定，将使用ID属性设置为该值的表单。
     formdata=None,  # 要在表单数据中重写的字段。如果响应中已存在字段 <form> 元素，其值将被此参数中传递的值重写。如果此参数中传递的值是 None ，即使响应中存在该字段，该字段也不会包含在请求中。
     clickdata=None,  # 用于查找submit 的属性，若为空，模拟单击第一个可单击元素。除了HTML属性之外，控件还可以通过其相对于表单内其他可提交输入的基于零的索引进行标识，方法是 nr 属性。
     # 如 {'name': 'ubmitBtn'}表示查询 name为 ubmitBtn 的提交按钮。
     dont_click=False,  # scrapy默认采用自动模拟单击方案，但有时候表单提交并不是源自于标签点击，如javascript提交，可以通过此选项禁用该行为。
     formdata=0, # 当响应中包含多个form 时，选择第n个form。
     ...
    ]
):pass
```

#### （3）案例-模拟登录

```python
import scrapy

def authentication_failed(response):
    # TODO: 核对响应内容，如果符合要求返回True，如果不符合返回False
    pass

class LoginSpider(scrapy.Spider):
    name = 'example.com'
    start_urls = ['http://www.example.com/users/login.php']

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={'username': 'john', 'password': 'secret'},
            callback=self.after_login
        )

    def after_login(self, response):
        if authentication_failed(response):
            self.logger.error("Login failed")
            return
```

### 3.请求子类 JsonRequest 

`JsonRequest`类扩展了 `Request` 类，具有处理`JSON`请求的功能，在参数层面扩展了`data`和`dumps_kwargs`方法。

```python
class scrapy.http.JsonRequest(
```

> 使用 `JsonRequest`将自动设置请求头域 `Content-Type` 报头到 `application/json` 和 `Accept` 报头到 `application/json, text/javascript, */*; q=0.01`。

#### （1）data参数

传递`json`对象或者字典，如果传入字典将自动将其序列化为json对象。如果提供了`Request.body`参数，`Request.method `将被自动地设置为`POST`。

#### （2）dumps_kwargs参数

传递给`json.dumps(**args)`方法的参数。

```python
import json
from scrapy import JsonRequest

data = {'key1': 'value1', 'key2': 'value2'}

# 设置一些自定义的 dumps 参数，比如缩进为 4 个空格
dumps_kwargs = {'indent': 4}

request = JsonRequest('https://example.com/api', data=data, dumps_kwargs=dumps_kwargs)
```

### 4.一些请求处理案例

#### （1）向回调函数中附加参数

```python
def parse(self, response):
    request = scrapy.Request('http://www.example.com/index.html',
                             callback=self.parse_page2,
                             cb_kwargs=dict(main_url=response.url))
    request.cb_kwargs['foo'] = 'bar'  # 向回调函数中增加更多参数。
    yield request

def parse_page2(self, response, main_url, foo):
    yield dict(main_url=main_url, other_url=response.url, foo=foo)
```

#### （2）使用`errbacks`捕获请求处理中的异常

```python
import scrapy
from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError

class ErrbackSpider(scrapy.Spider):
    name = "errback_example"
    start_urls = [
        "http://www.httpbin.org/",              # HTTP 200 expected
        "http://www.httpbin.org/status/404",    # Not found error
        "http://www.httpbin.org/status/500",    # server issue
        "http://www.httpbin.org:12345/",        # non-responding host, timeout expected
        "https://example.invalid/",             # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(u, callback=self.parse_httpbin,
                                    errback=self.errback_httpbin,
                                    dont_filter=True)

    def parse_httpbin(self, response):
        self.logger.info('Got successful response from {}'.format(response.url))

    def errback_httpbin(self, failure):
        if failure.check(HttpError):
            # 来自HttpError spider middleware 的异常
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)
```

#### （3）访问errback函数中的其他数据

在处理请求失败的情况下，也可以基于`cb_kwargs`访问回调函数的参数。

```python
def parse(self, response):
    request = scrapy.Request('http://www.example.com/index.html',
                             callback=self.parse_page2,
                             errback=self.errback_page2,
                             cb_kwargs=dict(main_url=response.url))
    yield request

def parse_page2(self, response, main_url):
    pass

def errback_page2(self, failure):
    yield dict(main_url=failure.request.cb_kwargs['main_url'])
```

#### （4）停止下载响应

当在`Scrapy`中使用信号`bytes_received`或`headers_received`时，可以通过在这些信号的处理函数中抛出`StopDownload`异常来停止特定响应的下载。这样做可以在下载过程中根据特定条件提前终止下载。

```python
import scrapy


class StopSpider(scrapy.Spider):
    name = "stop"
    start_urls = ["https://docs.scrapy.org/en/latest/"]

    @classmethod
    def from_crawler(cls, crawler):
        spider = super().from_crawler(crawler)
        crawler.signals.connect(spider.on_bytes_received, signal=scrapy.signals.bytes_received)
        return spider

    def parse(self, response):
        # 'last_chars' 用以说明该并未得到响应结果。
        yield {"len": len(response.text), "last_chars": response.text[-40:]}

    def on_bytes_received(self, data, request, spider):
        raise scrapy.exceptions.StopDownload(fail=False)
```

> 默认情况下，结果响应由相应的错误回复处理。

## 二、响应对象

表示`HTTP`响应的对象，通常下载(由`Downloader`)并提供给爬行器进行处理。

### 1.响应基类：Response

```python
class scrapy.http.Response(*args, **kwargs)
```

#### （1）基本常用参数

| 参数          | 类型    | 说明                                                         |
| ------------- | ------- | ------------------------------------------------------------ |
| `url`         | `str`   | 此响应的`URL`。                                              |
| `status`      | `int`   | 响应的`HTTP`状态码，默认为 `200`。                           |
| `headers`     | `dict`  | 对应响应头，`dict`值可以是字符串（对于单值头）或列表（对于多值头）。 |
| `body`        | `bytes` | 响应体，对应`requests`的响应体。                             |
| `flags`       | `list`  | 用于标记响应对象的标签，其内容将会被日志`log`记录，例如`'cached'`, `'redirected`’的输出。 |
| `Request`     | `--`    | 该响应对应的请求体。                                         |
| `certificate` | `--`    | 表示服务器的`SSL`证书的对象。                                |
| `ip_address`  | `--`    | 发出响应的服务器的`IP`地址。                                 |
| `protocol`    | `str`   | 用于下载响应的协议，如`HTTP/1.0`，`HTTP/1.1`                 |
| `selector`    | `--`    | 返回一个来自于`Scrapy`的`Selector(response.text)`选择器对象，该对象支持`css`、`xpath`和`re`方法。 |
| `encoding`    | `str`   | 来自于参数`encoding`，`TextResponse`按照如下逻辑解决编码问题：首先尝试参数`encoding`解码，失败后则开始尝试来自于`HTTP`标头中声明的编码，随后尝试响应正文中声明的编码（注意`TextResponse`可能并没有相应正文），最后尝试根据响应主体推断编码，如都失败，则会引发异常。 |
| `text`        | `--`    | 类似于`response.body.decode(response.encoding)`。            |
| `attributes`  | `list`  | 返回属性列表：` ('url', 'status', 'headers', ..., 'encoding')` |

#### （2）常见属性或方法

部分参数会被框架加载为属性，如`url`、`status`、`headers`、`body`、`flags`、`Request`、`certificate`、`ip_address`、`protocol`，除此之外还有，值得注意的是`response`的属性一般是只读的，不支持修改，如需强制修改，需要用到`response.replace()`方法。

| 属性或方法         | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| `meta`             | 来自于`Request.meta`的属性，与之不同的是， `Response.meta` 属性沿着重定向和重试传播，因此其来自于`spider`。 |
| `cb_kwargs`        | 来自于`Request.cb_kwargs`的属性，与之不同的是， `Response.cb_kwargs` 属性沿着重定向和重试传播，因此其来自于`spider`。 |
| `attributes`       | 包含类的所有公共属性的名称。                                 |
| `copy()`           | 返回此响应的副本的潜拷贝。                                   |
| `replace()`        | 同`request.replace()`，用于修改响应对象的属性。              |
| `urljoin(url)`     | 通过组合响应的 `url `有一个可能的相对`URL`，其源自于`urllib.parse.urljoin(response.url, url)` |
| `xpath(query)`     | `TextResponse.selector.xpath(query)`                         |
| `css(query)`       | `TextResponse.selector.css(query)`                           |
| `re()`             | `TextResponse.selector.re(query)`                            |
| `jmespath(query)`  | `TextResponse.selector.jmespath(query`                       |
| `json()`           | 将`json`相应字符串转化为`python`对象。                       |
| `follow(url)`      | 根据`response`和`url`构建一个新的`request`对象并传递给引擎，与一般的`rquest`不同的是其`url`参数支持绝对/相对URL和链接`scrapy.link.Link`对象。 |
| `follow_all(urls)` | 根据`response`和`urls`构建一个请求对象的`iterable`，与一般的`rquest`不同的是其`url`参数支持绝对/相对URL和链接`scrapy.link.Link`对象。 |

### 2.文本响应子类：TextResponse

`TextResponse`对象将编码功能添加到基`Response` 类，它只用于二进制数据，如图像、声音或任何媒体文件，因此除了以上参数外，其额外添加了其他参数和属性：

```python
class scrapy.http.TextResponse(
    url, 
    encoding,  # 默认为`None`，带有此响应编码的字符串，用于此响应的编码的字符串。
    ...[[]]
)
```

- `follow(url)`方法

可以快捷根据`response`生成一个新的`request`，它可以接受如下参数：

```python
follow(url, callback=None, method='GET', headers=None, body=None, cookies=None, meta=None, encoding=None, priority=0, dont_filter=False, errback=None, cb_kwargs=None, flags=None)
```

根据`response`和`url`构建一个新的`request`对象并传递给引擎，与一般的`rquest`不同的是其`url`参数支持绝对/相对URL和链接`scrapy.link.Link`对象。

> 与`response.follow()`不同，其`url`额外支持`Selector` 对象，如`response.css('a::attr(href)')[0]`和`response.xpath('//img/@src')[0]`。

- `follow_all(links)`

同`follow()`方法，该方法可以跟踪多个链接，快速生成`request`的迭代器。

> 注意`TextResponse`中的`follow_all()`和`follow()`除了支持绝对/相对URL和链接对象之外还支持选择器。
>

### 3.HTML响应子类：HTMLResponse

```python
class scrapy.http.HtmlResponse(url[, ...])
```

该类可以通过查看`HTML`编码自动处理`meta`和`http-equiv`属性。

### 4.XML响应子类：XmlResponse

```python
class scrapy.http.XmlResponse(url[, ...])
```

它通过查看XML声明行添加了编码自动发现支持。

## 三、响应解析

当进行网页抓取时，最常见的工作是从响应源代码中提取数据。为了实现这一目的，有多个库可供使用，比如`lxml`、`bs4`、`re`、`parsel`、`json`、`jsonpath`等，其中比较关键的共性是`xpath`语法、`css`选择器、正则表达式和`jsonpath`语法。`scrapy`基于`parsel`模块将响应结果封装为选择器`Selector`，其可以方便的调用各种语法提取数据且又有较高的效率。

### 1.基本使用

`Response`对象自身可以通过`selector`属性调用``xpath()`/`re()`/`css()`方法，也可以直接调用自身方法，选择器子元素仍能够调用这些方法，`Selector`将自动选择最优的方式解析`XML`或`HTML`字符串，分别对应`lxml`中的`HTML parsel`对象和`XML Parsel`对象。

```python
from scrapy.selector import Selector

# 直接调用
response.selector.xpath("//span/text()").get()
response.xpath("//span/text()").get()
response.css("span::text").get()
# 这里的re 等价于 re.searchall()
response.xpath('//a[contains(@href, "image")]/text()').re(r"Name:\s*(.*)")
# re_first则等等价于 re.search()
response.xpath('//a[contains(@href, "image")]/text()').re_first(r"Name:\s*(.*)")
# 手动编译
body = "<html><body><span>good</span></body></html>"
Selector(text=body).xpath("//span/text()").get()

# 选择器对象具有 .attrib属性
response.css("base").attrib["href"]  # 'http://example.com/'
response.css("img").attrib["src"]  # 'image1_thumb.jpg'
# 当 选择结果为 response 为 Selector对象时，attrib属性返回节点的所有属性名及属性值构成的字典
# 当 选择结果为 response 为 SelectorList对象时，attrib属性返回匹配结果的第一个节点 所有属性名及属性值构成的字典
# 当选择节点不存在任何属性时，attrib属性 返回一个空字典。

# xpath方法/css方法/re方法/.../返回的选择器对象是相同的，可以方便的联合使用。
sel.css(".shout").xpath("./time/@datetime").getall()
```

> 注意：使用`response.selector`或者`Selector(text=body)`可以保证编译只进行一次，从而提升代码效率。

### 2.`get()`与`getall()`

使用选择器对象的`get()`方法或`getall()`从选择器中提取选择结果，其中`get()`方法返回匹配到的第一个结果，`getall()`则返回匹配到的所有结果组成的列表，当匹配结果为空时，`get()`返回`None`，`getall()`则返回空列表。

```python
response.css("title::text").get(default="")  # get()方法可以手动设置默认值
# 'Example website'
response.css("img").xpath("@src").getall()
# ['image1_thumb.jpg', ..., 'image5_thumb.jpg']
```

> 或许能在一些老旧的项目中看到`extract_first()`和`extract()`语法，其对应关系为`SelectorList.get()`等价于`SelectorList.extract_first()`，`SelectorList.getall()`等价于`SelectorList.extract()`，`Selector.get()`等价于`Selector.extract()`，另外还有独立的`Selector.getall()`。与`get()`和`getall()`相比，`extract_first()`根据主体的不同返回列表或首个元素，`extract()`亦然，而`get()`总是返回单个元素，`getall()`则总是返回列表，相对更清晰。

> 如果没有合适的匹配结果，`get`方法将返回一个`None`，`getall()`方法则会返回一个`[]`。

### 3.语法扩展

#### （1）`css`选择器扩展

根据`W3C`标准，`css`选择器不支持直接选择文本内容和属性值，但是在爬虫开发过程中，选择如上数据是非常必要的，因此`parsel`模块对`css`语法进行了扩展，增加了`::text`用于选择文本节点，`::attr(name)`用于选择属性值。

#### （2）`xpath`语法扩展

- `contains`方法

匹配 某个[someclass] 字符串 处于某个属性[class]中 的节点。

```python
//*[contains(@class, 'someclass')]  
# 如果想要匹配一个包含固定内容的文本
# 传入一个`.`即可
//*[contains(., 'Next Page')]
```

- `starts-with()` 方法  

匹配 某个属性 以 某个字符串开头。

```python
//*[starts-with(@class, 'someclass')] 
# 匹配文本同样
//*[starts-with(., 'Next Page')]
```

- `string()` 方法

选择处匹配节点中的所有文本，并使用 空格将其连接起来。

```python
sel = Selector(text='<a href="#">Click here to go to the <strong>Next Page</strong></a>')
print(sel.xpath("//a//text()").getall())  # 选择节点中的所有文本：['Click here to go to the ', 'Next Page']
print(sel.xpath("string(//a[1]//text())").getall())  # ['Click here to go to the ']
print(sel.xpath("//a[1]").getall())  # 选择的是节点 ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
print(sel.xpath("string(//a[1])").getall()) # ['Click here to go to the Next Page']
```

- `concat()`方法

```python

```

- 使用变量

`XPath`允许`$somevariable语法`在XPath表达式中引用变量。

```python
# 将查找的属性写为一个变量
response.xpath("//div[@id=$val]/a/text()", val="images").get()
# 再扩展函数中使用变量，如下寻找一个 包含5个子元素a的div节点 的 id
response.xpath("//div[count(a)=$cnt]/@id", cnt=5).get()
```

-  EXSLT 扩展

```python
# re:test() 可以直接再xpath 语法中使用 re表达式
//li[re:test(@class, "item-\d$")]//@href
# 查找 itemprop 节点 并排除 处于其他 itemprop 中的元素
set:difference(./descendant::*/@itemprop,.//*[@itemscope]/*/@itemprop)
```

> 基于`c`的`libxslt`并不支持` EXSLT`语法扩展，因此上述语法基于`re`模块实现，其性能并不好。

- `has-class`语法

```python
# 查找包含 某个 classname 的节点，含有多个class 的要全部按照空格传入
"""
<p class="foo bar-baz">First</p>
<p class="foo">Second</p>
<p class="bar">Third</p>
<p>Fourth</p>
"""
//p[has-class("foo"")]
//p[has-class("foo", "bar-baz")]
```

> 该函数也是纯`python`实现的，性能较差。

- 自定义`xpath`函数

```python
from parsel.xpathfuncs import set_xpathfunc

def set_xpathfunc(fname: str, func: Optional[Callable])->None:
    """
    在 XPath 表达式中注册一个自定义的扩展函数，自定义函数会在每个匹配的节点上被调用，并可以接收到上下文参数和 XPath 表达式中传递的参数。
    参数：name-自定义函数名称
    参数：func-函数对象
    """
    ns_fns = etree.FunctionNamespace(None)
    if func is not None:
        ns_fns[fname] = func
    else:
        del ns_fns[fname]
```

使用案例

```python
hello = lambda context, a: "Hello %s" % a
loadsofargs = lambda context, *args: "Got %d arguments." % len(args)

node = Selector('<a><b>Haegar</b></a>')
node.xpath('hello("shelhen")') # Hello shelhen.
node.xpath('countargs(., b, ./*)')  # Got 3 arguments.
```

如果需要了解更多可以参考[lxml文档](https://lxml.de/extensions.html#xpath-extension-functions)，其中上文中`re`、`set`和`has-class`方法就是基于上述接口实现的，也可以参考上述函数。

### 4.内置选择器

#### （1）`Selector`对象

```python
class scrapy.selector.Selector(*args: Any, **kwargs: Any)
```

`Selector`对象是响应对象的包装器，用于基于某种方式解析其部分内容，其可以传入如下初始化参数：

- `Response`：直接传入`HtmlResponse`或`XmlResponse`对象。

- `text`中可以直接传入 `utf-8`或`unicode`编码的字符串。
- `type`用于定义选择器的类别，如果不传入，默认为`None`，它可以是`"html"`, `"xml"`, `"json"`和`"None"`，如果为`None`，它自动选择最合适的`type`，如果传入了，就强制按照传入参数解析。

| 属性或方法                                             | 说明                                                         |
| ------------------------------------------------------ | ------------------------------------------------------------ |
| `xpath(query, namespaces,**kwargs)`                    | 查找与`xpath`查询匹配的节点，并将结果作为`SelectorList`实例返回，其中所有元素都被扁平化为一维，`namespaces`参数用于注册将在`xptah`语法中使用的命名空间，其理论上应该是一个映射，常用于传递`xpath`变量。 |
| `css(query)`                                           | 应用给定的`CSS`选择语法并返回`SelectorList`实例。            |
| `jmespath(query, **kwargs)`                            | 应用给定的`JMESPath`查询语法没并返回`SelectorList`实例，任何其他命名参数都会传递给底层`jmespath.search`调用。 |
| `re(regex, replace_entities=True)`                     | 应用给定的正则表达式并返回匹配的字符串列表，正则表达式既可以是编译后的正则表达式，也可以是使用`re.compile（regex）`编译为正则表达式的字符串，字符实体引用默认（`replace_entities=True`）将被其相应的字符替换（`&amp；`和`&lt；`除外）。 |
| `re_first(regex, default=None, replace_entities=True)` | 应用给定的正则表达式并返回第一个匹配的字符串。如果不匹配，则返回默认值（如果未提供参数，则返回None）。 |
| `register_namespace(prefix, uri)`                      | 注册要在此选择器中使用的给定命名空间，如果不注册命名空间，则无法从非标准命名空间中选择或提取数据，[XML响应的选择器](https://docs.scrapy.org/en/latest/topics/selectors.html#selector-examples-xml)。 |
| `remove_namespaces()`                                  | 删除所有名称空间，并允许使用无名称空间的xpath遍历文档。      |
| `get(default=None)`                                    | 序列化并返回匹配的节点，对于HTML和XML，结果始终是一个字符串，并且百分比编码内容未加引号。 |
| `getall()`                                             | 序列化并返回元素字符串列表中的匹配节点，它在`SelectorList`中更有用。 |
| `attrib`                                               | 返回底层元素的属性字典。                                     |
| `__bool__`                                             | 选择了任何真实内容，则返回`True`，否则返回`False`。          |

#### （2）`SelectorList`对象

```python
class scrapy.selector.SelectorList(iterable=(), /)
```

`SelectorList`类是内置列表类的一个子类，它提供了一些额外的方法。

| 属性或方法                                             | 说明                                                         |
| ------------------------------------------------------ | ------------------------------------------------------------ |
| `xpath(query, namespaces,**kwargs)`                    | 对其中的每一个`Selector`对象都调用`xpath()`，会将查询结果扁平化为一维度。 |
| `css(query)`                                           | 对其中的每一个`Selector`对象都调用`css()`。                  |
| `jmespath(query, **kwargs)`                            | 对其中的每一个`Selector`对象都调用`jmespath()`。             |
| `re(regex, replace_entities=True)`                     | 对其中的每一个`Selector`对象都调用`re()`。                   |
| `re_first(regex, default=None, replace_entities=True)` | 对匹配到的第一个元素应用`re()`。                             |
| `get(default=None)`                                    | 对匹配到的第一个元素应用`get()`方法。                        |
| `getall()`                                             | 对匹配到的每个元素都调用`get()`方法，将返回一个匹配结果字符串构成的列表。 |
| `attrib`                                               | 对匹配到的第一个元素应用`attrib`。                           |

### 5.其他便利方法

#### （1）移除`namespaces`

```python
Selector.remove_namespaces()

删除命名空间需要迭代和修改文档中的所有节点，默认情况下，对于Scrapy抓取的所有文档来说，这是一个相当昂贵的操作。在某些情况下，可能确实需要使用命名空间，以防命名空间之间的某些元素名称冲突。然而，这些病例非常罕见。
```

> 移除`namespaces`需要手动调用，因为移除`namespaces`需要对整个文档遍历，这比较费时，而且使用命名空间避免元素名称冲突的案例比较罕见，因此框架本身并没有默认调用该函数。

### 6.扩展阅读

https://jmespath.org/

http://www.zvon.org/comp/r/tut-XPath_1.html



