# 12.Scarpy对接Driver

`scrapy`内部集成了`Request`请求对象，理论上可以很方便的实现任何请求，但有时我们更希望使用`Driver`来解决各种验证码问题、`cookie`问题，而不是对一些反爬手段进行算法还原，借助驱动浏览器，有时可以更方便的获取想要的数据。

## 一、对接原理

在`Downloader Middleware`中存在三个框架自动调用的方法，分别是`process_request`、`process_response`、`process_exception`，其中当`process_request`方法返回为`Response` 对象时，低级的下载器中间件将不会再处理该请求，且生成的响应将直接依次通过高优先级的下载器中间件的`process_response`方法，最终将该响应通过引擎返回给`spider`处理。

> 一旦我们在中间件`process_request`中构造`Response`对象并返回，对应的请求不会在传递给下载器。

因此，可以尝试重写一个中间件并实现其`process_request`方法，在该方法中直接获取`URL`通过`driver`访问链接，并构造响应对象返回给框架即可。

## 二、下载器中间件扩展案例

### 1.扩展`selenium-chrome`

```python
import warnings
from scrapy import signals
from scrapy.http import HtmlResponse
from selenium.webdriver import Chrome, ChromeOptions, ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.proxy import Proxy, ProxyType


class SeleniumChromeDownloaderMiddleware:

    def __init__(self, selenium_driver_path, driver_headless, **kwargs):
        self.stealth_min_path = kwargs.get("stealth_min_path")
        self.window_size = kwargs.get("window_size")
        self.download_timeout = kwargs.get("download_timeout")
        self.executable_path = selenium_driver_path
        self.headless = driver_headless
        self.driver = None

    @classmethod
    def from_settings(cls, settings):
        selenium_driver_path = settings.get('SELENIUM_EXECUTABLE_PATH')
        driver_headless = settings.getbool("SELENIUM_DRIVER_HEADLESS", False)
        window_size = settings.gettumple("SELENIUM_WINDOW_SIZE", (1280, 720))
        stealth_min_path = settings.get("SELENIUM_STEALTH_MIN_PATH", '')
        download_timeout = settings.get("DOWNLOAD_TIMEOUT", 60)
        return cls(
            selenium_driver_path=selenium_driver_path,
            driver_headless=driver_headless,
            window_size=window_size,
            stealth_min_path=stealth_min_path,
            download_timeout=download_timeout
        )

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls.from_settings(crawler.settings)
        # 如果爬虫关闭则调用spider_closed方法
        crawler.signals.connect(s.spider_closed, signal=signals.spider_closed)
        # 如果爬虫开启则调用 spider_opened方法
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        """在这里实现具体逻辑"""
        url = request.url
        self.driver.get(url)
        wait = WebDriverWait(self.driver, 10)
        wait.until(EC.presence_of_element_located(
            (By.CLASS_NAME, 'recruit-list')
        ))
        return HtmlResponse(url=url,
                            body=self.driver.page_source,
                            request=request,
                            encoding='utf-8',
                            status=200)

    def load_stealth_options(self):
        options = ChromeOptions()
        # 禁用 "Chrome is being controlled by automated software" 横幅
        options.add_experimental_option('excludeSwitches', ['enable-automation'])
        options.add_experimental_option('useAutomationExtension', False)
        # 隐藏webdriver特征
        options.add_argument('--disable-blink-features=AutomationControlled')
        # 要求图形化显示环境（非必须，根据需要启用）
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        # 完全依赖于硬件加速，有利于优化代码。
        options.add_argument("--disable-software-rasterizer")
        # 禁用gpu
        options.add_argument('--disable-gpu')
        # 设置User-Agent:可以考虑与scrapy 中间件协同
        options.add_argument('user-agent={0}'.format(
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'))
        # if self.proxy:
        #     proxy_auth_plugin_path = create_proxy_extension()
        #     options.add_extension(proxy_auth_plugin_path)

        options.add_argument("--start-maximized")
        if self.headless:
            # 添加无界面模式（headless，非必须，根据需要启用）
            options.add_argument('--headless')
        # 设置超时时间
        options.timeouts = {
            'script': 5000,  # 脚本最大加载时间
            'pageLoad': int(self.download_timeout)*1e3,  # 页面最大加载时间
            'implicit': 5000  # 默认隐式等待时间
        }
        # 用户偏好
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        return options

    def spider_opened(self, spider):
        source = """
        ...
        """
        options = self.load_stealth_options()
        service = ChromeService(executable_path=self.executable_path)
        self.driver = Chrome(service=service, options=options)
        # 执行Chrome开发者协议命令（在加载页面时执行指定的JavaScript代码）
        self.driver.execute_cdp_cmd(
            'Page.addScriptToEvaluateOnNewDocument', {'source': source}
        )
        if self.stealth_min_path:
            try:
                with open(self.stealth_min_path, 'r') as f:
                    source = f.read()
                # 隐藏 webdriver 特征
                self.driver.execute_cdp_cmd(
                    'Page.addScriptToEvaluateOnNewDocument', {"source": source}
                )
            except Exception as e:
                warnings.warn('Cannot load stealth_min_path.js, reason:{0}' % e)
        # 配置窗口大小
        self.driver.set_window_size(self.window_size[0], self.window_size[1])
        # self.driver.maximize_window()

    def spider_closed(self):
        """
        callback when spider closed
        :return:
        """
        if self.driver:
            self.driver.close()

    def __del__(self):
        if self.driver:
            # self.driver.quit()
            self.driver.close()
```

如上代码仅仅对`driver`进行了简单的封装，功能还是比较粗糙的，比如没有配置`proxy`，没有实现异常处理，没有写导出`cookie`的方法，也无法执行`javascript`，整个爬取过程也是阻塞式，没有发挥`scrapy`的优势。有人也对该中间件进行了更好的封装，可尝试下载如下模块观察其使用过程。

### 2.扩展`pyppeteer`

```python
from pyppeteer import launch
from scrapy.http import HtmlResponse
import asyncio
import logging
from twisted.internet.defer import Deferred

logging.getLogger('websockets').setLevel('INFO' )
logging.getLogger('pyppeteer').setLevel('INFO')


def as_deferred(f):
    return Deferred.fromFuture(asyncio.ensure_future(f))


class PyppeteerMiddleware(object):
    async def _process_request(self, request, spider):
        browser = await launch(headers=False)
        page = await browser.newPage()
        pyppeteer_response = await page.goto(request.url)
        await asyncio.sleep(5)
        html = await page.content()
        pyppeteer_response.headers.pop('content-encoding', None)
        pyppeteer_response.headers.pop('Content-Encoding', None)
        response =HtmlResponse(
            page.url,
            status=pyppeteer_response.status,
            headers=pyppeteer_response.headers,
            body=str.encode(html),
            encoding='utf-8',
            request=request
        )
        return response

    def process_request(self, request, spider):
        return as_deferred(self.process_request(request, spider))
```

如需更多自定义功能，可以参考`gerapy-pyppeteer`，直接使用`pip`安装即可。

### 3.扩展`splash`

`Splash`是一个`Javascript`渲染服务。它是一个实现了`HTTP API`的轻量级浏览器，`Splash`是用`Python`和`Lua`语言实现的，基于`Twisted`和`QT`等模块构建。使用`scrapy-splash`最终拿到的`response`相当于是在浏览器全部渲染完成以后的网页源代码。

splash官方文档 https://splash.readthedocs.io/en/stable/

#### （1）环境安装

`splash`依赖环境略微复杂，可以直接使用`splash`的`docker`镜像，如果不使用docker镜像请参考 [splash官方文档](https://github.com/scrapinghub/splash/blob/master/Dockerfile) 安装相应的依赖环境。

```python
# splash的dockerfile
https://github.com/scrapinghub/splash/blob/master/Dockerfile
```

 依次执行如下命令

```shell
# 获取镜像
sudo docker pull scrapinghub/splash
# 运行docker服务
sudo docker run -p 8050:8050 scrapinghub/splash
sudo docker run -d -p 8050:8050 scrapinghub/splash
# 访问 http://127.0.0.1:8050，观察访问结果。
```

若遇到下载超时，尝试改变镜像源到国内：

```shell
sudo vi /etc/docker/daemon.json
"""
{"registry-mirrors": ["https://registry.docker-cn.com"]}
"""
# 修改后需要重启docker服务
```

关闭docker服务

```shell
sudo docker ps -a
sudo docker stop CONTAINER_ID
sudo docker rm CONTAINER_ID
```

安装python包：

```python
pip install scrapy-splash
```

#### （2）在scrapy直接使用`scrapy-splash`

设置：

```python
# 渲染服务的url
SPLASH_URL = 'http://127.0.0.1:8050'
# 下载器中间件
DOWNLOADER_MIDDLEWARES = {
    'scrapy_splash.SplashCookiesMiddleware': 723,
    'scrapy_splash.SplashMiddleware': 725,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}
# 去重过滤器
# DUPEFILTER_CLASS = 'scrapy_splash.SplashAwareDupeFilter'
# DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter" # 指纹生成以及去重类
DUPEFILTER_CLASS = 'test_splash.spiders.splash_and_redis.SplashAwareDupeFilter' # 混合去重类的位置

# 使用Splash的Http缓存
HTTPCACHE_STORAGE = 'scrapy_splash.SplashAwareFSCacheStorage'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False
```

> 该组件配置了`DUPEFILTER_CLASS`，与`scrapy-redis`冲突，若想兼容二者，可尝试重写过滤器类。

```python
from __future__ import absolute_import

from copy import deepcopy
from scrapy.utils.request import request_fingerprint
from scrapy.utils.url import canonicalize_url
from scrapy_splash.utils import dict_hash
from scrapy_redis.dupefilter import RFPDupeFilter


def splash_request_fingerprint(request, include_headers=None):
    """ Request fingerprint which takes 'splash' meta key into account """

    fp = request_fingerprint(request, include_headers=include_headers)
    if 'splash' not in request.meta:
        return fp

    splash_options = deepcopy(request.meta['splash'])
    args = splash_options.setdefault('args', {})

    if 'url' in args:
        args['url'] = canonicalize_url(args['url'], keep_fragments=True)

    return dict_hash(splash_options, fp)


class SplashAwareDupeFilter(RFPDupeFilter):
    """
    DupeFilter that takes 'splash' meta key in account.
    It should be used with SplashMiddleware.
    """
    def request_fingerprint(self, request):
        return splash_request_fingerprint(request)
```

爬虫代码：

```python
from scrapy_redis.spiders import RedisSpider
from scrapy_splash import SplashRequest


class SplashAndRedisSpider(RedisSpider):
    name = 'splash_and_redis'
    allowed_domains = ['baidu.com']

    # start_urls = ['https://www.baidu.com/s?wd=13161933309']
    redis_key = 'splash_and_redis'
    # lpush splash_and_redis 'https://www.baidu.com'

    # 分布式的起始的url不能使用splash服务!
    # 需要重写dupefilter去重类!

    def parse(self, response):
        yield SplashRequest('https://www.baidu.com/s?wd=13161933309',
                            callback=self.parse_splash,
                            args={'wait': 10}, # 最大超时时间，单位：秒
                            endpoint='render.html') # 使用splash服务的固定参数

    def parse_splash(self, response):
        with open('splash_and_redis.html', 'w') as f:
            f.write(response.body.decode())
```

`splash`类似`selenium`，能够像浏览器一样访问请求对象中的`url`地址，相对于`selenium`更轻量，事实上上面的`selenium-scrapy`也是参考`scrapy-splash`开发的。
