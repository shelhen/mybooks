## 02.崔庆才电影网站逆向爬虫案例

https://spa6.scrape.center/

### 一、纯代码逆向

> 希望自己使用单纯扣代码方式实现。

#### 1.需求分析

##### 1）功能需求

爬取所有电影信息存储到`json`文档中，爬取内容同第一次，包括==电影名称，封面，类别，上映时间，评分，剧情简介==。

##### 2）抓包分析

列表页数据以`json`形式返回，其中仅仅包含一个加密参数`token`，`limit=10`定值/`offset=0`控制翻页。

https://spa6.scrape.center/api/movie/

```
limit=10
offset=0
token=
```

加密参数明显不是`base64`：`ZGVkM2EyNWY4MDQxMjM3ZTJlMGE0MGQ5OTNhMDUzM2FhNGNjZDEzOSwxNjkwMjY3MTcz`

详情页`url`：`/detail/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx`

说明网站还对电影的Id进行了加密，进入详情页后，又返回了一个`json`包，其中包含了所有数据，接口如下：

```
https://spa6.scrape.center/api/movie/ZWYzNCN0ZXVxMGJ0dWEjKC01N3cxcTVvNS0takA5OHh5Z2ltbHlmeHMqLSFpLTAtbWIx/?token=ZjYxYTkyODQwNjkwNDhhOGJmNWU4MmM5N2RjMTQ3MDNhMzkzY2Q3MSwxNjkwMjY3NDkw
```

##### 3）总体思路总结

因此总体思路就有了，需要计算三个`token`，第一个获取商品列表的token，具体还需要进一步分析，第二个前往商品详情页的token，第三个获取详情数据的token。

关键请求第三个，只要完成他，所有数据就可以拿到了。

想要实现第三个请求，需要第二个请求参数，因此必定前往列表页面，列表页面的请求可能不需要。

#### 2.参数逆向

##### 1）列表页token

全局搜索token：

```
Object(_0x2fa7bd['a'])('/api/movie')
扣下来这个方法
```

```python
import requests
import time
import base64
import hashlib
from urllib.parse import quote,unquote

class CMSpider(object):
    def get_token(self, arguments:list):
        """
        :param arguments: ['api']
        :return : token
        """
        ts = str(int(time.time()))
        arguments.append(ts)
        sign = hashlib.sha1(','.join(arguments).encode('utf8')).hexdigest()
        return base64.b64encode(f"{sign},{ts}".encode('utf8')).decode('utf8')

    def get_key(self, id):
        iv = 'ef34#teuq0btua#(-57w1q5o5--j@98xygimlyfxs*-!i-0-mb' + str(id)
        return base64.b64encode(iv.encode('utf8')).decode('utf8')

    def parse(self, response):
        return {
            'id': response['id'],
            'name': response['name'],
            'cover':response['cover'],
            'categories':response['categories'],
            'directors':response['directors'][0]['name'],
            'score':response['score'],
            'drama':response['drama'],
            'published_at':response['published_at']
        }

    def main(self):
        # movies =[]
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
        }
        for i in range(100):
            api = f"/api/movie/{self.get_key(i+1)}"
            token = quote(self.get_token([api]))
            url = f"https://spa6.scrape.center{api}/?token={token}"
            movie = self.parse(requests.get(url, headers=headers,timeout=10).json())
            print(movie)
            # movies.append(movie)
        # print(movies)

    def test(self):
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36"
        }
        i = 10
        api = f"/api/movie/{self.get_key(i + 1)}"
        token = self.get_token([str(api), ])
        url = 'https://' + f"spa6.scrape.center{api}/?token={token}"
        movie = self.parse(requests.get(url, headers=headers, timeout=10).json())
        print(movie)


if __name__ == '__main__':
    cms = CMSpider()
    # cms.get_list_token(['/api/movie'])
    cms.main()
    # cms.test()
```

#### 3.代码实现







### 二、

> 希望自己结合驱动方式实现

1.下载原网页本地执行







2.直接处理在线网页







### 三、反思回顾