## 06.科学计算库应用——Pandas

### 一、数据清洗

通常，从 Excel、CSV 或数据库中获取到的数据并不是非常完美的，里面可能因为系统或人为的原因混入了重复值或异常值，也可能在某些字段上存在缺失值；并且`DataFrame`中的数据也可能存在格式不统一、量纲不统一等各种问题，因此要对数据进行清洗。

#### 1.缺失值处理

```python
# 1.查找缺失值
emp_df.isnull()  # 或 emp_df.isna() 
# 查询缺失值，该方法将一一查询每个值，将所有空值标记为True,非空值标记为False
emp_df.notnull()  # 或 notna()
# 将非空的值标记为`True`。

# 2.删除缺失值
# 删除缺失值,axis决定沿着0轴（删除整行）还是1轴方向删除（删除整列）
emp_df.dropna(axis=0)

# 3.填充缺失值
emp_df.fillna(value=0)
# (1)填充空值时可以使用指定的值（通过 value 参数进行指定）

# (2)也可以用表格中前一个单元格（通过设置参数 method=ffill ）或后一个单元格（通过设置参数 method=bfill ）的值进行填充
emp_df.fillna(method=bfill)
# 在实际工作中，往往会选择更有深度的填充值（如均值、众数等）；
# 或使用某种插值法（拉格朗日插值法、随机插值法等）；
# 也有可能通过统计模型（聚类模型、回归模型、贝叶斯模型等预测计算缺失数据进行填充。）
```

> `DataFrame`对象的很多方法都有一个名为`inplace`的参数，该参数的默认值为`False`，表示我们的操作不会修改原来的`DataFrame`对象，而是将处理后的结果通过一个新的`DataFrame`对象返回。如果将该参数的值设置为`True`，那么我们的操作就会在原来的`DataFrame`上面直接修改，方法的返回值为`None`。

#### 2.重复值处理

```python
# 1.重复值的判断
dept_df.duplicated('dname')  # duplicated方法判断是否存在重复值
# 该方法在不指定参数时默认判断行索引是否重复，也可以指定根据 索引名称某列门是否重复
# 该方法执行后，将一一对照数据，所有重复值都会被标记为True，非重复值会被标记为Fasle

# 2.删除重复值
dept_df.drop_duplicates('dname')
# drop_duplicates方法删除重复值，其中 keep参数 可选 last;first;none
# 可以控制遇到重复值时保留第一项还是保留最后一项，或者一个都不用保留，全部删除。
```

#### 3.异常值处理

异常值在统计学上的称离群点（outlier），异常值的分析也称作离群点分析。

异常值是指样本中出现的“极端值”，数据值看起来异常大或异常小，其分布明显偏离其余的观测值。实际工作中，有些异常值可能是由系统或人为原因造成的，但有些异常值却不是，它们能够重复且稳定的出现，属于正常的极端值，例如很多游戏产品中头部玩家的数据往往都是离群的极端值。 因此对待异常值，不能简单忽视，也不能完全从数据分析中剔除，应该分析其产生的原因。

异常值的检测有Z-score 方法、IQR 方法、DBScan 聚类、孤立森林等方法，这里提供前两种方法的介绍。

##### (1)Z-score 方法

如果数据服从正态分布，依据3σ法则，异常值被定义与平均值的偏差超过三倍标准差的值。**在正态分布下**，距离平均值3σ之外的值出现的概率为$ P(|x-\mu|>3\sigma)<0.003 $，属于小概率事件。**若数据不服从正态分布**，那么可以用远离平均值的多少倍的标准差来描述，这里的倍数就是Z-score。Z-score以标准差为单位去度量某一原始分数偏离平均值的距离，公式：
$$
z = \frac {X - \mu} {\sigma}
$$
Z-score需要根据经验和实际情况来决定，常把远离标准差`3`倍距离以上的数据点视为离群点

```python
# Z-score方法检测异常值
import numpy as np

def detect_outliers_zscore(data, threshold=3):
    avg_value = np.mean(data)  # 计算平均数
    std_value = np.std(data)  # 计算标准差
	# 应用公式
    z_score = np.abs((data - avg_value) / std_value)
    return data[z_score > threshold]
```

##### (2)IQR 方法

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211004192858.png" style="zoom:50%;">

IQR 方法中的IQR（Inter-Quartile Range）代表四分位距离，即上四分位数（Q3）和下四分位数（Q1）的差值。通常情况下，可以认为小于 $ Q1 - 1.5 \times IQR $ 或大于 $ Q3 + 1.5 \times IQR $ 的就是异常值，这种检测异常值的方法也是箱线图 默认使用的方法。

```python
#  IQR 方法检测异常值
import numpy as np

def detect_outliers_iqr(data, whis=1.5):
    q1, q3 = np.quantile(data, [0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - whis * iqr, q3 + whis * iqr
    return data[(data < lower) | (data > upper)]
```

##### (3)异常值的替换删除

```python
# 异常值替换
# 可以通过给单元格赋值的方式来实现，或 replace 方法将指定的值替换掉。
avg_sal = np.mean(emp_df.sal).astype(int)
emp_df.replace({'sal': [1800, 9000], 'comm': 800}, {'sal': avg_sal, 'comm': 1000})

# 异常值删除  drop
emp_df.drop(emp_df[(emp_df.sal > 8000) | (emp_df.sal < 2000)].index)
# 该方法可以根据行索引或列索引删除指定的行或列。
```

#### 4.数据预处理

1）数据的离散化

连续属性离散化的目的是为了简化数据结构，**数据离散化技术可以用来减少给定连续属性值的个数**。离散化方法经常作为数据挖掘的工具。

**连续属性的离散化就是在连续属性的值域上，将值域划分为若干个离散的区间，最后用不同的符号或整数** **值代表落在每个子区间中的属性值。**



数据预处理包含了对数据的拆解、变换、归约、离散化等操作。





### 二、数据分析

#### 1.统计分析

#### 2.排序与筛选

#### 3.分组与聚合

#### 4.透视与交叉

#### 5.数据可视化



### 三、窗口计算

`DataFrame`对象的`rolling`方法允许我们将数据置于窗口中，然后就可以使用函数对窗口中的数据进行运算和处理。例如，我们获取了某只股票近期的数据，想制作5日均线和10日均线，那么就需要先设置窗口再进行运算。我们可以使用三方库`pandas-datareader`来获取指定的股票在某个时间段内的数据，具体的操作如下所示。

安装`pandas-datareader`三方库。

```Bash
pip install pandas-datareader
```

通过`pandas-datareader` 提供的`get_data_stooq`从 Stooq 网站获取百度（股票代码：BIDU）近期股票数据。

```Python
import pandas_datareader as pdr

baidu_df = pdr.get_data_stooq('BIDU', start='2021-11-22', end='2021-12-7')
baidu_df.sort_index(inplace=True)
baidu_df
```

输出：

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211208205710.png" style="zoom:38%;">

上面的`DataFrame`有`Open`、`High`、`Low`、`Close`、`Volume`五个列，分别代码股票的开盘价、最高价、最低价、收盘价和成交量，接下来我们对百度的股票数据进行窗口计算。

```Python
baidu_df.rolling(5).mean()
```

输出：

<img src="https://gitee.com/jackfrued/mypic/raw/master/20211208205932.png" style="zoom:38%;">

上面的`Close` 列的数据就是我们需要的5日均线，当然，我们也可以用下面的方法，直接在`Close`列对应的`Series`对象上计算5日均线。

```Python
baidu_df.Close.rolling(5).mean()
```

输出：

```
Date
2021-11-22        NaN
2021-11-23        NaN
2021-11-24        NaN
2021-11-26        NaN
2021-11-29    150.608
2021-11-30    151.014
2021-12-01    150.682
2021-12-02    150.196
2021-12-03    147.062
2021-12-06    146.534
2021-12-07    146.544
Name: Close, dtype: float64
```

#### 