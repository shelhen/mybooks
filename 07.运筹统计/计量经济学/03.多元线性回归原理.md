# 03.多元线性回归原理

## 一、最小二乘估计多元回归问题

### 1.二元线性回归模型

假设从总体中随机抽取出$n$个个体，则二元回归模型可写为：
$$
y_i=\alpha+\beta x_{i1} + \gamma x_{i2} +\epsilon_i(i=1,...,n)
$$
其中$y_i$为被解释变量，$\alpha$为截距项，$\beta$为给定$x_2$的条件下$x_1$对$y$的边际效益（忽略扰动项$\epsilon_i$），$\gamma$为给定$x_1$的条件下$x_2$对$y$的边际效益（忽略扰动项$\epsilon_i$）为扰动项（误差项），$n$为样本容量。

$OLS$估计量的最优化问题仍为无约束下求残差平方和的最小化：

$$
\min_{\widehat{\alpha},\widehat{\beta},\widehat{\gamma}}{\sum_{i=1}^n{\epsilon_i^2}}=\sum_{i=1}^n{(y_i-\widehat{\alpha}-\beta x_{i1}-\gamma x_{i2})^2}
$$
其几何意义为，寻找最优化回归平面，使其所有样本点$\{ (x_{1i},x_{2i},y_i) \}^n_{i=1}$距离此平面距离最近，同一元回归，仍是分别对$\widehat{\alpha},\widehat{\beta},\widehat{\gamma}$求偏导数，从而求得$\widehat{\alpha},\widehat{\beta},\widehat{\gamma}$的$OLS$估计量。

### 3.多元线性回归模型

假设总体模型（数据生成过程）为：
$$
y_i=\beta_1 x_{i1} + \beta_2 x_{i2}+...+\beta_K x_{iK} +\epsilon_i\qquad(i=1,...,n)\
$$
其中$n$为样本容量，解释变量$x_{ik}$第一个下标$i$表示第$i$个"观测值"，第二个下标$k$表示第$k$个解释变量，共有$K$个解释变量，如果包含常数项，则令第一个解释变量为单位向量，即$x_{i1}\equiv1$，其中$\beta_1,\beta_2,...,\beta_K$表示待估计参数（回归系数）。$\epsilon_i$为扰动项（误差项），其中包括遗漏的其他因素、变量测度的误差，回归函数设定误差（如非线性项、人类行为内在随机性）。

将公式（1）中所有解释变量和参数都写成向量，记第$i$个观测数据

$\mathbf{\vec{x_i}}=(x_{i1},x_{i2},...,x_{iK})^T$，$\mathbf{\vec{\beta}}=(\beta_1,\beta_2,...,\beta_K)^T$

则总体模型：
$$
y_i=\mathbf{\vec{x_i}^T}\mathbf{\vec{\beta_i}^T}+\epsilon_i\qquad(i=1,2,...,n)
$$

$$
\left(\begin{matrix}y_1\\y_2\\\vdots\\y_n\end{matrix}\right)=
\left(\begin{matrix}\mathbf{\vec{x_1}}\\\mathbf{\vec{x_2}}\\\vdots\\\mathbf{\vec{x_n}}\end{matrix}\right)\mathbf{\vec{\beta}}+
\left(\begin{matrix}\epsilon_1\\\epsilon_2\\\vdots\\\epsilon_n\end{matrix}\right)
$$

定义$\vec{\mathbf{y}}=(y_1,y_2,...,y_n)^T$，数据矩阵$\mathbf{\vec{X}}=(\mathbf{\vec{x_1}},\mathbf{\vec{x_2}},...,\mathbf{\vec{x_n}})$，$\vec{\mathbf{\epsilon}}=(\epsilon_1,\epsilon_2,...,\epsilon_n)$
$$
\vec{\mathbf{y}}=\mathbf{\vec{X}}\mathbf{\vec{\beta}}+\vec{\mathbf{\epsilon}}
$$

### 