## 03.聚类分析

> 在一些社会、经济问题中，我们面临的往往是比较复杂的研究对象，如果能把相似的样品(或指标)归成类，处理起来就大为方便，**聚类分析的目的就是把相似的研究对象归成类，使类内对象的同质性最大化和类与类间对象的异质性最大化。**

**聚类分析不仅可以用来对样品进行分类，而且可以用来对变量进行分类**。对样品的分类常称为𝑸型聚类分析，对变量的分类常称为𝑹型聚类分析。

与多元分析的其他方法相比，聚类分析的方法还是比较粗糙的，理论上也不算完善，有很多局限性，如对同一类对象使用不同距离单位和分类方法，将分为不同的类，但由于它能解决许多实际问题，所以很受实际研究者重视，同回归分析、判别分析一起称为多元分析的三大方法。

| 分类方法           | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| 系统聚类法         | 先将n个样品看成n类，然后依次将性质最接近的两类合并成一个新类，得到n-1类，再从中找出最接近的两类加以合并，以此类推，最后所有的样品均在一类，得到最终聚类图。 |
| 模糊聚类法         | 与系统聚类分析法类似，将模糊数学的思想观点用到聚类分析中产生的方法。 |
| K-Means均值法      | 把样品聚集成k个类的集合，类的个数k可以预先给定或者在聚类过程中确定。该方法可应用于比系统聚类法适用的大得多的数据组。（先确定K类，在进行聚类。） |
| 有序样本聚类       | n个样品按某种原因(时间、地层深度等)排成次序，必须是次序相邻的样品才能聚成一类。 |
| 分解法             | 与系统聚类法相反，首先将所有样本划分为一类，然后按照最优原则划分为两类，以此类推，直到每类只有一个样本为止(或者采取其他停止规则)，得到分类结果。 |
| 加入法(逐步聚类法) | 将样品依次加入,每次加入后将它放到当前聚类图的应在位置上,全部加入后,即可得到聚类图。 |

接下将重点学习 系统聚类法 、K-均值聚类法、FCM模糊聚类

### 一、相似度度量

#### 1.间隔尺度—样品分类

> 从一组复杂数据产生一个相当简单的类结构，必然要求进行相关性或相似性度量。当对样品进行聚类时，“靠近”往往用某种距离来刻画。当对指标聚类时,根据相关系数或某种关联性度量来聚类。

每个样品有`p`个指标，故每个样品可以看成`p`维空间中的一个点，`n`个样品就组成`p`维空间中的`n`个点，用**x~ij~**表示第`i`个样品的第`j`个指标，第`j`个指标的均值和标准差记作x~j~和S~j~，d~ij~表示第i个样品与第j个样品之间的距离。这时有：

绝对值距离：$$d_{ij}(1)=\sum_{k=1}^{p}|x_{ik}-x_{jk}|$$

欧氏距离：$$d_{ij}(2)=[ \sum_{k=1}^{p}x_{ik}-x_{jk}]^{1/2}$$

切比雪夫距离：$$d_{ij}(\infty)=\max_{1\leq k\leq p}|x_{ik}-x_{jk}|$$

明考斯基距离：$$d_{ij}(q)=[\sum_{k=1}^{p}|x_{ik}-x_{jk}|^{q}]^{1/q}]$$，当q=1，q=2时，就是绝对值距离和欧式距离，当q趋向于无穷大时，即为切比雪夫距离。

> 这里用k固定相同指标，用i和j分别代表不同的样本。

$$d_{ij}(q)$$**在实际应用中使用广泛，但是有些缺点。首先距离的大小与各指标之间的观测单位有关，具有一定的人为性；其次没有考虑指标之间的相关性。**这时有两种思路对其优化：

##### (1)当个指标之间的测量值相差较大时，应先将数据标准化，然后用标准化后的数据计算距离。

设X~j~，R~j~，S~j~分别表示第j个指标的样本均值、*样本极差*和样本标准差，则有：

$$X_j = \frac{1}{n}\sum_{i=1}^{n}x_{ij}$$

$$R_j=\max_{1\leq i \leq n} {x_{ij}}-\min_{1\leq i \leq n} {x_{ij}}$$

$$S_j=[ \frac{1}{n-1}\sum_{i=1}^{n}(x_{ij}-X_j)^{2}]^{1/2}$$

此时标准化后的数据为：$$x^{`}_{ij}=\frac{x_{ij}-X_j}{S_j}$$  或   $$x^{*}_{ij}=\frac{x_{ij}-X_j}{S_j}$$

当x~ij~>0时，还可以使用兰式距离标准化，该距离能克服距离的大小受各指标之间的观测单位的影响，但无法解决指标之间的相关性问题。

$$d_{ij}(LW)=\frac{1}{p}\sum_{k=1}^{p}\frac{|x_{ik}-x_{jk}|}{x_{ik}+x_{jk}}$$

##### (2)使用马氏距离

$$d_{ij}^2(M)=(x_{(i)}-x_{(j)})^{T}S^{-1}(x_{(i)}-x_{(j)})$$

> x~(i)~表示第i个行向量（样本）的转置，x~（j）~表示第j个行向量的转置，S^-1^ 矩阵X的协方差阵。

**它对一切线性变换是不变的，故不受指标量纲的影响，而且它也充分考虑了指标的相关性。**

但是，在实施聚类分析之前，我们往往并不知道研究对象具体有多少个不同的分类，因此马氏距离中的S^-1^ 难以计算，如果应用全部数据计算的均值和协方差阵来计算马氏距离，效果并不理想。

#### 2.有序尺度/名义尺度—样品分类

对于有序/名义尺度分类的样本，定义若两个样本的指标相同，称为配合；若不相同，则称为不配合。

记配合的指标数为m~1~ ，不配合的指标数为m~2~ ，则海伦距离：$$d_{12}=\frac{m_2}{m_1+m_2}$$

#### 3.相似系数—指标分类

聚类分析不仅需要将样品分类，也需要将指标分类。在指标之间也可以定义距离，这时更常用相似系数。若用 C~ij~  表示指标 i 和指标 j 之间的相似系数，则 C~ij~ 的绝对值越接近1，说明指标 i 和指标 j 之间的距离越近，C~ij~ 的绝对值越接近0，则说明指标 i 和指标 j 之间的距离越远。常用的相似系数包括夹角余弦和相关系数。

**使用相似系数不仅适用于指标分类也适用于样本分类，尤其是当数据是多维而非二维时。**

##### (1)夹角余弦

受相似形启发，对于长度不一、形状相似的曲线，可以通过夹角余弦表示其相似程度：

$C_{ij}=\frac{\sum^n_{k=1}x_{ki}x_{kj}}{[(\sum_{k=1}^n x^2_{ki})(\sum_{k=1}^n x^2_{kj})]^{1/2}}$[它表示向量指标$(x_{1i},x_{2i},...,x_{ni})$和$(x_{1j},x_{2j},...,x_{nj})$之间的夹角余弦。]

##### (2)相关系数

相关系数是将数据标准化后的夹角余弦：

$r_{ij}=\frac{\sum^n_{k=1}(x_{ki}-X_i)(x_{kj}-X_j)}{[\sum^n_{k=1}(x_{ki}-X_i)^2\sum^n_{k=1}(x_{kj}-X_j)^2]^{1/2}}$

> 聚类时，相似系数高的变量聚类倾向于归为一类，相似系数低的变量归属不同的类。**在Python中采用夹角余弦度量变量之间的相似度。**      
>

名义尺度指标之间也可以定义相似系数，若指标为仅有两个取值的名义尺度指标，也可定义相关系数。

有时也可以用距离来描述指标之间的接近程度。事实上，距离与相似系数之间可以相互转化，若C~ij~为非负的相似系数，则$d_{ij}=1-C^2_{ij}$可以看成距离（不一定符合距离定义）或把$d_{ij}=[2(1-C_{ij})]^{1/2}$看成距离，若d~ij~为一个距离，则$C_{ij}=\frac{1}{1+d_{ij}}$可看做相似系数。

### 二、类和类的特征

#### 1.类的定义

用G表示类，设G中有k个元素，这些元素分别用i，j表示。(这里的阈值指的是类的直径。)

##### (1)对阈值T，若对任意  $i,j\in G$  有$d_{ij}\leq T$，（d~ij~为 i 和 j 之间的距离）则称G为一个类。

##### (2)对阈值T，若对每个$i\in G$，有 $\frac{1}{k-1}\sum_{j\in G}d_{ij}\leq T$（这里指任意i到j的平均距离d），则称G为一个类。

##### (3)对阈值T，V，若$\frac{1}{k(k-1)}\sum_{i\in G}\sum_{j\in G}d_{ij}\leq T$  且  $d_{ij}\leq V$ ，对一切  $i,j\in G$  恒成立，则称G为一个类。

##### (4)对阈值T，若对任意的  $i\in G$ ，一定存在  $j\in G$  ，使得   $d_{ij}\leq $ T ，则称G为一个类。

> 这里(1)的要求最高，当(1))成立时，(2)必然成立，(2)成立时，(3)必然成立。

#### 2.类的特征

对于某类G的元素$x_1,x_2,x_3,x_4,....,x_m$，m为G内的样品数（指标数），可通过如下几个特征刻画G的特征：

##### (1)均值（重心）：$X_G=\frac{1}{m}\sum_{i=1}^mx_i$

##### (2)样本离差阵：$L_G=\sum_{i=1}^m(x_i-X_{G})(x_i-X_G)^T$

##### (3)协方差阵：$S_G=\frac{1}{n-1}L_G$

##### (4)G的阈值（直径）

一般有多种定义，如  $D_G = \sum_{i=1}^m(x_i-X_{G})^T(x_i-X_{G})=tr(L_G)$  或  $D_G = \max_{i,j\in G}d_{ij}$

#### 3.类的距离的计算方法

在聚类分析中，不仅要考虑类的特征，还应该考虑不同类之间的距离，由于类的形状并不固定，因此类间距的计算方式有多种。

设 类 $G_p$ 和 类 $G_q$ 分别有  n 个和  m  个 样本，他们的重心分别为  $x_p$  和  $x_q$  ，类间距用 $D(p,q)$ 表示，则有：

##### (1)最短距离法(single)

 类 $G_p$ 和 类 $G_q$ 之间距离最邻近的两个样品之间的距离：$D_k(p,q)=\min\{d_{ij} | i\in G_p, j\in G_q\}$。

##### (2)最长距离法(complete)

 定义类 $G_p$与类$G_q$ 之间的距离为两类最远样本间的距离：$D_k(p,q)=\max\{d_{ij} | i\in G_p, j\in G_q\}$。

##### (3)重心法(centroid)

类 $G_p$ 的重心 $x_p$ 和 类 $G_q$的重心 $x_q$  之间的距离：$D_c(p,q)=d_{pq}$

##### (4)类平均法(average)

类 $G_p$ 和 类 $G_q$ 中任意两个样品距离值的平均：$D_G(p,q)=\frac{1}{mn}\sum_{i \in G_p}\sum_{j \in G_q}d_{ij}$。

一种定义方法是把类与类之间的距离定义为所有样本对之间的平均距离；另一种定义方法是定义类与类之间的平方距离为样本对之间的平方距离的平均值。  

##### (5)离差平方和法(Ward)

用 $D_p$ 和 $D_q$ 分别表示类 $G_p$ 和 类 $G_q$ 的直径，用 $D_{p+q}$ 表示大类$G_{p+q}$的的直径，则类 $G_p$ 和 类 $G_q$ 之间距离的平方  ：$D^2_w(p,q)=D_{p+q}-D_p-D_q$，其中：

$D_p = \sum_{i\in G_p}(x_i-x_p)^T(x_i-x_p)=tr(L_p)$

$D_q = \sum_{i\in G_q}(x_i-x_q)^T(x_i-x_q)=tr(L_q)$

$D_{p+q} = \sum_{j\in G_pUG_q}(x_j-x_{p+q})^T(x_j-x_{p+q})=tr(L_{p+q})    $；   ($x_{p+q} = \frac{1}{m+n}\sum_{i\in G_pUG_q}x_i$)。

若样品间距离采用欧式距离，则Ward法定义的类间距$D_w$与重心法定义的类间距$D_c$之间的关系如下：

$D_w^2(p,q) = \frac{mn}{m+n}D^2_c(p,q)$，若令 $k=\frac{mn}{m+n}$，则$D^2_w=kD^2_c$，k与两类的样品数有关。

```python
# 距离计算
scipy.spatial.distance.pdist(
    X, # :接受ndarray数组,表示需要计算距离的数据。无默认值
    metric=‘euclidean’,  # 接受特定str或function,表示进行计算距离的方法。
    # 默认为euclidean，表示欧式距离；
    # seuclidean：标准化欧式距离
    # cityblock，表示绝对值距离
    # minkowski，明考斯基距离（综合欧氏距离、切比雪夫距离、绝对值距离）
    # cosine时，表示夹角余弦。
    # mahalanobis：马氏距离
    p=None, w=None, V=None, VI=None)

data = np.array([[1,2,5,9,13]])
print(f'欧式距离矩阵为：{distance.pdist(data.T, "euclidean")}')
print(f'标准化欧式距离矩阵为：{distance.pdist(data.T, "seuclidean")}')
print(f'欧式距离平方矩阵为：{distance.pdist(data.T, "sqeuclidean")}')
print(f'绝对值距离矩阵为：{distance.pdist(data.T, "cityblock")}')
print(f'明考距离矩阵为：{distance.pdist(data.T, "minkowski",p=2.)}') # p为1表示
print(f'马氏距离矩阵为：{distance.pdist(data.T, "mahalanobis")}')
print(f'夹角余弦矩阵为：{distance.pdist(data.T, "cosine")}')
```

### 三、聚类分析法

#### 1.步骤

- 分析需要研究的问题，确定实施聚类分析所需要的多元变量；
- 选择对样品聚类还是对指标聚类；
- 选择合适的聚类方法；
- 选择所需要的聚类结果；
- 对结果进行评价。

#### 2.系统聚类分析法（层次聚类法）

> **核心思想：**开始时将*n*个样品各自作为一类，并规定样品之间的距离和类与类之间的距离，然后将距离最近的两类合并成一个新类，计算新类与其他类的距离；重复进行两个最近类的合并，每次减少一类，直至所有的样品合并为一类。

系统聚类法(`hierarchical clustering method`)是聚类方法中使用最多的方法，它包含如下步骤：

```
st=>start: 计算n个样品两两之间的距离d,记为D。
e=>end: 决定分类的个数和类。
op1=>operation: 构造n个类，每个类只包含一个样品。
op2=>operation: 合并距离最近的两个类为一个新类。
op3=>operation: 计算新类与当前各类之间的距离。
op4=>operation: 画出聚类图。
cond=>condition: 类的个数是否为1？

st->op1->op2->op3->cond
cond(yes)->op4->e
cond(no)->op2
```

##### (1)算法详解

- 最短距离法与最长距离法

  | 方法 | 最短距离法                                                   | 最长距离法                                                   |
  | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | 原理 | 定义类之间的距离为各类**最近**样本间的距离，随后选取距离最短的合并为一个新类，再次计算新类与各类之间的最长距离，再次合并距离最短的类为新类，以此类推，直至合并为一类。 | 定义类之间的距离为各类**最远**样本间的距离，随后选取距离最短的合并为一个新类，**再次计算新类与各类之间的最长距离**，再次合并距离最短的类为新类，以此类推，直至合并为一类。 |
  | 区别 | 类间距离递推应计算新类与其他类的最短距离。                   | 类间距离递推应计算新类与其他类的最长距离。                   |
  | 联系 | 选取所得距离中的最小值合并为新类                             | 选取所得距离中的最小值合并为新类                             |
  | 优点 | 也可以用于对指标的分类，分类时可用距离，也可用相似系数，使用相似系数时，应选取值最大的元素并为一类，计算新类与其他类的距离时也应该取最大值。 | 在一定程度上克服了最短距离法链接聚合的缺陷，当两类合并后，与其他类的距离将取原本两个类中距离的最大者。 |
  | 缺点 | 在最短距离法计算迭代过程中，若某一步计算所得最短距离不止一个，则会优先选择直径较大的类来合并，如果一样大时才会任选一个合并或同时合并，这时容易产生链接聚合现象，导致大部分样品被聚合在一个类中。<br/>最短聚类法不适合对分离的很差的群体进行分类。 |                                                              |

  > 最短距离法和最长距离法的并类步骤的区别在于类间距离的递推公式不同。一般情况下，最短距离法的聚类效果并不好，在实践中不提倡使用，最短距离法更适合聚条形的类。

- 重心法与类平均法

  类与类之间的距离使用重心间距（样本间距均值）代表从物理上来看比较合理。若样品之间采用欧式距离，设某一步将类G~p~ 类G~q~ 合并为G~r~ ，他们各有n，m，和（m+n）个样本，其重心分别使用X~p~ 、X~q~ 、X~r~ 表示，显然有：$X_r=\frac{nX_p+mX_q}{m+n}$，若有令一类G~k~ ，则二者距离的平方：

  $D^2_c(r,k)=(X_k-X_r)^T(X_k-X_r)=\frac{n}{m+n}D^2_c(p,k)+\frac{m}{m+n}D^2_c(q,k)-\frac{mn}{(m+n)^2}D^2_C(p,q)$

  > 重心法虽具有较强的代表性，但并未充分利用各个样本的信息。

  如果能在距离上再次取平均值，就能更充分地利用样本信息，这就是类平均法:

  $D^2_A(r,k)=\frac{n}{m+n}D^2_A(p,k)+\frac{m}{m+n}D^2_A$

  > 类平均法是 聚类效果比较好，应用比较广泛的聚类方法。

  但在类平均法的递推公式中未能反映$D_A(p,q)$ ，有学者将其改为：

  $D^2_A(r,k)=\frac{n}{m+n}(1-\beta)D^2_A(p,k)+\frac{m}{m+n}(1-\beta)D^2_A+\beta D^2_A(q,p)$   	($\beta<1$)

  该方法又称可变类平均法，该方法中分类效果与$\beta$选择有关，有一定的人为性，因此实际并不长使用，并且$\beta$越接近1，效果越差，因此$\beta$常取负值。

- 离差平方和法

  离差平方和的核心思想源于方差分析，如果类分的正确，同类样品的离差平方和应尽可能的小，类与类之间的离差平方和应尽量大。

  设将n个样品分为k类$G_1,G_2,...,G_k$，用$x_{it}$表示类$G_t$中的第i个样品（$x_{it}是p维向量$），用$n_t$表示类$G_t$中的样品个数，$X_t$是类$G_t$的重心，

  则在类$G_t$中的样品的离差平方和为：$L_t = \sum^{n_{t}}_{i=1} (x_{it}-X_t)^T(x_{it}-X_t)$

  整个类的平方和为：$L=\sum^k_{t=1}L_t =\sum^k_{t=1}\sum^{n_{t}}_{i=1} (x_{it}-X_t)^T(x_{it}-X_t)$

  当k为定值时，要选择使得L达到极小的分类，n个样品分为k类，一切可能的分法有：

  $R(n,k)=\frac{1}{k}\sum^k_{i=0}(-1)^{k-i}\left[\begin{matrix} k \\ i \\ \end{matrix} \right]i^n$  	，当n=21，k=2时，可能的分法r=2097151，非常大，当n和k更大时，rJ将取一个天文数字。于是在实际应用中通常会放弃从所有可能分类中求 L 的极小值，而是设计出某种规格，去寻找一个局部最优解。如先让n个样品各自成一类，然后每次缩小一类，这时选择使离差平方和 L 增加最小的两类合并，以此类推，直至所有样品归一。

  将某类G~p~ 和G~q~ 合并为 G~r~ ，则G~k~ 与新类 G~r~ 之间距离的递推关系式为：

  $D^2_w(r,k)=\frac{n+n_k}{m+n+n_k}D^2_w(p,k) + \frac{m+n_k}{m+n+n_k}D^2_w(q,k) - \frac{n_k}{m+n+n_k}D^2_w(p,q)$

  > 离差平方和只能求出局部最优解，至今并没有好办法求得精确最优解。

- 系统聚类法的统一

  兰斯和威廉姆斯于1967年提出一个统一的公式：

  $D^2(r,k)=\alpha_pD^2(p,k) + \alpha_qD^2(q,k) + \beta D^2(p,q) + \gamma |D^2(p,k)-D^2(q,k)|$  	(其中$\alpha_p,\alpha_q,\beta,\gamma$对于不同的方法有不同的取值)。下表中列举了不同方法对应的不同取值：

| 方法         | $\alpha_p$                | $\alpha_q$                | $\beta$             | $\gamma$ |
| ------------ | ------------------------- | ------------------------- | ------------------- | -------- |
| 最短距离法   | $1/2$                     | $1/2$                     | $0$                 | $-1/2$   |
| 最长距离法   | $1/2$                     | $1/2$                     | $0$                 | $1/2$    |
| 中间距离法   | $1/2$                     | $1/2$                     | $-1/4$              | $0$      |
| 重心法       | $n_p/(n_q+n_p)$           | $n_q/(n_q+n_p)$           | $-\alpha_p\alpha_q$ | $0$      |
| 类平均法     | $n_p/(n_q+n_p)$           | $n_q/(n_q+n_p)$           | $0$                 | $0$      |
| 可变类平均法 | $(1-\beta)n_p/(n_p+n_q)$  | $(1-\beta)n_q/(n_p+n_q)$  | $\beta<1$           | $0$      |
| 可变法       | $(1-\beta)/2$             | $(1-\beta)/2$             | $\beta<1$           | $0$      |
| 离差平方和法 | $(n_k+n_p)/(n_k+n_p+n_q)$ | $(n_k+n_p)/(n_k+n_p+n_q)$ | $-n_k/n_k+n_p+n_q$  | $0$      |

一般而言，使用不同的聚类方法所得到的结果不完全相同，效果也不完全相同，**接下来将面对如何选取合适的K值及选择什么样的结果比较好的问题**。

- 系统聚类法的性质

  **单调性：**令D~r~ 为系统聚类法中第r次并类时的距离，一种聚类分析法若能保证{$D_r$}严格单调上升的，则称其具有单调性，根据单调性绘制出的聚类图能符合系统聚类的思想，先结合的类关系较近，后结合的类关系较远。显然**最短距离法与最长距离法都具有单调性，另外可以以证明，类平均法，离差平方和法，可变法和可变类平均法都具有单调性，重心法和中间距离法不具备单调性。**

  **空间距离的浓缩与扩张：**对同一问题作聚类图时，横坐标（并类距离）的范围相差过大的，称其使空间扩张，范围相差过小的，称其使空间浓缩。**最短距离法、重心法会使得空间浓缩，最长距离法、离差平方和法会使空间扩张。太浓缩的方法不够灵敏，太扩张的方法又过于灵敏。**类平均法比较适中，很多书籍推荐使用此方法。

##### (2)分类数确定与效果评价

聚类分析的目的就是对研究对象进行分类，层次分析法的结果是得到一个树状结构图，如何来确定类的最佳个数呢？

确定分类数是聚类分析过程中难以解决的问题，主要障碍在于无法在排除人为干预的情况下对类的结构和内容给出一个统一的定义因此无法给出在理论上和实践中都可行的虚无假设。实际使用中人们只能根据研究目的，从实用角度出发，选择合适的分类数。

德米尔曼曾提出一个准则：

```python
# 1.任何类都必须在邻近各类中是突出的，即各类重心之间距离必须足够大；
# 2.各类包含的元素都不应该过多；
# 3.分类的数目应该符合使用的目的；
# 4.若采用几种不同的聚类方法处理，则在各自聚类图上应发现相同的类。
```
> 系统聚类中每次合并的类与类之间的距离可以作为确定分类数的一个辅助工具，在系统聚类的过程中，首先合并距离最近的类，因此并类过程中`聚合系数`呈增大趋势，聚合系数越小，合并的两类之间的相似程度越大，差异过大的类合并在一起后会使该系数大量增加。

> 以y轴作为聚合系数，x轴为分类数，画出聚合系数随分类数变化的曲线，可以得到类似因子分析中的碎石图，可以在曲线变得平缓处选择合适的分类数。

此外，Charrad等总结了CH指标，Hartigand指标等30种方法来计算所谓的聚合系数，

- **CH指标：** m为类的个数

  $CH(k) = \frac{tr(B_k)/(k-1)}{tr(W_k)/(n-k)}$  

  其中$B_k=\sum^k_{t=1}\sum_{i\in C_j}(x_{it}-X_t)(x_{it}-X_t)^T$	和	$W_k=\sum^k_{t=1}n_t(X_t-X)(X_t-X)^T$   。

  > W~k~ （类内距）将随着类的数目增大而减小，其越小说明类越紧凑，B~k~ （类间距）则随着类的数目增大而增大，其越大说明类之间的界限越清晰。其最优类的数目是：$k^* =argmaxCH(k)$

- **Hartigand指标：**

  $HA(k)=(\frac{tr(W_k)}{tr(W_{K+1})}-1)(n-k-1)$		，最优k应最大化上式的值。

##### (3)Python代码实现

使用SciPy库cluster模块的`hierarchy`子模块可以实现系统聚类，该模块下的`linkage`函数可以实现最短距离法、最长距离法、类平均法和重心法等。


```python
# 表示最短距离法；
scipy.cluster.hierarchy.linkage(
    y,  # 接受ndarray,表示需要聚类的数据。无默认值    
    method='single',  # 接受str,表示计算聚类的方法。默认为single
    # single:表示最短距离法；
    # complete：表示最长距离法；

    # average：等权重算术平均距离法（类平均法）；【UPGMA】
    # weighted：加权算术平均距离法(可变类平均法),【WPGMA】 
    # centroid：等权形心聚类,把类与类中的质心间距离作为类间距,重心法【UPGMC】
    # median：加权形心聚类，加权质心距离法（中间距离法）；【WPGMC】  
    
    # ward：表示离差平方和法（ward法）；
    metric='euclidean',  # 接受特定str或function，表示在y是观测向量集合的情况下使用的计算距离的方法。
    # euclidean：表示欧式距离；
    # minkowski：表示Minkowski距离；
    # cityblock：表示绝对值距离；
    # cosine：表示夹角余弦。默认为euclidean
    optimal_ordering=False
)
```

`linkage`对象将返回一个值 为Z 的 维度为(n-1)\*4的矩阵，记录了层次聚类每一次的合并信息，其中的 4个值分别对应合并的两个cluster的序号、两个cluster之间的距离以及本次合并后产生的新的 cluster 所包含的样本点的个数。     

```python
Z = 
[[ 2.  7.  0.  2.]
 [ 5.  6.  0.  2.]
 [ 0.  4.  1.  2.]
 [ 8. 10.  1.  4.]
 [ 1.  9.  1.  3.]
 [ 3. 11.  2.  5.]
 [12. 13.  4.  8.]]
# 一共有8个元素，需要进行7次聚类，所以Z有7行;
# 每一行有四个元素，前两个是进行cluster操作的向量的索引值(合并向量的序号)，第三个是两个cluster之间的距离，第四个是该类目前有几个元素。
# 在生成新的cluster后，其会被赋予索引值8,9,10等，一点一点往上加，这也是为什么只有8个元素但是最后几行的索引值有10+。
```

在得到聚类结果Z后，直接观看其值往往不太明显，可以通过可视化方式绘制树状结构图来观察聚类效果，Python中使用`dendrogram`实现绘制树状图

```python
def dendrogram(
    Z, # 层次聚类编码为树状图的链接矩阵 
    truncate_mode=None, # 当导出链接的原始观察矩阵很大时，树状图可能难以阅读。
    # 该参数用于截断压缩树状图，可选参数None：不截断；level：不会显示超过 p 级别的树状图树（p在下面）。lastp：链接中最后形成的p非单子簇是链接中唯一的非叶子节点；它们对应于 Z 中的 Z[n-p-2:end] 行。所有其他非单例集群都收缩为叶节点。 
    p=30, 
     orientation='top',  # 绘制树状图的方向，可以是以下任意字符串：top,bottom,left,right
    labels=None,  # 选择坐标轴刻度名称的列表
    ax=None,   # 树状图将绘制在给定的Axes实例。
    
    color_threshold=None,
    get_leaves=True, 
    count_sort=False, distance_sort=False, show_leaf_counts=True, no_plot=False, no_labels=False, leaf_font_size=None, leaf_rotation=None, leaf_label_func=None, show_contracted=False, link_color_func=None, above_threshold_color=‘C0’)
```

**python代码实现层次聚类**

```python
import pandas as pd
import time
import matplotlib.pylab as plt
from scipy.cluster import hierarchy
from sklearn.decomposition import PCA  # 加载PCA算法包

plt.rcParams['font.sans-serif']=['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus']=False    # 用来正常显示负号

# 读取数据
data = pd.read_csv('./datas/例3-5.txt',index_col=0,sep='\t')

# 进行 Agglomerative 层次聚类
linkage_method_list = ['single', 'complete', 'average', 'weighted', 'centroid', 'ward']
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20), dpi=100)
res_list = []
for i, linkage_method in enumerate(linkage_method_list):
    Z = hierarchy.linkage(data, method=linkage_method)
    j = 0 if i%2==0 else 1
    if i in (0,1):
        i=0
    elif i in (2,3):
        i = 1
    else:
        i=2
    # print('Adjust mutual information: %.3f' % adjusted_mutual_info_score(labels, labels_pred))
    res_list.append(Z)
    # 这里通过dendrogram实现绘制树状结构图
    hierarchy.dendrogram(Z, orientation="top",ax=axes[i,j],labels=data.index) #  truncate_mode='lastp'
    axes[i,j].set_title(linkage_method + "法层次聚类分析树状图", fontsize=15)
plt.show()
```

##### (4)样本选择与评价(该模块待补充)

如果有了类别标签，那么也可以通过公式去计算聚类结果的准确率和召回率，但是不应该完全将分类标签作为聚类结果的评价指标，除非有相关先验知识或某种假设。聚类评价的标准是组内的对象相互之间是相似的（相关的），而不同组中的对象是不同的（不相关的）。即组内的相似性越大，组间差别越大，聚类效果就越好。`sklearn`的`metrics`模块提供的聚类模型评价指标。

|        评价方法         | 真实值 | 最佳值       | 函数                  |
| :---------------------: | ------ | ------------ | --------------------- |
|   AMI评价法（互信息）   | 需要   | 1.0          | ``                    |
|  ARI评价法（兰德系数）  | 需要   | 1.0          | `adjusted_rand_score` |
|      V-Meature评分      | 需要   | 1.0          | ``                    |
|        FMI评价法        | 需要   | 1.0          | ``                    |
|     轮廓系数评价法      | 不需要 | 畸变程度最大 | ``                    |
| calinski_harabasz评价法 | 不需要 | 相较最大     | ``                    |



```python
from sklearn.metrics import cluster

__all__ = [
    "mutual_info_score", # MI互信息是用来衡量两个数据分布的吻合程度的指标，取值范围为[0,1]，
    "adjusted_mutual_info_score",  # AMI（Adjust Mutual Information）调整互信息，评估聚类结果与样本的真实类之间的相近程度的AMI，取值范围应该为[-1,1]。
    "normalized_mutual_info_score", # NMI标准化互信息，不管标签分配之间的“互信息”的实际数量如何，互信息或者标准化后的值不会因此而调整，而会随着标签（簇）数量的增加而增加。
    # 以上三个量都是越大（接近1）说明聚类算法产生的类越接近于真实情况，越小（接近0或为负值）说明效果差。
    "expected_mutual_information",
    # 
    "adjusted_rand_score",  # 计算样本预测值和真实值之间的相似度similarity：同属于这一类或都不属于这一类，而不考虑数据元素顺序和归一化，该结果越接近1说明相似度越高，越接近0或为负值说明相似度低。
    "rand_score",
    
    "completeness_score",
    "pair_confusion_matrix",
    "contingency_matrix",
    "homogeneity_completeness_v_measure",
    "homogeneity_score",
    "v_measure_score",
    "fowlkes_mallows_score",
    "entropy",
    "silhouette_samples",
    "silhouette_score",
    "calinski_harabasz_score",
    "davies_bouldin_score",
    "consensus_score",
]
# 如下 可以计算其中一个指标
score = cluster.adjusted_mutual_info_score(data.index, labels_pred)
```







##### (5)

- 直观评价法





- 聚类效果的图形评价







```python
pca = PCA(n_components=2)  # 加载PCA算法，设置降维后PC数目为2
reduced_x = pca.fit_transform(data)  # 对样本进行降维
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 30), dpi=100)

red_x, red_y = [], []
blue_x, blue_y = [], []
green_x, green_y = [], []

for Z in res_list:
    # 根据聚类数目返回聚类结果,经观察，分成3类比较合适
    labels = hierarchy.fcluster(Z, t=5, criterion='maxclust') # 聚类结果
    for i in range(3):
        for j in range(2):
            # 聚类的结果可视化，相同的类的样本点用同一种颜色表示
            for k in range(len(reduced_x)):
                if labels[k]==1:
                    red_x.append(reduced_x[k][0])
                    red_y.append(reduced_x[k][1])
                elif labels[k] == 2:
                    blue_x.append(reduced_x[k][0])
                    blue_y.append(reduced_x[k][1])
                else:
                    green_x.append(reduced_x[k][0])
                    green_y.append(reduced_x[k][1])
            axes[i,j].scatter(red_x, red_y, c='r', marker='x')
            axes[i,j].scatter(blue_x, blue_y, c='b', marker='D')
            axes[i,j].scatter(green_x, green_y, c='g', marker='.')
plt.show()
```

在得到了层次聚类的过程信息 Z 后，可以使用 fcluster 函数来获取聚类结果，可以从两个维度来得到距离的结果，一个是指定临界距离 d，得到在该距离以下的未合并的所有 cluster 作为聚类结果；另一个是指定cluster 的数量 k，函数会返回最后的 k 个 cluster 作为聚类结果。 

```python
labels_1 = hierarchy.fcluster(Z, t=5, criterion='distance')  # 根据类间距离分类
labels_2 = hierarchy.fcluster(Z, t=5, criterion='maxclust')  # 根据类别确定聚类结果
```

在对同一份样本进行了各类层次聚类分析后，如何评价其效果呢？



##### (1)使用图形作直观的聚类

- 当*p*=2时，可以直接在散点图上进行主观的聚类，其效果未必逊于、甚至好于正规的聚类方法，特别是在寻找“自然的”类和符合我们实际需要的类方面。

- 当*p*=3时，我们可使用统计软件产生三维旋转图，通过三维旋转从各个角度来观测散点图，作直观的聚类。但由于其视觉效果及易操作性远不如平面散点图，故实践中很少采用。

- 当*p*≥3时，有时我们可采用主成分分析或因子分析的技术将维数降至2（或3）维，然后再生成散点图（或旋转图），从直觉上进行主观的聚类。

##### (2)聚类效果的图形评价

不同类之间是否分离得较好，同一类内的样品（或变量）是否彼此相似。通常可通过构造图形作直观的观测，所使用的图形有如下两种：

- 将*p*维数据画于平面图上，方法有平行（坐标）图、星形图、切尔诺夫脸谱图、星座图和安德鲁曲线图等；

- 使用费希尔判别的降维方法，将*p*维数据降至2（或3）维再构造散点图（或旋转图）。

> 如果方法(2)能够成功，则往往更值得推荐，尤其在样品数很大的场合下。

一是在聚类之前我们没法知道合理的聚类的数目或者最大的距离临界值，只有在得到全部的层次聚类信息并对其进行分析后我们才能预估出一个较为合理的数值；二是本次实验的数据集比较简单，所以聚类的结果较好，但对于复杂的数据集（比如非凸的、噪声点比较多的数据集），层次聚类算法有其局限性。

#####  (3)聚类效果统计评价



#### 3.`K-means`聚类分析

