# 01.è®¡é‡ç»æµå­¦ä¸ç»Ÿè®¡å­¦åŸºç¡€

## ä¸€ã€ä»€ä¹ˆæ˜¯è®¡é‡ç»æµå­¦

è®¡é‡ç»æµå­¦(`Econometrics`)ä¸€è¯æ®è¯´æ˜¯ç”±æŒªå¨ç»æµå­¦å®¶` R.Frisch(1895-1973)`åˆ›é€ å‡ºæ¥çš„ã€‚`Frisch` åœ¨ `Econometrca `ç¬¬ä¸€å·çš„å·é¦–è¯­ä¸­å†™é“ï¼š

```python
ç»æµç†è®ºä¸ç»Ÿè®¡å­¦å’Œæ•°å­¦ä¹‹é—´è”ç³»çš„è¿›å±•;
ç»æµé—®é¢˜çš„ç†è®ºå®šé‡ç ”ç©¶å’Œç»éªŒå®šé‡ç ”ç©¶;
è®¡é‡ç»æµå­¦ä¸ç»æµç»Ÿè®¡å­¦ã€æ•°å­¦åœ¨ç»æµå­¦ä¸­çš„åº”ç”¨ä¸æ˜¯ä¸€å›äº‹;
ç»éªŒæ˜¾ç¤ºç»Ÿè®¡å­¦ã€ç»æµç†è®ºå’Œæ•°å­¦éƒ½å¾ˆé‡è¦ï¼Œåªæœ‰å®ƒä»¬ç›¸äº’ç»“åˆæ‰èƒ½å¯¹ç°å®ä¸–ç•Œçš„ç»æµå…³ç³»æœ‰æ›´å¥½çš„ç†è§£;
```

è¿™ä¸‰è€…çš„ç»“åˆæ„æˆäº†è®¡é‡ç»æµå­¦ã€‚Frisch çš„è¿™äº›å®šä¹‰åœ¨ä»Šå¤©ä»ç„¶é€‚ç”¨ï¼Œåªæ˜¯åœ¨æŸäº›ç”¨æ³•æ–¹é¢å¯èƒ½å‘ç”Ÿäº†ä¸€äº›å˜åŒ–ã€‚

è®¡é‡ç»æµå­¦å°±æ˜¯ç»¼åˆåˆ©ç”¨ç»æµæ¨¡å‹ã€æ•°ç†ç»Ÿè®¡å’Œç»æµæ•°æ®æ¥åˆ†æç»æµé—®é¢˜ã€‚`Stockand Watson(2015Introduce to Econometrics Updated 3rd)`è¯´ï¼šâ€œè®¡é‡ç»æµå­¦æ˜¯åˆ©ç”¨ç»æµç†è®ºå’Œæ•°ç†ç»Ÿè®¡æŠ€æœ¯æ¥åˆ†æç»æµæ•°æ®ã€‚â€å®ƒå¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š

- è®¡é‡ç»æµç†è®ºï¼Œæˆ–è€…ç†è®ºè®¡é‡ç»æµå­¦åŒ…æ‹¬å·¥å…·å’Œæ–¹æ³•çš„å‘å±•ï¼Œä»¥åŠå¯¹æ–¹æ³•æ€§è´¨çš„ç ”ç©¶;
- åº”ç”¨è®¡é‡ç»æµå­¦æè¿°äº†å®šé‡ç»æµå­¦çš„å‘å±•ï¼Œä»¥åŠåˆ©ç”¨ç»æµæ•°æ®æ¥åº”ç”¨è¿™äº›æ¨¡å‹ã€‚

### 1.è®¡é‡ç»æµå­¦æ–¹æ³•

ç°ä»£è®¡é‡ç»æµå­¦çš„ç»Ÿä¸€æ–¹æ³•æ˜¯ç”±æŒªå¨ç»æµå­¦å®¶`T. Haavelmo(1911-1999)`å¼€åˆ›çš„ã€‚1944 å¹´ä»–åœ¨`Econometrica `ä¸Šå‘è¡¨â€œ`The probability approach in econometrics`â€ã€‚ä»–è®¤ä¸ºå®šé‡ç»æµæ¨¡å‹å°±æ˜¯ä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ï¼Œå› æ­¤ï¼Œè¦åœ¨ç»æµæ¨¡å‹ä¸­åŠ å…¥éšæœºæ€§ã€‚é‚£ä¹ˆï¼Œå¯¹ç»æµæ¨¡å‹çš„é‡åŒ–ã€ä¼°è®¡å’Œæ¨æ–­çš„æ°å½“æ–¹æ³•å¿…é¡»è¦ä»¥æ•°ç†ç»Ÿè®¡å­¦ä¸ºåŸºç¡€ã€‚è¿™å°±æ˜¯è®¡é‡ç»æµå­¦çš„æ¦‚ç‡æ–¹æ³•ã€‚

`Haavelmo` çš„æ¦‚ç‡æ–¹æ³•å¾ˆå¿«å°±è¢«ç»æµå­¦ä¸“ä¸šæ¥å—ï¼Œå‘å±•ï¼Œå¹¶å¹¿ä¸ºä¼ æ’­ã€‚å› æ­¤ï¼Œå½“ä»Šçš„ç»æµå­¦å®šé‡ç ”ç©¶ç¦»ä¸å¼€æ¦‚ç‡æ–¹æ³•ã€‚ä½†æ˜¯ï¼Œ`Haavelmo` åŸå§‹æƒ³æ³•çš„å¹¶ä¸æ˜¯æ¦‚ç‡æ–¹æ³•ï¼Œè€Œæ˜¯ç»“æ„æ–¹æ³•ã€‚é€šå¸¸ï¼Œè®¡é‡ç»æµæ¨¡å‹å’Œå®šé‡åˆ†æéƒ½æ˜¯åœ¨æ¨¡å‹æ­£ç¡®è®¾å®šçš„å‡è®¾ä¸‹è¿›è¡Œçš„ã€‚ç»“æ„æ–¹æ³•åˆ™å¼•å‡ºäº†ä¼¼ç„¶åˆ†æï¼Œä¾‹å¦‚æå¤§ä¼¼ç„¶ä¼°è®¡`ï¼ˆMLEï¼‰`å’Œè´å¶æ–¯ä¼°è®¡`ï¼ˆBEï¼‰`ã€‚ä½†æ˜¯ç»“æ„æ–¹æ³•æœ€å¤§çš„ç¼ºç‚¹æ˜¯è®¤ä¸ºç»æµæ¨¡å‹è®¾å®šæ­£ç¡®ã€‚

ä½†æ˜¯ï¼Œåº”è¯¥æŠŠæ¨¡å‹å½“åšç°å®ä¸–ç•Œçš„ä¸€ç§æŠ½è±¡å’Œè¿‘ä¼¼ã€‚å› æ­¤ï¼Œæ¨æ–­çš„å‡†ç»“æ„æ–¹æ³•å°±æŠŠæ¨¡å‹å½“åšä¸€ç§è¿‘ä¼¼ï¼Œè€ŒéçœŸå®çš„ã€‚è¿™ç§ç†è®ºå¼•å‡ºäº†â€œä¼ªçœŸå®å€¼â€`ï¼ˆpseudo-true valueï¼‰`ã€‚æ‹Ÿä¼¼ç„¶å‡½æ•°ã€æ‹Ÿ`MLE `å’Œæ‹Ÿä¼¼ç„¶æ¨æ–­ã€‚ä¸æ­¤ç´§å¯†è”ç³»çš„æ˜¯åŠå‚æ•°æ–¹æ³•ã€‚æ¦‚ç‡ç»æµæ¨¡å‹æ˜¯ä¸€ç§å±€éƒ¨è®¾å®šæ¨¡å‹ï¼Œæœ‰ä¸€äº›ç»æµç‰¹å¾å¹¶æ²¡æœ‰è¢«è®¾å®šã€‚è¿™ç§æ–¹æ³•å‘å±•äº†æœ€å°äºŒä¹˜`ï¼ˆLSï¼‰`ã€å¹¿ä¹‰çŸ©æ–¹æ³•`ï¼ˆGMMï¼‰`ã€‚è¿™æ˜¯ç›®å‰åº”ç”¨å¹¿æ³›çš„æ–¹æ³•ã€‚

å®šé‡ç»“æ„æ¨¡å‹çš„å¦ä¸€ä¸ªåˆ†æ”¯å°±æ˜¯æ ¡å‡†æ–¹æ³•ã€‚ä¸å‡†ç»“æ„æ–¹æ³•ç›¸ä¼¼ï¼Œæ ¡å‡†æ–¹æ³•æŠŠæ¨¡å‹ç†è§£ä¸ºä¸€ç§è¿‘ä¼¼ã€‚å®ƒä»¬ä¹‹é—´çš„åŒºåˆ«åœ¨äºï¼Œæ ¡å‡†æ–¹æ³•æ‹’ç»ç»Ÿè®¡æ¨æ–­ï¼Œè€Œæ˜¯ç”¨æ¨¡å‹ä¸æ•°æ®çŸ©åŒ¹é…çš„æ–¹æ³•æ¥é€‰æ‹©å‚æ•°ã€‚è¿™æ˜¯å®è§‚è®¡é‡ä¸­çš„ä¸»è¦æ–¹æ³•ã€‚

### 2.è®¡é‡ç»æµå­¦ä¸­å¸¸è§çš„æ¦‚å¿µ

æœ€å¸¸ç”¨çš„è®¡é‡ç»æµå­¦æ¦‚å¿µå°±æ˜¯æ•°æ®ã€æ•°æ®é›†å’Œæ ·æœ¬ã€‚

ç»æµå­¦å®¶æ€»æ˜¯é¢å¯¹ç€æœ‰å…³å˜é‡çš„ä¸€ç³»åˆ—é‡å¤æµ‹é‡å€¼ã€‚å¯¹äºå˜é‡çš„ä¸åŒé‡å¤æµ‹é‡ç§°ä¸ºè§‚æµ‹å€¼ã€‚ç»æµå­¦å®¶é€šå¸¸ç”¨xï¼Œy æˆ–z æ¥è¡¨ç¤ºè§‚æµ‹å€¼ã€‚è®¡é‡ç»æµå­¦ä¸­ï¼Œé€šå¸¸ç”¨y æ¥è¡¨ç¤ºè¢«è§£é‡Šå˜é‡/å› å˜é‡ï¼Œè€Œx å’Œz è¡¨ç¤ºè§£é‡Šå˜é‡/è‡ªå˜é‡ã€‚å®æ•°ç”¨å°å†™å­—æ¯è¡¨ç¤ºï¼Œä¾‹å¦‚yï¼›å‘é‡ç”¨ç²—ä½“å°å†™å­—æ¯è¡¨ç¤ºï¼Œä¾‹å¦‚xã€‚

åŠ ç²—å¤§å†™å­—æ¯$X$ è¡¨ç¤ºçŸ©é˜µã€‚

å¸¦ä¸‹æ ‡iï¼ˆæœ‰æ—¶å€™ä¹Ÿç”¨j æˆ–å…¶å®ƒå­—æ¯è¡¨ç¤ºï¼‰çš„å˜é‡è¡¨ç¤ºè§‚æµ‹å€¼ï¼Œä¾‹å¦‚$y_ğ‘–$ï¼Œ$x_ğ‘–$å’Œ$z_ğ‘–$ã€‚æ­¤å¤–ï¼Œå¸¦æ—¶é—´ä¸‹æ ‡$t$çš„å˜é‡è¡¨ç¤ºæ—¶é—´åºåˆ—è§‚æµ‹å€¼ã€‚é¢æ¿æ•°æ®è§‚æµ‹å€¼å¸¦æœ‰$it$â€‹ä¸‹æ ‡ã€‚

å°å†™å¸Œè…Šå­—æ¯$ğ›½$ï¼Œ$ğœƒ$ ç­‰è¡¨ç¤ºè®¡é‡æ¨¡å‹çš„æœªçŸ¥å‚æ•°ã€‚åŠ ç²—å¸Œè…Šå­—æ¯$ğ›½$ï¼Œ$ğœƒ$ è¡¨ç¤ºç³»æ•°å‘é‡ã€‚

å›å½’æ¨¡å‹â€”â€”å¾®è§‚è®¡é‡ä¸­æœ€ä¸»è¦çš„æ¨¡å‹ï¼Œé‡åŒ–ä¸€ä¸ªå˜é‡å‘ç”Ÿå˜åŒ–å¯¼è‡´å¦ä¸€ä¸ªå˜é‡çš„å˜åŒ–ç¨‹åº¦ï¼ˆå› æœæ•ˆåº”ï¼‰ã€‚

### 3.æ•°æ®ã€æ•°æ®ç»“æ„ä¸æ•°æ®æ¥æº

#### ï¼ˆ1ï¼‰è§‚æµ‹æ•°æ®

è®¡é‡ç»æµå­¦é€šå¸¸å°±æ˜¯é‡åŒ–ä¸€ä¸ªå˜é‡å¯¹å¦ä¸€ä¸ªå˜é‡çš„å½±å“ã€‚ä»è‡ªç„¶ç§‘å­¦çš„è§’åº¦æ¥çœ‹ï¼Œæœ€ç†æƒ³çš„æƒ…å½¢å°±æ˜¯åˆ©ç”¨å®éªŒæ•°æ®æ¥å›ç­”è¿™äº›é—®é¢˜ã€‚ä½†æ˜¯ï¼Œåœ¨ç»æµå­¦æˆ–è€…ç¤¾ä¼šç§‘å­¦ä¸­åšå®éªŒã€‚

è¦ä¹ˆæˆæœ¬å¾ˆå¤§ï¼Œä¸èƒ½è®©ä¸€ä¸ªä¼ä¸šç¼´çº³30% ç¨ç‡ï¼Œå¦ä¸€ä¸ªä¼ä¸šåªç¼´çº³10% ç¨ç‡ï¼›è¦ä¹ˆï¼Œä¸é“å¾·ï¼Œæ•™è‚²çš„å½±å“ï¼Œè®©ä¸€éƒ¨åˆ†å­©å­ä¸ä¸Šå­¦ï¼Œè¿™å¤ªä¸é“å¾·äº†ã€‚

å› æ­¤ï¼Œå¤§å¤šæ•°çš„ç»æµæ•°æ®æ˜¯å¯**è§‚æµ‹æ•°æ®**ã€‚ï¼ˆæ³¨æ„ï¼šç›®å‰çš„å®éªŒç»æµå­¦æ˜¯å¯ä»¥å¾—åˆ°æŸäº›å®éªŒæ•°æ®çš„ã€‚ï¼‰ä¾‹å¦‚ï¼Œæˆ‘ä»¬é€šå¸¸èƒ½æ”¶é›†åˆ°æ•™è‚²ä¸å·¥èµ„çš„è®°å½•æ•°æ®ï¼Œæ®æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æµ‹ç®—ä¸¤ä¸ªå˜é‡çš„è”åˆåˆ†å¸ƒã€‚ä½†æˆ‘ä»¬å¹¶ä¸èƒ½ä»è§‚æµ‹æ•°æ®ä¸­æ¨æ–­å®ƒä»¬ä¹‹é—´çš„å› æœå…³ç³»ã€‚å› ä¸ºæˆ‘ä»¬ä¸èƒ½æ“çºµä¸ªäººæ•™è‚²å±‚æ¬¡å’Œå¹´é™ï¼Œæ¥è§‚æµ‹ä»–çš„ä¸åŒå·¥èµ„ç»“æœã€‚

#### ï¼ˆ2ï¼‰æ•°æ®ç»“æ„

äº”ç§ä¸»è¦çš„æ•°æ®ç»“æ„ï¼š

- æˆªé¢æ•°æ®
- æ—¶é—´åºåˆ—æ•°æ®
- é¢æ¿æ•°æ®
- èšç±»æ•°æ®ï¼šä¸é¢æ¿æ•°æ®ç›¸å…³ã€‚åœ¨èšç±»æŠ½æ ·ä¸­ï¼Œè§‚æµ‹å€¼è¢«å½’ç±»â€”â€”ç±»åˆ«é—´ç›¸äº’ç‹¬ç«‹ï¼Œç±»
  åˆ«ä¸­ç›¸å…³ã€‚ä¸é¢æ¿æ•°æ®çš„ä¸»è¦å·®åˆ«åœ¨äºï¼Œèšç±»æŠ½æ ·å¹¶ä¸æ˜¾æ€§å»ºæ¨¡è¯¯å·®ç»“æ„ã€‚
- ç©ºé—´æ•°æ®ï¼šæ ¹æ®ç©ºé—´æŒ‡æ ‡è€Œå…·æœ‰ç›¸äº’ä¾èµ–æ€§ã€‚

#### ï¼ˆ3ï¼‰æ•°æ®æ¥æº

ç›®å‰ï¼Œæœ‰è®¸å¤šå…¬å¼€çš„æ•°æ®æ¥æºï¼š

- å›½å®¶ç»Ÿè®¡å±€
- å„ç§ç±»å‹çš„ç»Ÿè®¡å¹´é‰´
- CGSS
- å…¶å®ƒå¾®è§‚è°ƒç ”æ•°æ®

## äºŒã€æ¦‚ç‡ä¸ç»Ÿè®¡åŸºç¡€

### 1.æè¿°æ€§ç»Ÿè®¡

#### ï¼ˆ1ï¼‰ç¦»æ•£åˆ†å¸ƒï¼šé¢‘ç‡ä¸åˆ—è”è¡¨

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéšæœºå˜é‡`x`å’Œ`y`çš„æ ·æœ¬ï¼Œåˆ†åˆ«å­˜å‚¨åœ¨`numpy`æˆ–`pandas`æ•°æ®ç±»å‹`X`å’Œ`Y`ä¸­ã€‚å¯¹äºç¦»æ•£å˜é‡ï¼Œæœ€åŸºæœ¬çš„ç»Ÿè®¡æ˜¯ç»“æœçš„é¢‘ç‡ã€‚`numpy`å‘½ä»¤`unique(z,returm counts=True)`æˆ–`pandas`å‘½ä»¤`x.value.count()`è¿”å›è¿™æ ·ä¸€ä¸ªè®¡æ•°è¡¨ã€‚å¦‚æœæˆ‘ä»¬å¯¹åˆ—è”è¡¨æ„Ÿå…´è¶£ï¼Œå³å˜é‡xå’Œyçš„æ¯ä¸ªç»“æœç»„åˆçš„è®¡æ•°ï¼Œæˆ‘ä»¬å°†å…¶æä¾›ç»™`pandas`ä¸­çš„`crosstab`å‡½æ•°ã€‚

ä¸ºäº†è·å¾—æ ·æœ¬ä»½é¢è€Œä¸æ˜¯è®¡æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°†å‡½æ•°å‚æ•°å½’ä¸€åŒ–:

```python
æ€»ä½“æ ·æœ¬ä»½é¢:äº¤å‰è¡¨(x,y,normalize='all')
xå€¼å†…çš„ä»½é¢(è¡Œç™¾åˆ†æ¯”):äº¤å‰è¡¨(x,y,normalize='index')
yå€¼ä¸­çš„ä»½é¢(åˆ—ç™¾åˆ†æ¯”):äº¤å‰è¡¨(x,y,normalize='columns')
```

ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹`Script 1.22 (Descr-Tables.py)`ä¸­çš„æ•°æ®é›†äº‹åŠ¡ã€‚æˆ‘ä»¬ç”¨ä¸¤ä¸ªå˜é‡æ¼”ç¤º`numpy`å’Œ`pandas`å‘½ä»¤çš„å·¥ä½œåŸç†ï¼š

```python
å¦‚æœåº”ç­”è€…è‡³å°‘æœ‰ä¸€ä¸ªå­©å­ï¼Œåˆ™kids=1
ratemar = å¯¹è‡ªå·±å©šå§»çš„è¯„ä»·(1=éå¸¸ä¸å¹¸ç¦ï¼Œâ€¦ï¼Œ5-éå¸¸å¹¸ç¦)
```

æ¡ˆä¾‹ï¼š

```python
import wooldridge as woo
import numpy as np
import pandas as pd

affairs = woo.dataWoo("affairs")
# adjust codings to [0-4] (Categoricals require a start from 0): 
affairs["ratemarr"] = affairs["ratemarr"] - 1
# use a pandas.Categorical object to attach labels for "haskids": 
affairs["haskids"] = pd.Categorical.from_codes(affairs["kids"], categories=["no", "yes"])
# ... and "marriage" (for example: 0 = "very unhappy", 1 = "unhappy",...): 
mlab = ["very unhappy", "unhappy", "average", "happy", "very happy"]
affairs["marriage"] = pd.Categorical.from_codes(affairs["ratemarr"], categories=mlab)
# frequency table in numpy (alphabetical order of elements): 
ft_np = np.unique(affairs["marriage"], return_counts=True)
unique_elem_np = ft_np[0]
counts_np = ft_np[1]
print(f"unique_elem_np: {unique_elem_np}")
print(f"counts_np: {counts_np}")
# frequency table in pandas: 
ft_pd = affairs["marriage"].value_counts()
print(f"ft_pd: {ft_pd}")

# frequency table with groupby: 
ft_pd2 = affairs["marriage"].groupby(affairs["haskids"]).value_counts()
print(f"ft_pd2: {ft_pd2}")
# contingency table in pandas: 
ct_all_abs = pd.crosstab(affairs["marriage"], affairs["haskids"], margins=3)
print(f"ct_all_abs: {ct_all_abs}")
ct_all_rel = pd.crosstab(affairs["marriage"], affairs["haskids"],normalize="all")
print(f"ct_all_rel: {ct_all_rel}")
# share within "marriage" (i.e. within a row): 
ct_row = pd.crosstab(affairs["marriage"], affairs["haskids"], normalize="index")
print(f"ct_row: {ct_row}")
# share within "haskids" (i.e. within a column): 
ct_col = pd.crosstab(affairs["marriage"], affairs["haskids"],normalize="columns")
print(f"ct_col: {ct_col}")
```

åœ¨`Python`è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»æ•°æ®é›†`affairs`æä¾›çš„ç¼–ç å€¼ä¸­ç”Ÿæˆæ„Ÿå…´è¶£çš„ä¸¤ä¸ªå˜é‡çš„åˆ†ç±»ç‰ˆæœ¬ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºç»“æœç”Ÿæˆå¸¦æœ‰æœ‰æ„ä¹‰æ ‡ç­¾è€Œä¸æ˜¯æ•°å­—çš„è¡¨ï¼Œå‚è§`1.2.4`èŠ‚ã€‚ç„¶åç”Ÿæˆä¸åŒçš„è¡¨ã€‚åœ¨`601`åå—è®¿è€…ä¸­ï¼Œæœ‰`430`äºº(`71.5%`)æœ‰å­©å­ã€‚æ€»ä½“è€Œè¨€ï¼Œ`16`åå—è®¿è€…è¡¨ç¤ºå¯¹è‡ªå·±çš„å©šå§»éå¸¸ä¸æ»¡æ„ï¼Œ`232`åå—è®¿è€…è¡¨ç¤ºéå¸¸å¹¸ç¦ã€‚ä¾‹å¦‚ï¼Œåœ¨æœ‰è®¡æ•°çš„åˆ—è”è¡¨ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°`136`åå—è®¿è€…éå¸¸å¹¸ç¦ï¼Œå¹¶æœ‰äº†å­©å­ã€‚

æŠ¥å‘Šè¡Œå†…ä»½é¢çš„è¡¨æ ¼(`ct row`)å‘Šè¯‰æˆ‘ä»¬ï¼Œä¾‹å¦‚ï¼Œ`81.25%`çš„éå¸¸ä¸å¿«ä¹çš„äººæœ‰å­©å­ï¼Œåªæœ‰`58.6%`çš„éå¸¸å¿«ä¹çš„å—è®¿è€…æœ‰å­©å­ã€‚æœ€åä¸€ä¸ªè¡¨æ ¼åˆ†åˆ«æŠ¥å‘Šäº†æœ‰å­©å­å’Œæ²¡æœ‰å­©å­çš„äººçš„å©šå§»è¯„çº§åˆ†å¸ƒ:`56.1%`çš„æ²¡æœ‰å­©å­çš„å—è®¿è€…éå¸¸å¹¸ç¦ï¼Œè€Œåªæœ‰`31.6%`çš„æœ‰å­©å­çš„äººæŠ¥å‘Šè¯´ä»–ä»¬çš„å©šå§»å¾ˆå¹¸ç¦ã€‚åœ¨ä¸ºä½ è‡ªå·±çš„è®¡åˆ’ç”Ÿè‚²å¾—å‡ºä»»ä½•ç»“è®ºä¹‹å‰ï¼Œè¯·è‡³å°‘ç»§ç»­ç ”ç©¶è®¡é‡ç»æµå­¦ï¼Œç›´åˆ°ä½ å……åˆ†è®¤è¯†åˆ°ç›¸å…³æ€§å’Œå› æœå…³ç³»çš„åŒºåˆ«ï¼

æœ‰å‡ ç§æ–¹æ³•å¯ä»¥å›¾å½¢åŒ–åœ°æç»˜å‡ºè¿™äº›è¡¨æ ¼ä¸­çš„ä¿¡æ¯ã€‚è„šæœ¬`1.23(Descr-Figures.py)`æ¼”ç¤ºäº†åˆ†åˆ«ä½¿ç”¨å‘½ä»¤`pie`å’Œ`bar`åˆ›å»ºåŸºæœ¬é¥¼å›¾å’ŒæŸ±çŠ¶å›¾ã€‚è¿™äº›å›¾å½¢å½“ç„¶å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼è¿›è¡Œè°ƒæ•´ï¼Œå‚è§`1.4`èŠ‚ä¸­çš„å¸®åŠ©é¡µé¢å’Œå›¾å½¢çš„ä¸€èˆ¬è®¨è®ºã€‚æ¢ç´¢è¿™äº›é€‰é¡¹çš„æœ€ä½³æ–¹å¼æ˜¯å¯¹è§„èŒƒè¿›è¡Œä¿®è¡¥ï¼Œå¹¶è§‚å¯Ÿç»“æœã€‚

```python
import wooldridge as woo
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


affairs = woo.dataWoo("affairs")
# attach labels (see previous script): 
affairs["ratemarr"] = affairs["ratemarr"] - 1
affairs["haskids"] = pd.Categorical.from_codes(affairs["kids"], categories=["no", "yes"])
mlab = ["very unhappy", "unhappy", "average", "happy", "very happy"]
affairs["marriage"] = pd.Categorical.from_codes(affairs["ratemarr"], categories=mlab)
# counts for all graphs: 
counts = affairs["marriage"].value_counts()
counts_bykids = affairs["marriage"].groupby(affairs["haskids"]).value_counts()
counts_yes = counts_bykids["yes"]
counts_no = counts_bykids["no"]
# pie chart (a): 
grey_colors = ["0.3", "0.4", "0.5", "0.6", "0.7"]
plt.pie(counts, labels=mlab, colors=grey_colors)
# plt.savefig("PyGraphs/Descr-Pie.pdf")
plt.close()
# horizontal bar chart (b): 
y_pos = [0, 1, 2, 3, 4]
# the y locations for the bars
plt.barh(y_pos, counts, color="0.6")
plt.yticks(y_pos, mlab, rotation=60)
# add and adjust labeling
# plt.savefig("PyGraphs/Descr-Bar1.pdf")
plt.close()
# stacked bar plot (c):
x_pos = [0, 1, 2, 3, 4]
# the x locations for the bars 
plt.bar(x_pos, counts_yes, width=0.4, color="0.6", label="Yes")
# with "bottom=counts_yes" bars are added on top of previous ones:
plt.bar(x_pos, counts_no, width=0.4, bottom=counts_yes, color="0.3", label="No") 
plt.ylabel("Counts")
plt.xticks(x_pos, mlab)
# add labels on x axis 
plt.legend()
# plt.savefig("PyGraphs/Descr-Bar2.pdf")
plt.close()
# grouped bar plot (d) 
# add left bars first and move bars to the left: 
x_pos_leftbar = [-0.2, 0.8, 1.8, 2.8, 3.8]
plt.bar(x_pos_leftbar, counts_yes, width=0.4, color="0.6", label="Yes")
# add right bars first and move bars to the right: 
x_pos_rightbar = [0.2, 1.2, 2.2, 3.2, 4.2]
plt.bar(x_pos_rightbar, counts_no, width=0.4, color="0.3", label="No")
plt.ylabel("Counts")
plt.xticks(x_pos, mlab)
plt.legend()
# plt.savefig("PyGraphs/Descr-Bar3.pdf")
plt.show()
```

![image-20240403135234414](imgs/image-20240403135234414.png)

#### ï¼ˆ2ï¼‰è¿ç»­åˆ†å¸ƒ:ç›´æ–¹å›¾å’Œå¯†åº¦

å¯¹äºè¿ç»­å˜é‡ï¼Œæ¯ä¸ªè§‚æµ‹å€¼éƒ½æœ‰ä¸åŒçš„å€¼ã€‚åœ¨å®è·µä¸­ï¼Œå…·æœ‰è®¸å¤š(ä½†ä¸æ˜¯æ— é™å¤š)ä¸åŒå€¼çš„å˜é‡å¯ä»¥ç”¨åŒæ ·çš„æ–¹æ³•å¤„ç†ã€‚ç”±äºæ¯ä¸ªå€¼åœ¨æ•°æ®ä¸­åªå‡ºç°ä¸€æ¬¡(æˆ–éå¸¸å°‘çš„ä¸€æ¬¡)ï¼Œé¢‘ç‡è¡¨æˆ–æŸ±çŠ¶å›¾æ˜¯æ²¡æœ‰ç”¨çš„ã€‚ç›¸åï¼Œè¿™äº›å€¼å¯ä»¥æŒ‰åŒºé—´åˆ†ç»„ã€‚ç„¶åï¼Œè¿™äº›é—´éš”å†…å€¼çš„é¢‘ç‡å¯ä»¥è¢«åˆ¶æˆè¡¨æ ¼æˆ–åœ¨ç›´æ–¹å›¾ä¸­æè¿°ã€‚
åœ¨`Python`æ¨¡å—`matplotib`ä¸­ï¼Œå‡½æ•°`hist(x,options)`å°†è§‚æµ‹å€¼åˆ†é…ç»™å¯ä»¥æ‰‹åŠ¨è®¾ç½®æˆ–è‡ªåŠ¨é€‰æ‹©çš„åŒºé—´ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç›´æ–¹å›¾ï¼Œå°†xçš„å€¼ä¸ç›¸åº”binä¸­çš„è®¡æ•°æˆ–å¯†åº¦ç»˜åˆ¶åœ¨ä¸€èµ·ã€‚æœ€ç›¸å…³çš„é€‰é¡¹æ˜¯

```python
åƒåœ¾ç®±=â€¦:è®¾ç½®é—´éš”è¾¹ç•Œ:
-no binsæŒ‡å®š:è®©Pythoné€‰æ‹©numberå’Œpositionã€‚
-bins =nç”¨äºæ ‡é‡n:é€‰æ‹©binsçš„æ•°é‡ï¼Œä½†è®©Pythoné€‰æ‹©ä½ç½®ã€‚
-bins =vç”¨äºåˆ—è¡¨v:æ˜¾å¼è®¾ç½®è¾¹ç•Œã€‚
å¯†åº¦-True:ä¸è¦ä½¿ç”¨è®¡æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨çºµè½´ä¸Šçš„å¯†åº¦ã€‚
```

è®©æˆ‘ä»¬çœ‹çœ‹åœ¨`Wooldridge(2019ï¼Œä¾‹2.3)`ä¸­æè¿°å’Œä½¿ç”¨çš„æ•°æ®é›†`CEOSAL1`ã€‚å®ƒåŒ…å«äº†`ceo`çš„å·¥èµ„ä¿¡æ¯å’Œå…¶ä»–ä¿¡æ¯ã€‚æˆ‘ä»¬å°†å°è¯•æè¿°è‚¡æœ¬å›æŠ¥ç‡(ROE)çš„åˆ†å¸ƒï¼Œä»¥ç™¾åˆ†æ¯”è¡¡é‡ã€‚

è„šæœ¬`1.24(Histogram.py)`ç”Ÿæˆäº†å›¾1.11çš„å›¾å½¢ã€‚åœ¨å­å›¾(b)ä¸­ï¼Œæ–­ç‚¹æ˜¯æ‰‹åŠ¨é€‰æ‹©çš„ï¼Œé—´éš”ä¸æ˜¯ç›¸ç­‰çš„ã€‚è®¾ç½®å¯†åº¦=Trueç»™å‡ºäº†çºµè½´ä¸Šçš„å¯†åº¦:å› æ­¤ï¼Œä¸€ä¸ªå®¹å™¨å†…çš„è§‚æµ‹æ ·æœ¬ä»½é¢æ˜¯ç”±å„è‡ªçŸ©å½¢çš„é¢ç§¯åæ˜ çš„ï¼Œè€Œä¸æ˜¯é«˜åº¦ã€‚

```python
import wooldridge as woo
import matplotlib.pyplot as plt


ceosal1 = woo.dataWoo("ceosal1")
# extract roe:
roe = ceosal1["roe"]
# subfigure a (histogram with counts):
plt.hist(roe, color="grey")
plt.ylabel("Counts")
plt.xlabel("roe")
# plt.savefig("PyGraphs/Histogram1.pdf")
plt.close()
# subfigure b (histogram with density and explicit breaks): 
breaks = [0, 5, 10, 20, 30, 60]
plt.hist(roe, color="grey", bins=breaks, density=True)
plt.ylabel("density")
plt.xlabel("roe")
# plt.savefig("PyGraphs/Histogram2.pdf")
plt.show()
```

æ ¸å¯†åº¦å›¾å¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ç§æ›´å¤æ‚çš„ç›´æ–¹å›¾ã€‚æˆ‘ä»¬ä¸èƒ½åœ¨è¿™é‡Œè¯¦ç»†è¯´æ˜ï¼Œä½†ä¸€ä¸ªç›´è§‚çš„(è€Œä¸”è¿‡äºç®€åŒ–çš„)æ€è€ƒæ–¹å¼æ˜¯è¿™æ ·çš„ï¼šæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä¸€å®šå®½åº¦çš„ç›´æ–¹å›¾åº“ï¼Œä»¥xçš„ä»»æ„ç‚¹ä¸ºä¸­å¿ƒã€‚æˆ‘ä»¬å°†å¯¹è®¸å¤šç‚¹è¿™æ ·åšï¼Œå¹¶å°†è¿™äº›xå€¼ä¸ç»“æœå¯†åº¦ç»˜åˆ¶åœ¨ä¸€èµ·ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ä¼šä½¿ç”¨è¿™ä¸ªç»˜å›¾ä½œä¸ºæ€»ä½“åˆ†å¸ƒçš„ä¼°è®¡ï¼Œè€Œæ˜¯ä½œä¸ºæ ·æœ¬åˆ†å¸ƒçš„æè¿°æ€§æè¿°çš„ç›´æ–¹å›¾çš„ä¸€ä¸ªå¾ˆå¥½çš„æ›¿ä»£å“ã€‚è¯¦æƒ…è¯·å‚è§ä¾‹å¦‚Silverman(1986)ã€‚

é€šè¿‡æ¨¡å—`statsmomodels`ç”Ÿæˆä¸€ä¸ªæ ¸å¯†åº¦å›¾å¾ˆç®€å•:`nonparameter .kdeunivariate(x).fit()`ä¼šè‡ªåŠ¨åœ¨`Pvthon`é€‰æ‹©åˆé€‚çš„å‚æ•°ã€‚

![image-20240403135617784](imgs/image-20240403135617784.png)

åœ¨ç»™å®šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šå¸¸ä¼šäº§ç”Ÿæœ‰ç”¨çš„ç»“æœå½“ç„¶ï¼Œè¿™äº›å‚æ•°(æ¯”å¦‚å†…æ ¸å’Œå¸¦å®½ï¼Œå¯¹äºé‚£äº›çŸ¥é“é‚£æ˜¯ä»€ä¹ˆçš„äººæ¥è¯´å¯ä»¥æ‰‹åŠ¨è®¾ç½®ã€‚

è„šæœ¬`1.25 (KDensity.py)`æ¼”ç¤ºäº†å¦‚ä½•ç”¨`matplotlib`ç»˜åˆ¶å¯†åº¦ä¼°è®¡çš„ç»“æœ,ï¼Œå¹¶ç”Ÿæˆå›¾1.12çš„å›¾å½¢ã€‚åœ¨å­å›¾(b)ä¸­ï¼Œç›´æ–¹å›¾ä¸æ ¸å¯†åº¦å›¾é‡å ã€‚

```python
import wooldridge as woo
import statsmodels.api as sm
import matplotlib.pyplot as plt

ceosal1 = woo.dataWoo("ceosal1")
# extract roe:
roe = ceosal1["roe"]
# estimate kernel density:
kde = sm.nonparametric.KDEUnivariate(roe)
kde.fit()
# subfigure a (kernel density):
plt.plot(kde.support, kde.density, color="black", linewidth=2)
plt.ylabel("density")
plt.xlabel("roe")
# plt.savefig("PyGraphs/Density1.pdf")
plt.close()
# subfigure b (kernel density with overlayed histogram): 
plt.hist(roe, color="grey", density=True)
plt.plot(kde.support, kde.density, color="black", linewidth=2)
plt.ylabel("density")
plt.xlabel("roe")
# plt.savefig("PyGraphs/Density2.pdf")
```

![image-20240403135748200](imgs/image-20240403135748200.png)

```python
[14] The module statsmodels will be introduced in Chapter 2.
```

#### ï¼ˆ3ï¼‰ç»éªŒç´¯ç§¯åˆ†å¸ƒå‡½æ•°(ECDE)

`ECDF`æ˜¯ä¸€ä¸ªå˜é‡çš„æ‰€æœ‰å€¼xä¸å€¼å°äºæˆ–ç­‰äº`x`çš„è§‚æµ‹å€¼çš„æ¯”ä¾‹çš„å›¾è¡¨ã€‚ç»˜åˆ¶`ROE`å˜é‡çš„`ECDF`çš„ç›´æ¥æ–¹æ³•è§è„šæœ¬`1.26 (desr -ECDF.py)`å’Œå›¾1.13ã€‚æ›´è‡ªåŠ¨åŒ–çš„æ–¹æ³•æ˜¯ä½¿ç”¨`statmodels`å‡½æ•°`distribution.empirical distribution.ecdf(x)`ï¼Œå®ƒä¼šç»™å‡ºç›¸åŒçš„ç»“æœã€‚ä¾‹å¦‚ï¼Œå¯¹äºç‚¹`roe=15.5`ï¼Œ`ECDF`çš„å€¼ä¸º`0.5`ã€‚åŠæ•°æ ·æœ¬çš„`ROE`å°äºæˆ–ç­‰äº`15.5%`ã€‚æ¢å¥è¯è¯´`ROE`ä¸­ä½æ•°ä¸º`15.5%`ã€‚

```python
import wooldridge as woo
import numpy as np
import matplotlib.pyplot as plt
ceosal1 = woo.dataWoo("ceosal1")
# extract roe: 
roe = ceosal1["roe"]
# calculate ECDF: 
x = np.sort(roe)
n = x.size
y = np.arange(1, n + 1) / n
# generates cumulative shares of observations
# plot a step function: 
plt.step(x, y, linestyle="-", color="black")
plt.xlabel("roe")
# plt.savefig("PyGraphs/ecdf.pdf")
```

![image-20240403135950592](imgs/image-20240403135950592.png)

#### ï¼ˆ4ï¼‰åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯

ç”¨`numpy`è®¡ç®—æœ€é‡è¦çš„æè¿°æ€§ç»Ÿè®¡çš„å‡½æ•°åˆ—åœ¨è¡¨`1.5`ä¸­ã€‚è„šæœ¬`1.27 (Descr-Stats.py)`ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬`1.5.2`èŠ‚ä¸­å·²ç»ä»‹ç»è¿‡çš„`CEOSAL1`æ•°æ®é›†æ¼”ç¤ºäº†è¿™ä¸€ç‚¹ã€‚

![image-20240403140011918](imgs/image-20240403140011918.png)

```python
import wooldridge as woo
import numpy as np

ceosal1 = woo.dataWoo("ceosal1")
# extract roe and salary: 
roe = ceosal1["roe"]
salary = ceosal1["salary"]
# sample average: 
roe_mean = np.mean(salary)
print(f"roe_mean: {roe_mean}")
# sample median: 
roe_med = np.median(salary)
print(f"roe_med: {roe_med}")
# standard deviation:
roe_s = np.std(salary, ddof=1)
print(f"roe_s: {roe_s}")
# correlation with ROE: 
roe_corr = np.corrcoef(roe, salary)
print(f"roe_corr: {roe_corr}")
```

ç›’å­å›¾ä»¥å›¾å½¢çš„æ–¹å¼æ˜¾ç¤ºäº†ä¸­ä½æ•°(ä¸­é—´çš„çº¿)ã€ä¸Šä¸‹å››åˆ†ä½æ•°(æ–¹æ¡†)å’Œæå€¼ç‚¹ã€‚å›¾1.14å±•ç¤ºäº†ä¸¤ä¸ªä¾‹å­ã€‚50%çš„è§‚æµ‹å€¼åœ¨æ–¹æ¡†è¦†ç›–çš„åŒºé—´å†…ï¼Œ`25%`åœ¨ä¸Šæ–¹ï¼Œ`25%`åœ¨ä¸‹æ–¹ã€‚æå€¼ç‚¹è¢«â€œèƒ¡é¡»â€æ ‡è®°ï¼Œå¼‚å¸¸å€¼è¢«æ‰“å°ä¸ºå•ç‹¬çš„ç‚¹ã€‚åœ¨`matplotlib`ä¸­ï¼Œä½¿ç”¨`boxplot`å‘½ä»¤ç”Ÿæˆç®±çº¿å›¾ã€‚æˆ‘ä»¬å¿…é¡»æä¾›ä¸€ä¸ªæˆ–å¤šä¸ªæ•°æ®æ•°ç»„ï¼Œå¹¶å¯ä»¥åƒè„šæœ¬1.28 (`Descr-Boxplot.py`)ä¸­æ¼”ç¤ºçš„é‚£æ ·ï¼Œé€šè¿‡å¤§é‡é€‰é¡¹çµæ´»åœ°æ”¹å˜è®¾è®¡ã€‚

```python
import wooldridge as woo
import matplotlib.pyplot as plt

ceosal1 = woo.dataWoo("ceosal1")
# extract roe and salary: 
roe = ceosal1["roe"]
consprod = ceosal1["consprod"]
# plotting descriptive statistics:
plt.boxplot(roe, vert=False)
plt.ylabel("roe")
# plt.savefig("PyGraphs/Boxplot1.pdf")
plt.close()
# plotting descriptive statistics: 
roe_cp0 = roe[consprod == 0]
roe_cp1 = roe[consprod == 1]
plt.boxplot([roe_cp0, roe_cp1])
plt.ylabel("roe")
# plt.savefig("PyGraphs/Boxplot2.pdf")
plt.show()
```

![image-20240403140227716](imgs/image-20240403140227716.png)

å›¾1.14(a)å±•ç¤ºäº†å¦‚ä½•å¾—åˆ°ä¸€ä¸ªæ°´å¹³å¯¹é½çš„å›¾å½¢ï¼Œå›¾`1.14(6)`å±•ç¤ºäº†å¦‚ä½•ä¸ºä¸¤ä¸ªå­ç»„ç”Ÿæˆå¤šä¸ªç®±å½¢å›¾ã€‚æ¥è‡ªæ•°æ®é›†`ceosall`çš„å˜é‡`conprod`åœ¨ä¼ä¸šå¤„äºæ¶ˆè´¹å“ä¸šåŠ¡æ—¶ç­‰äº1ï¼Œå¦åˆ™ç­‰äº0.æ˜¾ç„¶ï¼Œè¿™ä¸ªè¡Œä¸šçš„ROEè¦é«˜å¾—å¤šã€‚

### 2.æ¦‚ç‡åˆ†å¸ƒ

![image-20240403140412336](imgs/image-20240403140412336.png)

`Wooldridge(2019)`é™„å½•Bä»‹ç»äº†éšæœºå˜é‡åŠå…¶æ¦‚ç‡åˆ†å¸ƒçš„æ¦‚å¿µã€‚`15scipy`æ¨¡å—æœ‰è®¸å¤šåŠŸèƒ½ï¼Œæ–¹ä¾¿åœ°å¤„ç†å¤§é‡çš„ç»Ÿè®¡åˆ†å¸ƒã€‚16å¯¹äºè¿ç»­åˆ†å¸ƒï¼Œè¯„ä¼°æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå‡½æ•°`(PDF)`ï¼›å¯¹äºç¦»æ•£åˆ†å¸ƒï¼Œè¯„ä¼°æ¦‚ç‡ç¾¤åˆ†å¸ƒå‡½æ•°`(PMF)`ï¼šå¯¹äºæœ€ç›¸å…³çš„åˆ†å¸ƒï¼Œè¯„ä¼°ç´¯ç§¯åˆ†å¸ƒå‡½æ•°`(CDF)`ä»¥åŠåˆ†ä½æ•°å‡½æ•°(é€†`CDF`)çš„å‘½ä»¤å¦‚è¡¨1.6æ‰€ç¤ºã€‚è¿™äº›å‡½æ•°åœ¨æ‰§è¡Œåå¯ç”¨

```python
import scipy.stats as stats
```

æ¨¡å—æ–‡æ¡£å®šä¹‰äº†å‘è¡Œç‰ˆçš„å‚æ•°é›†ä¸ `scipy `ä¸­çš„å‡½æ•°å‚æ•°ä¹‹é—´çš„å…³ç³»ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ç®€è¦è®¨è®ºæ¯ç§å‡½æ•°ç±»å‹ã€‚

#### ï¼ˆ1ï¼‰ç¦»æ•£åˆ†å¸ƒ

ç¦»æ•£éšæœºå˜é‡åªèƒ½å–æœ‰é™(æˆ–â€œå¯æ•°æ— é™â€)çš„å€¼é›†åˆã€‚`PMF f (x) = P(X = x)`ç»™å‡ºäº†å…·æœ‰æ­¤åˆ†å¸ƒçš„éšæœºå˜é‡xå–ç»™å®šå€¼xçš„æ¦‚ç‡ã€‚å¯¹äºè¿™äº›æœ€é‡è¦çš„åˆ†å¸ƒ`(Bernoulli, Binomial, Hypergeometric17, Poisson, and Geometric18)`ï¼Œè¡¨1.6åˆ—å‡ºäº†åœ¨ç»™å®šå„è‡ªåˆ†å¸ƒå‚æ•°çš„æƒ…å†µä¸‹è¿”å›ä»»æ„å€¼`x`çš„`PMF`çš„`scipy`å‡½æ•°ã€‚å¦‚æœä½ å¯¹`pmt`çš„å½¢å¼åŒ–å®šä¹‰æ„Ÿå…´è¶£ï¼Œè¯·å‚é˜…æ¨¡å—æ–‡æ¡£ã€‚

å¯¹äºä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œè®©Xè¡¨ç¤ºæˆ‘ä»¬ä»ä¸€ä¸ªåŒ…å«20%ç™½çƒçš„ç“®ä¸­ç»˜åˆ¶10ä¸ªçƒæ—¶å¾—åˆ°çš„ç™½çƒæ•°é‡ã€‚æ˜¯äºŒé¡¹åˆ†å¸ƒ

```python
[15] The stripped-down textbook for Europe and Africa Wooldridge (2014) does not include this appendix. But the material is pretty standard. 
[16] scipy is part of the Anaconda distribution and more information about the module is given in Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, van der Walt, Brett, Wilson, Jarrod Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng, Moore, Vand erPlas, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris, Archibald, Ribeiro, Pedregosa, van Mulbregt, and Contributors (2020).
[17] The parameters of the distribution are deï¬ned as follows: M is the total number of balls in an urn, n is the total number of marked balls in this urn, k is the number of drawn balls and x is number of drawn marked balls. 
[18] x is the total number of trials, i.e. the number of failures in a sequence of Bernoulli trials before a success occurs plus the success trial.
```

å‚æ•° `n = 10, p = 20% = 0.2`. æˆ‘ä»¬çŸ¥é“ç²¾ç¡®å¾—åˆ°` x âˆˆ {0, 1, . . . , 10} `è¿™ä¸ªåˆ†å¸ƒçš„ç™½çƒä¸º19
$$
f (x) = P(X = x) = \left(\begin{matrix}n \\x \\\end{matrix}\right) Â· p^x Â· (1 âˆ’ p)^{nâˆ’x} = \left(\begin{matrix}10 \\x \\\end{matrix}\right) Â· 0.2^x Â· 0.8^{10âˆ’x}
$$

ä¾‹å¦‚ï¼Œç²¾ç¡®å¾—åˆ°x=2ä¸ªç™½çƒçš„æ¦‚ç‡ä¸º`0.302`ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥è®©Pythonç”¨æˆ‘ä»¬åœ¨1.1èŠ‚ä¸­çŸ¥é“çš„åŸºæœ¬Pyhonå‘½ä»¤æ¥åšè¿™äº›è®¡ç®—ã€‚æ›´æ–¹ä¾¿çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨binomå‡½æ•°ã€‚äºŒé¡¹åˆ†å¸ƒçš„pmfï¼š

```python
import scipy.stats as stats
import math

# pedestrian approach:
c = math.factorial(10) / (math.factorial(2) * math.factorial(10 - 2))
p1 = c * (0.2 ** 2) * (0.8 ** 8)
print(f"p1: {p1}")

# scipy function: 
p2 = stats.binom.pmf(2, 10, 0.2)
print(f"p2: {p2}")
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†æ•°ç»„ä½œä¸ºä¸€ä¸ªæˆ–å¤šä¸ªå‚æ•°æä¾›ç»™`stats.binom.pmf(x,n,p)`ï¼Œå¹¶ä»¥æ•°ç»„çš„å½¢å¼æ¥æ”¶ç»“æœã€‚è„šæœ¬`1.30 (PMF-example.py)`åœ¨`x(0~10)`çš„æ‰€æœ‰å¯èƒ½å€¼ä¸Šè®¡ç®—æˆ‘ä»¬ç¤ºä¾‹çš„`PMF`ã€‚å®ƒæ˜¾ç¤ºäº†ä¸€ä¸ªæ¦‚ç‡è¡¨ï¼Œå¹¶åˆ›å»ºäº†è¿™äº›æ¦‚ç‡çš„æ¡å½¢å›¾ï¼Œå¦‚å›¾`1.15(a)`æ‰€ç¤ºã€‚ä¸€å¦‚æ—¢å¾€:é¼“åŠ±ä½ å»å°è¯•!

```python
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# values for x (all between 0 and 10):
x = np.linspace(0, 10, num=11)
# PMF for all these values: 
fx = stats.binom.pmf(x, 10, 0.2)
# collect values in DataFrame:
result = pd.DataFrame({"x": x, "fx": fx})
print(f"result: {result}")
# plot: 
plt.bar(x, fx, color="0.6")
plt.ylabel("x")
plt.ylabel("fx")
# plt.savefig("PyGraphs/PMF-example.pdf")
```

```python
[19] see Wooldridge (2019, Equation (B.14))
```

![image-20240403142222159](imgs/image-20240403142222159.png)

#### ï¼ˆ2ï¼‰è¿ç»­åˆ†å¸ƒ

å¯¹äºè¿ç»­åˆ†å¸ƒï¼Œå¦‚å‡åŒ€ï¼Œ`logistic`ï¼ŒæŒ‡æ•°ï¼Œæ­£æ€ï¼Œ`1`ï¼Œ`x2`ï¼Œæˆ–Fåˆ†å¸ƒï¼Œæ¦‚ç‡å¯†åº¦å‡½æ•°`F(x)`ä¹Ÿå¯ä»¥åœ¨`scipy`ä¸­ç›´æ¥ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œè¿™äº›å¯ä»¥ä½¿ç”¨`plot`å‘½ä»¤æ¥ç»˜åˆ¶å¯†åº¦å‡½æ•°(è§ç¬¬1.4èŠ‚)ã€‚å›¾`1.15(6)`æ˜¾ç¤ºäº†è‘—åçš„æ ‡å‡†å¸¸æ€åˆ†é…çš„é’Ÿå½¢`PDF`ï¼Œå®ƒæ˜¯å‡º`Script 1.31 (PDF-example.py)`åˆ›å»ºçš„ã€‚

```python
import scipy.stats as stats
import numpy as np
import matplotlib.pyplot as plt

# support of normal density: 
x_range = np.linspace(-4, 4, num=100)
# PDF for all these values:
pdf = stats.norm.pdf(x_range)
plt.plot(x_range, pdf, linestyle="-", color="black")
plt.xlabel("x")
plt.ylabel("dx")
plt.show()
```

#### ï¼ˆ3ï¼‰ç´¯ç§¯åˆ†å¸ƒå‡½æ•°(CDE)

å¯¹äºæ‰€æœ‰åˆ†å¸ƒï¼Œ`CDF F(x) = P(X â‰¤ x)`è¡¨ç¤ºéšæœºå˜é‡xå–å€¼ä¸è¶…è¿‡xçš„æ¦‚ç‡ã€‚xä»‹äºä¸¤ä¸ªå€¼aå’Œbä¹‹é—´çš„æ¦‚ç‡ä¸º`P(a < X â‰¤ b) = F(b) âˆ’ F(a)`ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨è¡¨1.6ç¬¬äºŒåˆ—ä¸­çš„`scipy`å‡½æ•°æ¥æ‰§è¡Œè„šæœ¬`1.32(CDF-example.py)`ä¸­æ¼”ç¤ºçš„è¿™äº›è®¡ç®—ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°3ä¸ªæˆ–æ›´å°‘çš„ç™½çƒçš„æ¦‚ç‡æ˜¯`F(3)`ï¼Œä½¿ç”¨é€‚å½“çš„äºŒé¡¹åˆ†å¸ƒçš„`CDF`ã€‚åˆè®¡ä¸º`87.9%`ã€‚æ ‡å‡†æ­£æ€éšæœºå˜é‡å–å€¼åœ¨`-1.96~1.96`ä¹‹é—´çš„æ¦‚ç‡ä¸º`25%`ã€‚

```python
import scipy.stats as stats
# binomial CDF: 
p1 = stats.binom.cdf(3, 10, 0.2)
print(fâ€™p1: {p1}\nâ€™) 
# normal CDF:
p2 = stats.norm.cdf(1.96) - stats.norm.cdf(-1.96) 
print(fâ€™p2: {p2}\nâ€™)
# p1: 0.8791261184000001 
# p2: 0.950004209703559
```

##### ä¾‹1ï¼šæ­£æ€éšæœºå˜é‡çš„æ¦‚ç‡

å‡è®¾$X\sim N(4,9)$ï¼Œç¬¬ä¸€ä¸ªä¾‹å­è®¡ç®—$P(2<X\leq 6)$ï¼Œæ€è·¯ä¸€æ˜¯é‡å†™è¿™ä¸ªé—®é¢˜ï¼Œä½¿å…¶æŒ‰ç…§Wooldridge(2019)æ‰€ç¤ºçš„æ ‡å‡†å¸¸æ€åˆ†é…æ¥è¡¨è¿°ï¼š$P(2<X<6)=Ğ¤(\frac{2}{3})-Ğ¤(-\frac{2}{3})$ï¼Œæ€è·¯äºŒå¯ä»¥çœå»å˜æ¢ç›´æ¥å¤„ç†éæ ‡å‡†æ­£æ€åˆ†å¸ƒã€‚è¦æ³¨æ„çš„æ˜¯ï¼Œå…³äºæ­£æ€åˆ†å¸ƒçš„scipyå‘½ä»¤ä¸­çš„ç¬¬ä¸‰ä¸ªå‚æ•°ä¸æ˜¯æ–¹å·®$\sigma^2=9$ï¼Œè€Œæ˜¯æ ‡å‡†å·®$\sigma=3$ã€‚

ç¬¬äºŒä¸ªä¾‹å­è®¡ç®—$P(|X| > 2) = \underbrace{1 âˆ’ P(X â‰¤ 2)}_{P(X>2)} + P(X < âˆ’2)$ï¼Œæ³¨æ„æˆ‘ä»¬æ²¡æœ‰è¿›è¡Œå››èˆäº”å…¥ï¼Œå› æ­¤å¾—åˆ°äº†ä¸€ä¸ªæ¯”ã€ŠWooldridgeã€‹(2019)æ›´ç²¾ç¡®çš„ç»“æœã€‚

```python
import scipy.stats as stats
# first example using the transformation: 
p1_1 = stats.norm.cdf(2 / 3) - stats.norm.cdf(-2 / 3) 
print(fâ€™p1_1: {p1_1}\nâ€™)

# first example working directly with the distribution of X: 
p1_2 = stats.norm.cdf(6, 4, 3) - stats.norm.cdf(2, 4, 3) 
print(fâ€™p1_2: {p1_2}\nâ€™)
# second example: 
p2 = 1 - stats.norm.cdf(2, 4, 3) + stats.norm.cdf(-2, 4, 3) 
print(fâ€™p2: {p2}\nâ€™)
```

`CDF`å›¾æ˜¯ç¦»æ•£åˆ†å¸ƒçš„é˜¶è·ƒå‡½æ•°ã€‚ä»¥urnä¸ºä¾‹ï¼ŒCDFå¦‚å›¾1.16(a)æ‰€ç¤ºã€‚è¿ç»­åˆ†å¸ƒçš„CDFç”¨æ­£æ€åˆ†å¸ƒçš„så½¢CDFè¡¨ç¤ºï¼Œå¦‚å›¾1.16(b)æ‰€ç¤ºã€‚

```python
import scipy.stats as stats 
import numpy as np 
import matplotlib.pyplot as plt 

# binomial: 
# support of binomial PMF:
x_binom = np.linspace(-1, 10, num=1000)
# PMF for all these values: 
cdf_binom = stats.binom.cdf(x_binom, 10, 0.2) 
# plot: 
plt.step(x_binom, cdf_binom, linestyle=â€™-â€™, color=â€™blackâ€™) 
plt.xlabel(â€™xâ€™) 
plt.ylabel(â€™Fxâ€™) 
plt.savefig(â€™PyGraphs/CDF-figure-discrete.pdfâ€™) 
plt.close() 
# normal: 
# support of normal density:
x_norm = np.linspace(-4, 4, num=1000) 
# PDF for all these values:
cdf_norm = stats.norm.cdf(x_norm) 
# plot: 
plt.plot(x_norm, cdf_norm, linestyle=â€™-â€™, color=â€™blackâ€™)
plt.xlabel(â€™xâ€™) 
plt.ylabel(â€™Fxâ€™)
plt.savefig(â€™PyGraphs/CDF-figure-cont.pdfâ€™)
```

![image-20240403144044274](imgs/image-20240403144044274.png)

#### ï¼ˆ4ï¼‰åˆ†ä½æ•°å‡½æ•°

ä¸€ä¸ªéšæœºå˜é‡çš„`q`-åˆ†ä½æ•°`x[q]`æ˜¯æŠ½æ ·ä¸€ä¸ªå€¼$x\leq x[q]$çš„æ¦‚ç‡ï¼Œåªæ˜¯qçš„å€¼ã€‚è¿™äº›å€¼æ˜¯é‡è¦çš„ï¼Œä¾‹å¦‚ï¼Œç”¨äºè®¡ç®—æµ‹`test`ç»Ÿè®¡çš„ä¸´ç•Œå€¼ã€‚ä¸¾ä¸ªç®€å•çš„ä¾‹å­ï¼šå‡è®¾Xæ˜¯æ ‡å‡†æ­£æ€ï¼Œé‚£ä¹ˆ`0.975-åˆ†ä½æ•°`å°±æ˜¯`X[0.9751]â‰ˆ1.96`ã€‚å› æ­¤é‡‡æ ·å€¼å°äºæˆ–ç­‰äº`1.96`çš„æ¦‚ç‡ä¸º`97.5%`ï¼š

```python
import scipy.stats as stats 
q_975 = stats.norm.ppf(0.975) 
print(fâ€™q_975: {q_975}\nâ€™)
```

#### ï¼ˆ5ï¼‰ä»æ¦‚ç‡åˆ†å¸ƒä¸­ç”Ÿæˆéšæœºæ•°

ä»å…·æœ‰ç»™å®šåˆ†å¸ƒçš„éšæœºå˜é‡ä¸­å–ä¸€ä¸ªæ ·æœ¬æ¥æ¨¡æ‹Ÿéšæœºç»“æœæ˜¯å¾ˆå®¹æ˜“çš„ã€‚ä¸¥æ ¼åœ°è¯´ï¼Œåƒè®¡ç®—æœºè¿™æ ·çš„ç¡®å®šæ€§æœºå™¨æ°¸è¿œä¸èƒ½äº§ç”Ÿä»»ä½•çœŸæ­£éšæœºçš„ç»“æœï¼Œæˆ‘ä»¬åº”è¯¥å°†ç”Ÿæˆçš„æ•°å­—ç§°ä¸ºä¼ªéšæœºæ•°ã€‚ä½†å°±æˆ‘ä»¬çš„ç›®çš„è€Œè¨€ï¼Œåªè¦ç”Ÿæˆçš„æ ·æœ¬çœ‹èµ·æ¥ã€æ„Ÿè§‰ä¸Šå’Œè¡Œä¸ºä¸Šéƒ½åƒçœŸæ­£çš„éšæœºæ•°å°±è¶³å¤Ÿäº†ï¼Œå› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œçš„æœ¯è¯­ä¸Šæœ‰ç‚¹è‰ç‡ã€‚å…³äºæŠ½æ ·å’Œç›¸å…³æ¦‚å¿µçš„å›é¡¾ï¼Œè¯·å‚è§Wooldridge(2019ï¼Œé™„å½•C.1)ã€‚

åœ¨æˆ‘ä»¬åœ¨1.9èŠ‚å¤§é‡ä½¿ç”¨ç”Ÿæˆéšæœºæ ·æœ¬ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä»‹ç»ä¸€ä¸‹æœºåˆ¶ã€‚scipyä¸­ç”Ÿæˆ(ä¼ª)éšæœºæ ·æœ¬çš„å‘½ä»¤æ˜¯é€šè¿‡ç»“åˆå„è‡ªåˆ†å¸ƒçš„å‘½ä»¤(è§è¡¨1.6)å’Œå‡½æ•°årvsæ¥æ„é€ çš„ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ¨¡æ‹ŸæŠ›10æ¬¡å‡åŒ€ç¡¬å¸çš„ç»“æœã€‚æˆ‘ä»¬ä»å‚æ•°$p = \frac{1}{2}$çš„ä¼¯åŠªåˆ©åˆ†å¸ƒä¸­æå–äº†ä¸€ä¸ªå¤§å°ä¸º$n=10$çš„æ ·æœ¬ã€‚ç”Ÿæˆçš„10ä¸ªæ•°å­—ä¸­çš„æ¯ä¸€ä¸ªéƒ½å°†å–æ¦‚ç‡ä¸º$p = \frac{1}{2}$çš„å€¼1å’Œæ¦‚ç‡ä¸º$$p = 1-\frac{1}{2}$$çš„å€¼0ã€‚ç»“æœçš„è¡¨ç°æ–¹å¼å°±åƒæˆ‘ä»¬å®é™…ä¸ŠæŠ›äº†ä¸€æšç¡¬å¸ï¼Œå¹¶å°†æ­£é¢è½¬æ¢ä¸º1ï¼Œåé¢è½¬æ¢ä¸º0(æˆ–åä¹‹äº¦ç„¶)ã€‚ä¸‹é¢æ˜¯ç”±å®ƒç”Ÿæˆçš„ä»£ç å’Œç¤ºä¾‹ï¼š

```python
import scipy.stats as stats 
sample = stats.bernoulli.rvs(0.5, size=10) 
print(fâ€™sample: {sample}\nâ€™)
```

ç¿»è¯‘æˆç¡¬å¸ï¼Œæˆ‘ä»¬çš„æ ·æœ¬æ˜¯æ­£é¢-åé¢-åé¢-æ­£é¢-åé¢-æ­£é¢-æ­£é¢-æ­£é¢-åé¢-åé¢-æ­£é¢ã€‚ç”¨Pyhonè€Œä¸æ˜¯ç”¨çœŸæ­£çš„ç¡¬å¸æ¥åšè¿™ä»¶äº‹çš„ä¸€ä¸ªæ˜æ˜¾ä¼˜åŠ¿æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥æ— ç—›åœ°å°†æ ·æœ¬é‡å¢åŠ åˆ°1000æˆ–1000ä¸‡ã€‚ä»æ ‡å‡†å¸¸æ€åˆ†é…ä¸­æå–ç»“æœåŒæ ·ç®€å•ã€‚

```python
import scipy.stats as stats
sample = stats.norm.rvs(size=10)
print(fâ€™sample: {sample}\nâ€™)
```

ä½¿ç”¨è®¡ç®—æœºç”Ÿæˆçš„éšæœºæ ·æœ¬ä¼šç»™ç»“æœçš„å¯é‡å¤æ€§å¸¦æ¥é—®é¢˜ã€‚å› ä¸ºåœ¨æ¯æ¬¡è¿è¡Œä¸Šé¢çš„ä»£ç æ—¶éƒ½ä¼šå¾—åˆ°ä¸åŒçš„æ ·æœ¬ã€‚å¯ä»¥é€šè¿‡ä½¿ç”¨éšæœºæ•°çš„å®é™…ç”Ÿæˆæ–¹å¼æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ­£å¦‚å‰é¢æåˆ°çš„ï¼Œè¿™å¹¶ä¸æ¶‰åŠçœŸæ­£çš„éšæœºæ€§ã€‚å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬å°†éšæœºæ•°ç”Ÿæˆå™¨é‡ç½®ä¸ºæŸäº›ç‰¹å®šçš„çŠ¶æ€(â€œseedâ€)ï¼Œæˆ‘ä»¬å°†æ€»æ˜¯å¾—åˆ°ç›¸åŒçš„æ•°å­—åºåˆ—ã€‚åœ¨pyhonä¸­ï¼Œè¿™å¯ä»¥é€šè¿‡numpyçš„å‡½æ•°random.seed(number)æ¥å®ç°ï¼Œå…¶ä¸­numberæ˜¯æŸä¸ªä»»æ„çš„æ•´æ•°ï¼Œå®ƒå®šä¹‰äº†çŠ¶æ€ï¼Œä½†æ²¡æœ‰å…¶ä»–å«ä¹‰ã€‚å¦‚æœæˆ‘ä»¬å°†seedè®¾ç½®ä¸ºæŸä¸ªä»»æ„æ•´æ•°ï¼Œå–ä¸€ä¸ªæ ·æœ¬ï¼Œå°†ç§å­é‡ç½®ä¸ºç›¸åŒçš„çŠ¶æ€å¹¶å–å¦ä¸€ä¸ªæ ·æœ¬ï¼Œä¸¤ä¸ªæ ·æœ¬å°†æ˜¯ç›¸åŒçš„ã€‚åŒæ ·çš„ï¼Œå¦‚æœæˆ‘ç”¨é‚£ä¸ªç§å­ç”»ä¸€ä¸ªæ ·æœ¬å®ƒå°†ç­‰äºä½ ç”»çš„æ ·æœ¬å¦‚æœæˆ‘ä»¬éƒ½ä»åŒä¸€ä¸ªç§å­å¼€å§‹ã€‚

è„šæœ¬1.38 (Random-Numbers.py)æ¼”ç¤ºäº†random.seedçš„å·¥ä½œåŸç†ã€‚

```python
import numpy as np
import scipy.stats as stats 

# sample from a standard normal RV with sample size n=5: 

sample1 = stats.norm.rvs(size=5)
print(fâ€™sample1: {sample1}\nâ€™) 

# a different sample from the same distribution: 
sample2 = stats.norm.rvs(size=5)
print(fâ€™sample2: {sample2}\nâ€™) 

# set the seed of the random number generator and take two samples: 
np.random.seed(6254137) 
sample3 = stats.norm.rvs(size=5) 
print(fâ€™sample3: {sample3}\nâ€™)
sample4 = stats.norm.rvs(size=5) 
print(fâ€™sample4: {sample4}\nâ€™) 

# reset the seed to the same value to get the same samples again: 
np.random.seed(6254137)
sample5 = stats.norm.rvs(size=5) 
print(fâ€™sample5: {sample5}\nâ€™) 
sample6 = stats.norm.rvs(size=5) 
print(fâ€™sample6: {sample6}\nâ€™)
```

### 3.ç½®ä¿¡åŒºé—´ä¸ç»Ÿè®¡æ¨æ–­

Wooldridge(2019)å¯¹åŸºæœ¬æŠ½æ ·ã€ä¼°è®¡å’Œæµ‹è¯•è¿›è¡Œäº†ç®€è¦æ¦‚è¿°ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹æ–‡è§¦åŠå…¶ä¸­ä¸€äº›é—®é¢˜ã€‚20

#### ï¼ˆ1ï¼‰ç½®ä¿¡åŒºé—´

Wooldridge(2019ï¼Œé™„å½•C.5)å¼•å…¥äº†ç½®ä¿¡åŒºé—´(CI)ã€‚å®ƒä»¬çš„æ„é€ æ˜¯ä¸ºäº†ä»¥ç»™å®šçš„é«˜æ¦‚ç‡è¦†ç›–æ„Ÿå…´è¶£çš„çœŸå®æ€»ä½“å‚æ•°ï¼Œä¾‹å¦‚95%ã€‚æ›´æ˜ç¡®åœ°è¯´ï¼šå¯¹äºæ‰€æœ‰æ ·æœ¬çš„95%ï¼Œéšå«çš„CIåŒ…æ‹¬æ€»ä½“å‚æ•°ã€‚CIå¾ˆå®¹æ˜“è®¡ç®—ã€‚å¯¹äºå‡å€¼å’Œæ–¹å·®Ïƒ2æœªçŸ¥çš„æ­£æ€æ€»ä½“ï¼Œuçš„100(1-a)%ç½®ä¿¡åŒºé—´ç”±Wooldridge(2019ï¼Œæ–¹ç¨‹C.24å’ŒC.25)ç»™å‡ºï¼š
$$
\begin{bmatrix}\bar{y}-c_{\frac{\alpha}{2}}\cdot se(\bar{y}),\bar{y}+c_{\frac{\alpha}{2}}\cdot se(\bar{y})\end{bmatrix}
$$
å…¶ä¸­ï¼Œ$\bar{y}$æ˜¯æ ·æœ¬å‡å€¼ï¼Œ$se(\bar{y})=\frac{s}{\sqrt{n}}$æ˜¯$\bar{y}$çš„æ ‡å‡†è¯¯å·®ï¼ˆå…¶ä¸­sæ˜¯yçš„æ ·æœ¬â€œæ ‡å‡†å·®â€ï¼‰ï¼Œnæ˜¯æ ·æœ¬é‡ï¼Œ$c_{\frac{\alpha}{2}}$æœä»æ˜¯$t_{n-1}$åˆ†å¸ƒçš„$(1-\frac{\alpha}{2})$åˆ†ä½æ•°ï¼Œä¸ºäº†è®¡ç®—å‡º$95\% CI(\alpha=5\%)$ï¼Œæˆ‘ä»¬éœ€è®¡ç®—å‡º0.975çš„åˆ†ä½æ•°$c_{0.025}$ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“å¦‚ä½•è®¡ç®—æ‰€æœ‰è¿™äº›æˆåˆ†ã€‚è®¡ç®—CIçš„æ–¹æ³•åœ¨ä¾‹å­cçš„è§£å†³æ–¹æ¡ˆä¸­ä½¿ç”¨ã€‚åœ¨ç¬¬1.9.3èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ä¸€ä¸ªæ¨¡æ‹Ÿå®éªŒä¸­è®¡ç®—ç½®ä¿¡åŒºé—´ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬ç†è§£ç½®ä¿¡åŒºé—´çš„å«ä¹‰ã€‚

```python
[20] The stripped-down textbook for Europe and Africa Wooldridge (2014) does not include the discussion of this material.
```

##### ä¾‹1ï¼šèŒä¸šåŸ¹è®­è¡¥åŠ©å¯¹å·¥äººç”Ÿäº§ç‡çš„å½±å“

æˆ‘ä»¬æ­£åœ¨åˆ†æ1988å¹´è·å¾—èŒä¸šåŸ¹è®­è¡¥åŠ©çš„å…¬å¸çš„åºŸå“ç‡ã€‚1987å¹´å’Œ1988å¹´çš„æŠ¥åºŸç‡ä»¥Wooldridgeæ ¼å¼æ‰“å°(2019å¹´ï¼Œè¡¨C.3)ï¼Œå¹¶åœ¨è„šæœ¬1.39(Example-C-2.py)çš„å¼€å¤´æ‰‹åŠ¨è¾“å…¥ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯å¹´ä»½ä¹‹é—´çš„å˜åŒ–ã€‚å…¶å¹³å‡å€¼å’Œç½®ä¿¡åŒºé—´çš„è®¡ç®—æ˜¯ç²¾ç¡®åœ°æ‰§è¡Œå¦‚ä¸Šæ‰€ç¤ºã€‚æ‰€å¾—åˆ°çš„CIä¸Wooldridge(2019)ä¸­æå‡ºçš„CIç›¸åŒï¼Œé™¤äº†æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ç¡®åˆ‡çš„æ•°å­—æ¥é¿å…èˆå…¥è¯¯å·®ã€‚

```python
import numpy as np 
import scipy.stats as stats 

# manually enter raw data from Wooldridge, Table C.3: 
SR87 = np.array([10, 1, 6, .45, 1.25, 1.3, 1.06, 3, 8.18, 1.67, .98, 1, .45, 5.03, 8, 9, 18, .28, 7, 3.97]) 
SR88 = np.array([3, 1, 5, .5, 1.54, 1.5, .8, 2, .67, 1.17, .51, .5, .61, 6.7, 4, 7, 19, .2, 5, 3.83]) 

# calculate change: 
Change = SR88 - SR87 

# ingredients to CI formula: 
avgCh = np.mean(Change) 
print(fâ€™avgCh: {avgCh}\nâ€™) 
n = len(Change) 
sdCh = np.std(Change, ddof=1) 
se = sdCh / np.sqrt(n)
print(fâ€™se: {se}\nâ€™) 
c = stats.t.ppf(0.975, n - 1) 
print(fâ€™c: {c}\nâ€™) 

# confidence interval: 
lowerCI = avgCh - c * se 
print(fâ€™lowerCI: {lowerCI}\nâ€™) 
upperCI = avgCh + c * se 
print(fâ€™upperCI: {upperCI}\nâ€™)
```

##### ä¾‹2ï¼šæ‹›è˜ä¸­çš„ç§æ—æ­§è§†

æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ•°æ®é›†å®¡è®¡æ¥è°ƒæŸ»ç§æ—æ­§è§†é—®é¢˜ã€‚å˜é‡$y$ä»£è¡¨äº†ç®€å†ç›¸åŒçš„é»’äººå’Œç™½äººä¸­è¯·äººåœ¨æ‹›è˜ç‡ä¸Šçš„å·®å¼‚ã€‚åœ¨è®¡ç®—äº†å¹³å‡å€¼ã€æ ·æœ¬é‡ã€æ ‡å‡†å’Œæ ·æœ¬å¹³å‡å€¼çš„æ ‡å‡†è¯¯å·®ä¹‹åï¼Œscript1.40 (Example-c-3.py)å°†å› å­cçš„å€¼è®¡ç®—ä¸ºæ ‡å‡†å¸¸æ€åˆ†é…çš„97.5ç™¾åˆ†ä½æ•°ï¼Œå³(éå¸¸æ¥è¿‘)1.96ã€‚æœ€åï¼ŒæŠ¥å‘Šäº†95%å’Œ99%çš„CI

```python
import wooldridge as woo
import numpy as np 
import scipy.stats as stats

audit = woo.dataWoo(â€™auditâ€™)
y = audit[â€™yâ€™] 
# ingredients to CI formula:
avgy = np.mean(y) 
n = len(y)
sdy = np.std(y, ddof=1)
se = sdy / np.sqrt(n)
c95 = stats.norm.ppf(0.975)
c99 = stats.norm.ppf(0.995) 

# 95% confidence interval: 
lowerCI95 = avgy - c95 * se 
print(fâ€™lowerCI95: {lowerCI95}\nâ€™)
upperCI95 = avgy + c95 * se 

print(fâ€™upperCI95: {upperCI95}\nâ€™) 

# 99% confidence interval:
lowerCI99 = avgy - c99 * se 
print(fâ€™lowerCI99: {lowerCI99}\nâ€™) 
upperCI99 = avgy + c99 * se 
print(fâ€™upperCI99: {upperCI99}\nâ€™)
```

```python
[21] Note that Wooldridge (2019) has a typo in the discussion of this example, therefore the numbers donâ€™t quite match for the 95% CI.
```

#### ï¼ˆ2ï¼‰tæµ‹è¯•

Wooldridge(2019å¹´ï¼Œé™„å½•C.6)ä¸­æ¶µç›–äº†å‡è®¾æ£€éªŒã€‚å…³äºæ­£æ€åˆ†å¸ƒéšæœºå˜é‡$\Upsilon$çš„å‡å€¼$\mu$çš„å‡è®¾æ£€éªŒçš„$t$ç»Ÿè®¡é‡å¦‚å¼C.35æ‰€ç¤ºã€‚ç»™å®šé›¶å‡è®¾$H_0:\mu=\mu_0$ï¼Œ
$$
t=\frac{\bar{y}-\mu_0}{se(\bar{y})}
$$
æˆ‘ä»¬å·²ç»çŸ¥é“äº†å¦‚ä½•è®¡ç®—ç¬¬1.7.1èŠ‚ä¸­çš„æˆåˆ†ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åœ¨è„šæœ¬1.42(Example-C-5.py)ä¸­ä½¿ç”¨å®ƒä»¬æ¥æ‰§è¡Œ$t$â€‹æ£€éªŒã€‚æˆ‘ä»¬è¿˜å°†è¯¥ç»“æœä¸scipyå‡½æ•°`ttest_1samp`çš„è¾“å‡ºè¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯¥å‡½æ•°æ‰§è¡Œäº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„$t-test$â€‹ã€‚

è¿™ä¸ªæµ‹è¯•ç»Ÿè®¡é‡çš„ä¸´ç•Œå€¼å–å†³äºæµ‹è¯•æ˜¯å•è¾¹çš„è¿˜æ˜¯åŒè¾¹çš„ã€‚åŒé¢æµ‹è¯•æ‰€éœ€çš„å€¼$c_{\frac{\alpha}{2}}$å·²ç»ä¸º$CI$è®¡ç®—è¿‡äº†ï¼Œå…¶ä»–çš„å€¼å¯ä»¥æ˜¯ç›¸åº”çš„ç”Ÿæˆã€‚ä¸åŒè‡ªç”±åº¦$n-1$å’Œæ˜¾è‘—æ€§æ°´å¹³açš„å€¼åˆ—äºWooldridge(2019,Table G2)ã€‚è„šæœ¬1.41(critical-values -t.py)æ¼”ç¤ºäº†æˆ‘ä»¬å¦‚ä½•ä¸º19è‡ªç”±åº¦çš„ä¾‹å­è®¡ç®—æˆ‘ä»¬è‡ªå·±çš„ä¸´ç•Œå€¼è¡¨ã€‚

```python
mport numpy as np 
import pandas as pd 
import scipy.stats as stats

# degrees of freedom = n-1:
df = 19 
# significance levels: 
alpha_one_tailed = np.array([0.1, 0.05, 0.025, 0.01, 0.005, .001]) 
alpha_two_tailed = alpha_one_tailed * 2
# critical values & table:
CV = stats.t.ppf(1 - alpha_one_tailed, df) 
table = pd.DataFrame({â€™alpha_one_tailedâ€™: alpha_one_tailed, â€™alpha_two_tailedâ€™: alpha_two_tailed, â€™CVâ€™: CV})
print(fâ€™table: \n{table}\nâ€™)
```

##### ä¾‹1ï¼šæ‹›è˜ä¸­çš„ç§æ—æ­§è§†

æˆ‘ä»¬ç»§ç»­è„šæœ¬1.42ä¸­çš„ç¤ºä¾‹C.3(Example-C-5.py)ï¼Œå¹¶å¯¹ç›¸åŒæ ·æœ¬çš„é›¶å‡è®¾$H_0:\mu=0$å¯¹$H_1:\mu<0$æ‰§è¡Œå•ä¾§$t$æ£€éªŒã€‚å¦‚è¾“å‡ºæ‰€ç¤ºï¼Œ$t$æ£€éªŒç»Ÿè®¡é‡ç­‰äº-4.27ã€‚è¿™æ¯”ä»»ä½•åˆç†æ˜¾è‘—æ€§æ°´å¹³çš„ä¸´ç•Œå€¼çš„è´Ÿå€¼éƒ½è¦å°å¾—å¤šã€‚å› æ­¤ï¼Œå¯¹äºè¿™ä¸ªå•ä¾§æ£€éªŒæˆ‘ä»¬æ‹’ç»$H_0:\mu=0$ï¼Œè§Wooldridge(2019ï¼Œæ–¹ç¨‹C.38)ã€‚

```python
import wooldridge as woo
import numpy as np
import pandas as pd 
import scipy.stats as stats

audit = woo.dataWoo(â€™auditâ€™) 
y = audit[â€™yâ€™] 
# automated calculation of t statistic for H0 (mu=0):
test_auto = stats.ttest_1samp(y, popmean=0)
t_auto = test_auto.statistic  # access test statistic 
p_auto = test_auto.pvalue   # access two-sided p value
print(fâ€™t_auto: {t_auto}\nâ€™) 
print(fâ€™p_auto/2: {p_auto / 2}\nâ€™) 
# manual calculation of t statistic for H0 (mu=0): 
avgy = np.mean(y)
n = len(y) 
sdy = np.std(y, ddof=1) 
se = sdy / np.sqrt(n) 
t_manual = avgy / se 
print(fâ€™t_manual: {t_manual}\nâ€™) 

# critical values for t distribution with n-1=240 d.f.: 
alpha_one_tailed = np.array([0.1, 0.05, 0.025, 0.01, 0.005, .001])
CV = stats.t.ppf(1 - alpha_one_tailed, 240)
table = pd.DataFrame({â€™alpha_one_tailedâ€™: alpha_one_tailed, â€™CVâ€™: CV}) 
print(fâ€™table: \n{table}\nâ€™)
```

#### ï¼ˆ3ï¼‰på€¼

æµ‹è¯•çš„på€¼æ˜¯(åœ¨å¯¼å‡ºæµ‹è¯•ç»Ÿè®¡é‡åˆ†å¸ƒæ‰€éœ€çš„å‡è®¾ä¸‹)ä¸€ä¸ªä¸åŒçš„éšæœºæ ·æœ¬äº§ç”Ÿç›¸åŒæˆ–æ›´æç«¯çš„æµ‹è¯•ç»Ÿè®¡é‡çš„æ¦‚ç‡22ï¼Œä½¿ç”¨på€¼è¿›è¡Œç»Ÿè®¡æ£€éªŒçš„ä¼˜ç‚¹æ˜¯ä½¿ç”¨æ–¹ä¾¿ã€‚æˆ‘ä»¬ä¸å¿…å°†æ£€éªŒç»Ÿè®¡é‡ä¸æ˜¾è‘—æ€§æ°´å¹³aæ‰€æš—ç¤ºçš„ä¸´ç•Œå€¼è¿›è¡Œæ¯”è¾ƒï¼Œè€Œæ˜¯ç›´æ¥å°†på€¼ä¸aå€¼è¿›è¡Œæ¯”è¾ƒã€‚å¯¹äºåŒä¾§tæ£€éªŒï¼Œpå€¼çš„å…¬å¼ç”±Wooldridge (2019,Equation C.42)ç»™å‡ºï¼š
$$
p=2\cdot P(T_{n-1}>|t|)=2\cdot (1-F_{t_{n-1}}(|t|))
$$
å…¶ä¸­$F_{t_{n-1}}(\cdot)$ä¸º$t_{n-1}$åˆ†å¸ƒçš„CDFï¼Œæˆ‘ä»¬çŸ¥é“å¦‚ä½•ä»è¡¨1.6ä¸­è®¡ç®—ã€‚

ç±»ä¼¼åœ°ï¼Œåªæœ‰åœ¨ä¼°ä»·å€¼ç›¸å¯¹äºé›¶å‡è®¾â€œè¿‡é«˜â€æˆ–â€œè¿‡ä½â€çš„æƒ…å†µä¸‹ï¼Œå•æ–¹é¢æ£€éªŒæ‰ä¼šæ‹’ç»é›¶å‡è®¾ã€‚è¿™å‡ ç±»æ£€éªŒçš„på€¼ä¸º
$$
p=\begin{cases} 
P(T_{n-1}<t)=F_{t_{n-1}}(t),H_1:\mu<\mu_0\\ 
P(T_{n-1}>t)=1-F_{t_{n-1}}(t), H_1:\mu>\mu_0 
\end{cases} 
$$
ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†ä¸€ä¸ªçŸ¥é“åˆ†å¸ƒçš„CDFçš„è®¡ç®—æœºç¨‹åºï¼Œå› æ­¤è®¡ç®—på€¼å¾ˆç®€å•ï¼Œå¦‚è„šæœ¬1.43(Example-C-6.py)æ‰€ç¤ºã€‚ä¹Ÿè®¸æ‚¨æ³¨æ„åˆ°Script 1.42(Example-C-5.py)ä¸­çš„scipyå‡½æ•°`ttest_1samp`ä¹Ÿå¯ä»¥è®¡ç®—på€¼ï¼Œä½†è¯·æ³¨æ„ï¼Œè¯¥å‡½æ•°å§‹ç»ˆåŸºäºåŒä¾§$t$æ£€éªŒã€‚

```python
[22] The p value is often misinterpreted. It is for example not the probability that the null hypothesis is true. For a discussion, see for example https://www.nature.com/news/scientific-method-statistical-errors-1.14700.
```

##### ä¾‹1ï¼šèŒä¸šåŸ¹è®­è¡¥åŠ©é‡‘å¯¹å·¥äººç”Ÿäº§ç‡çš„å½±å“

æˆ‘ä»¬ç»§ç»­ä»è„šæœ¬1.43 (Example-c-6py)ä¸­çš„ç¤ºä¾‹c.2å¼€å§‹ã€‚æˆ‘ä»¬å¯¹$H_1:\mu<0$è¿›è¡Œ$H_0:\mu=0$çš„æ£€éªŒã€‚$t$ç»Ÿè®¡é‡ä¸º$-2.15$ï¼Œè¯¥å•ä¾§æ£€éªŒçš„på€¼å…¬å¼è§Toaldidee (2019,Eguaion c.41)ã€‚ä»scnipt 1.43 Example-c-6,py)çš„è¾“å‡ºä¸­å¯ä»¥çœ‹åˆ°ï¼Œå®ƒçš„å€¼(ä½¿ç”¨tçš„ç³Ÿç¡®å€¼)å¤§çº¦æ˜¯0.022ã€‚å¦‚æœä½ æƒ³ä½¿ç”¨scipyå‡½æ•°`ttest_1samp`ï¼Œä½ å¿…é¡»å°†på€¼é™¤ä»¥2ï¼Œå› ä¸ºæˆ‘ä»¬å¤„ç†çš„æ˜¯ä¸€ä¸ªå•è¾¹æµ‹è¯•ã€‚

```python
import numpy as np
import scipy.stats as stats 

# manually enter raw data from Wooldridge, Table C.3: 
SR87 = np.array([10, 1, 6, .45, 1.25, 1.3, 1.06, 3, 8.18, 1.67, .98, 1, .45, 5.03, 8, 9, 18, .28, 7, 3.97])
SR88 = np.array([3, 1, 5, .5, 1.54, 1.5, .8, 2, .67, 1.17, .51, .5, .61, 6.7, 4, 7, 19, .2, 5, 3.83])
Change = SR88 - SR87 

# automated calculation of t statistic for H0 (mu=0):
test_auto = stats.ttest_1samp(Change, popmean=0) 
t_auto = test_auto.statistic
p_auto = test_auto.pvalue 
print(fâ€™t_auto: {t_auto}\nâ€™) 
print(fâ€™p_auto/2: {p_auto / 2}\nâ€™)

# manual calculation of t statistic for H0 (mu=0):
avgCh = np.mean(Change) 
n = len(Change) 
sdCh = np.std(Change, ddof=1)
se = sdCh / np.sqrt(n) 
t_manual = avgCh / se 
print(fâ€™t_manual: {t_manual}\nâ€™) 

# manual calculation of p value for H0 (mu=0):
p_manual = stats.t.cdf(t_manual, n - 1)
print(fâ€™p_manual: {p_manual}\nâ€™)
```

##### ä¾‹2ï¼šæ‹›è˜ä¸­çš„ç§æ—æ­§è§†

åœ¨ç¤ºä¾‹C.5ä¸­ï¼Œæˆ‘ä»¬å‘ç°$H_0:\mu=0$å¯¹$H_1:\mu<0$çš„ç»Ÿè®¡é‡ä¸º$t=-4.276816$ã€‚å¯¹åº”çš„$p$å€¼åœ¨è„šæœ¬1.44(Example-C-7.py)ä¸­è®¡ç®—å¾—åˆ°ã€‚æ•°å­—$1.369271e-05$æ˜¯$1.369271Â·10^{-5}$=$.00001369271$çš„ç§‘å­¦è¡¨ç¤ºæ³•ã€‚å› æ­¤$p$å€¼åœ¨$0.0014%$å·¦å³ï¼Œæ¯”ä»»ä½•åˆç†çš„æ˜¾è‘—æ€§æ°´å¹³éƒ½è¦å°å¾—å¤šï¼Œé€šè¿‡æ„é€ ï¼Œæˆ‘ä»¬å¾—å‡ºäº†ä¸ç¤ºä¾‹C.5ä¸­ç»Ÿè®¡é‡ä¸ä¸´ç•Œå€¼æ¯”è¾ƒæ—¶ç›¸åŒçš„ç»“è®ºã€‚æˆ‘ä»¬æ‹’ç»é›¶å‡è®¾ï¼Œå³ä¸å­˜åœ¨å·®åˆ«ã€‚

```python
import wooldridge as woo
import numpy as np
import pandas as pd
import scipy.stats as stats

audit = woo.dataWoo(â€™auditâ€™)
y = audit[â€™yâ€™] 

# automated calculation of t statistic for H0 (mu=0):
test_auto = stats.ttest_1samp(y, popmean=0)
t_auto = test_auto.statistic 
p_auto = test_auto.pvalue
print(fâ€™t_auto: {t_auto}\nâ€™)
print(fâ€™p_auto/2: {p_auto/2}\nâ€™)

# manual calculation of t statistic for H0 (mu=0):
avgy = np.mean(y) 
n = len(y) 
sdy = np.std(y, ddof=1)
se = sdy / np.sqrt(n) 
t_manual = avgy / se
print(fâ€™t_manual: {t_manual}\nâ€™) 

# manual calculation of p value for H0 (mu=0): 
p_manual = stats.t.cdf(t_manual, n - 1) 
print(fâ€™p_manual: {p_manual}\nâ€™)
```

### 4.è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿ(Monte Carlo Simulation)

Wooldridge(2019)çš„é™„å½•C.2åŒ…å«äº†å¯¹ä¼°è®¡å™¨åŠå…¶æ€§è´¨çš„ç®€è¦ä»‹ç»ã€‚23åœ¨ç°å®ä¸–ç•Œçš„åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå¯¹åº”äºä¸€ä¸ªå®šä¹‰è‰¯å¥½çš„æ€»ä½“ä¸­çš„ä¸€ä¸ªéšæœºæ ·æœ¬ã€‚æˆ‘ä»¬ä¸çŸ¥é“æ€»ä½“å‚æ•°ï¼Œå¹¶ä½¿ç”¨æ ·æœ¬æ¥ä¼°è®¡å®ƒä»¬ã€‚
å½“æˆ‘ä»¬åƒåœ¨ç¬¬1.6.4èŠ‚ä¸­ä»‹ç»çš„é‚£æ ·ï¼Œä½¿ç”¨è®¡ç®—æœºç¨‹åºç”Ÿæˆæ ·æœ¬æ—¶ï¼Œæˆ‘ä»¬çŸ¥é“æ€»ä½“å‚æ•°ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿›è¡ŒéšæœºæŠ½å–æ—¶å¿…é¡»é€‰æ‹©å®ƒä»¬ã€‚æˆ‘ä»¬å¯ä»¥å¯¹è¿™ä¸ªäººå·¥æ ·æœ¬åº”ç”¨ç›¸åŒçš„ä¼°è®¡é‡æ¥ä¼°è®¡æ€»ä½“å‚æ•°ã€‚

ä»»åŠ¡å°†æ˜¯:

- é€‰æ‹©ä¸€ä¸ªæ€»ä½“åˆ†å¸ƒåŠå…¶å‚æ•°ã€‚
- ä»è¿™ä¸ªåˆ†å¸ƒä¸­ç”Ÿæˆä¸€ä¸ªæ ·æœ¬ã€‚
- åˆ©ç”¨æ ·æœ¬æ¥ä¼°è®¡æ€»ä½“å‚æ•°ã€‚

å¦‚æœä½ è§‰å¾—è¿™å¬èµ·æ¥æœ‰ç‚¹ç–¯ç‹‚ï¼šåˆ«æ‹…å¿ƒï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¥åº·çš„ç¬¬ä¸€ååº”ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ˜¯æˆ‘ä»¬ç²¾ç¡®çŸ¥é“çš„ä¸œè¥¿çš„å˜ˆæ‚ä¼°è®¡ã€‚ä½†è¿™ç§åˆ†æå®é™…ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ã€‚å› ä¸ºæˆ‘ä»¬ä¼°è®¡çš„æ˜¯æˆ‘ä»¬å®é™…çŸ¥é“çš„ä¸œè¥¿ï¼Œæ‰€ä»¥æˆ‘ä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°ç ”ç©¶æˆ‘ä»¬çš„ä¼°è®¡å™¨çš„è¡Œä¸ºã€‚åœ¨è¿™æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å‡ºäºè¯´æ˜æ€§å’Œè¯´æ•™æ€§çš„åŸå› ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚åœ¨æœ€å…ˆè¿›çš„ç ”ç©¶ä¸­ï¼Œå®ƒè¢«å¹¿æ³›ä½¿ç”¨ï¼Œå› ä¸ºå®ƒé€šå¸¸æä¾›äº†äº†è§£ä¼°è®¡å™¨å’Œç»Ÿè®¡æµ‹è¯•çš„é‡è¦ç‰¹å¾çš„å”¯ä¸€æ–¹æ³•ã€‚è¿™ç±»åˆ†æé€šå¸¸è¢«ç§°ä¸ºè’™ç‰¹å¡ç½—æ¨¡æ‹Ÿï¼ŒæŒ‡çš„æ˜¯ç”Ÿæˆéšæœºæ ·æœ¬çš„â€œèµŒåšâ€ã€‚

#### ï¼ˆ1ï¼‰ä¼°è®¡é‡çš„æœ‰é™æ ·æœ¬æ€§è´¨

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªæˆ‘ä»¬æƒ³è¦ä¼°è®¡æ­£æ€åˆ†å¸ƒéšæœºå˜é‡çš„å‡å€¼$\mu$çš„æƒ…å†µ
$$
\gamma \sim Normal(\mu,\sigma^2)
$$
ä½¿ç”¨ç»™å®šå¤§å°çš„æ ·æœ¬ã€‚æ€»ä½“å‡å€¼çš„æ˜æ˜¾ä¼°è®¡é‡æ˜¯æ ·æœ¬å¹³å‡å€¼,ä½†æ˜¯è¿™ä¸ªä¼°è®¡é‡æœ‰ä»€ä¹ˆç‰¹æ€§å‘¢?çŸ¥æƒ…çš„è¯»è€…ç«‹å³çŸ¥é“çš„æŠ½æ ·åˆ†å¸ƒæ˜¯
$$
\bar{\gamma} \sim Normal(\mu,\frac{\sigma^2}{2})
$$

```python
[23] The stripped-down textbook for Europe and Africa Wooldridge (2014) does not include this either.
```

è„šæœ¬1.51(simulation-estimate .py)å±•ç¤ºäº†ä¸€ä¸ªå®é™…çš„æ¨¡æ‹Ÿå®éªŒ:æˆ‘ä»¬è®¾ç½®ç§å­ä»¥ç¡®ä¿å†ç°æ€§ï¼Œå¹¶ä»ç§ç¾¤åˆ†å¸ƒä¸­æå–ä¸€ä¸ªå¤§å°ä¸ºn=100çš„æ ·æœ¬(ç§ç¾¤å‚æ•°$\mu=10$å’Œ$\sigma=2$)ã€‚24 ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—æ ·æœ¬å¹³å‡å€¼ä½œä¸ºuçš„ä¼°è®¡å€¼ã€‚æˆ‘ä»¬çœ‹åˆ°äº†ä¸‰ä¸ªä¸åŒæ ·æœ¬çš„ç»“æœã€‚

```python
import numpy as np import scipy.stats as stats # set the random seed: np.random.seed(123456) # set sample size: n = 100 # draw a sample given the population parameters: sample1 = stats.norm.rvs(10, 2, size=n) # estimate the population mean with the sample average: estimate1 = np.mean(sample1) print(fâ€™estimate1: {estimate1}\nâ€™) # draw a different sample and estimate again: sample2 = stats.norm.rvs(10, 2, size=n) estimate2 = np.mean(sample2) print(fâ€™estimate2: {estimate2}\nâ€™) # draw a third sample and estimate again: sample3 = stats.norm.rvs(10, 2, size=n) estimate3 = np.mean(sample3) print(fâ€™estimate3: {estimate3}\nâ€™)
```

æ‰€æœ‰æ ·æœ¬å‡å€¼$\bar{\gamma}$éƒ½åœ¨çœŸå®å‡å€¼$\mu=10$é™„è¿‘ï¼Œè¿™ä¸æˆ‘ä»¬åœ¨å¼1.7ä¸­åˆ¶å®šçš„å‡è®¾ä¸€è‡´ã€‚æˆ‘ä»¬æ— æ³•å¾—åˆ°å‡†ç¡®çš„æ€»ä½“å‚æ•°ä¹Ÿå°±ä¸è¶³ä¸ºå¥‡äº†--è¿™å°±æ˜¯é‡‡æ ·å™ªå£°çš„æ€§è´¨ã€‚æ ¹æ®å…¬å¼1.7ï¼Œç»“æœçš„æ–¹å·®é¢„è®¡ä¸º$\frac{\sigma^2}{2}=0.04$ï¼Œè¿™ç±»ä¸‰ä¸ªæ ·æœ¬ä¸è¶³ä»¥å¯¹å¼1.7çš„æ•ˆåº¦å¾—å‡ºå¼ºæœ‰åŠ›çš„ç»“è®ºã€‚å¥½çš„è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿç ”ç©¶åº”è¯¥ä½¿ç”¨å°½å¯èƒ½å¤šçš„æ ·æœ¬ã€‚

åœ¨1.8.2èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†`for`å¾ªç¯ã€‚è™½ç„¶å®ƒä»¬ä¸æ˜¯Pvhonä¸­å®ç°è’™ç‰¹å¡ç½—ç ”ç©¶çš„æœ€å¼ºå¤§çš„æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬å°†åšæŒä½¿ç”¨å®ƒä»¬ï¼Œå› ä¸ºå®ƒä»¬éå¸¸é€æ˜å’Œç›´æ¥ã€‚è„šæœ¬1.52(simulation-repeat.py)ä¸­æ˜¾ç¤ºçš„ä»£ç ä½¿ç”¨forå¾ªç¯ç»˜åˆ¶å¤§å°ä¸º$n=10000$çš„æ ·æœ¬ï¼Œå¹¶è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ã€‚è®¾ç½®å®Œéšæœºç§å­åï¼Œä½¿ç”¨`np.empty`åˆå§‹åŒ–å¤§å°ä¸º`10000`çš„ç©ºæ•°ç»„$ybar$ï¼Œæˆ‘ä»¬å°†åœ¨å¾ªç¯ä¸­ç”¨ä¼°è®¡å€¼ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°æ›¿æ¢è¿™äº›ç©ºæ•°ç»„å€¼ã€‚åœ¨æ¯ä¸€ä¸ªè¿™æ ·çš„å¤åˆ¶ä¸­j= 0,1,2,...,9999ï¼ŒæŠ½å–ä¸€ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å…¶å¹³å‡å€¼å¹¶å­˜å‚¨åœ¨ybarçš„iå·ä½ç½®ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº†ä¸€ä¸ªæ¥è‡ªä¸åŒæ ·æœ¬çš„10000ä¸ªä¼°è®¡å€¼çš„åˆ—è¡¨ã€‚è„šæœ¬simulation-repeat .pyä¸ç”Ÿæˆä»»ä½•è¾“å‡ºã€‚

```python
import numpy as np
import scipy.stats as stats

# set the random seed: 
np.random.seed(123456) 
# set sample size:
n = 100 
# initialize ybar to an array of length r=10000 to later store results:
r = 10000
ybar = np.empty(r) 

# repeat r times: 
for j in range(r): 
    # draw a sample and store the sample mean in pos. j=0,1,... of ybar: 
    sample = stats.norm.rvs(10, 2, size=n) 
    ybar[j] = np.mean(sample)
```

è„šæœ¬1.53 (simulation-repeat -results .py)åˆ†æäº†è¿™10000ä¸ªä¼°è®¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªè®¨è®ºè¾“å‡ºï¼Œä½†ä½ å¯ä»¥åœ¨é™„å½•ä¸­æ‰¾åˆ°å®Œæ•´çš„ä»£ç ã€‚ybarçš„å¹³å‡å€¼éå¸¸æ¥è¿‘æ–¹ç¨‹1.7ä¸­çš„æ¨å®š$\mu=10$ã€‚æ¨¡æ‹Ÿçš„æŠ½æ ·æ–¹å·®ä¹Ÿä¸ç†è®ºç»“æœ$\frac{\sigma^2}{n}=0.04$éå¸¸æ¥è¿‘ã€‚

æ³¨æ„ï¼Œåœ¨np.var()ä¸­ç”¨dof=1å¯¹è‡ªç”±åº¦è¿›è¡Œäº†è°ƒæ•´ï¼Œä»¥è®¡ç®—æ–¹å·®çš„æ— åä¼°è®¡ã€‚æœ€åå°†ä¼°è®¡çš„å¯†åº¦(ä½¿ç”¨æ¥è‡ªæ¨¡å—statmodelsçš„æ ¸å¯†åº¦ä¼°è®¡)ä¸ç†è®ºæ­£æ€åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒã€‚ç»“æœå¦‚å›¾1.17æ‰€ç¤ºã€‚é™¤äº†æ¨¡æ€é™„è¿‘çš„åŒºåŸŸ(å·²çŸ¥æ ¸å¯†åº¦ä¼°è®¡å™¨å­˜åœ¨é—®é¢˜çš„åŒºåŸŸ)ï¼Œè¿™ä¸¤æ¡çº¿å‡ ä¹æ— æ³•åŒºåˆ†ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ¨¡æ‹Ÿç»“æœè¯å®äº†å¼1.7ä¸­çš„ç†è®ºç»“æœã€‚å‡å€¼ã€æ–¹å·®å’Œå¯†åº¦éå¸¸æ¥è¿‘ï¼Œä¼¼ä¹å‰©ä¸‹çš„å¾®å°å·®å¼‚å¾ˆå¯èƒ½æ˜¯ç”±äºæˆ‘ä»¬â€œåªâ€ä½¿ç”¨äº†10000ä¸ªæ ·æœ¬ã€‚
è®°ä½ï¼šå¯¹äºå¤§å¤šæ•°é«˜çº§ä¼°è®¡å™¨æ¥è¯´ï¼Œè¿™æ ·çš„æ¨¡æ‹Ÿæ˜¯ç ”ç©¶å®ƒä»¬çš„ä¸€äº›ç‰¹å¾çš„å”¯ä¸€æ–¹æ³•ï¼Œå› ä¸ºä¸å¯èƒ½æ¨å¯¼å‡ºæ„Ÿå…´è¶£çš„ç†è®ºç»“æœã€‚å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™ä¸ªç®€å•çš„ä¾‹å­å¸Œæœ›èƒ½å¤Ÿé˜æ˜è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿçš„æ–¹æ³•å’ŒæŠ½æ ·åˆ†å¸ƒçš„å«ä¹‰ï¼Œå¹¶ä¸ºæˆ‘ä»¬è¿›è¡Œå…¶ä»–æœ‰è¶£çš„æ¨¡æ‹Ÿç»ƒä¹ åšå¥½å‡†å¤‡ã€‚

![image-20240403162810123](imgs/image-20240403162810123.png)

```python
[24] See Section 1.6.4 for the basics of random number generation.
```

#### ï¼ˆ2ï¼‰ä¼°è®¡é‡çš„æ¸è¿‘æ€§è´¨

æ¸è¿‘åˆ†æä¸å¤§æ ·æœ¬æœ‰å…³ï¼Œå¹¶ä¸ä¼°è®¡é‡å’Œå…¶ä»–ç»Ÿè®¡é‡çš„è¡Œä¸ºæœ‰å…³ï¼Œå› ä¸ºæ ·æœ¬å¤§å°næ— é™åˆ¶åœ°å¢åŠ ã€‚å…³äºè¿™äº›ä¸»é¢˜çš„è®¨è®ºï¼Œå‚è§Wooldridge(2019ï¼Œé™„å½•C.3)ã€‚æ ¹æ®å¤§æ•°å®šå¾‹ï¼Œä¸Šè¿°ä¾‹å­ä¸­çš„æ ·æœ¬å¹³å‡$\bar{\gamma}$éšç€ $(n\rightarrowâˆ)$ä»¥æ¦‚ç‡æ”¶æ•›äºæ€»ä½“å¹³å‡$\mu$ã€‚åœ¨(æ— é™)å¤§æ ·æœ¬ä¸­ï¼Œè¿™æ„å‘³ç€$E(\bar{\gamma} \rightarrow \mu),Var(\bar{\gamma})\rightarrow0$

æœ‰äº†è’™ç‰¹å¡ç½—æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬å°±æœ‰äº†ä¸€ä¸ªå·¥å…·ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­çœ‹åˆ°è¿™æ˜¯å¦‚ä½•å®ç°çš„ã€‚æˆ‘ä»¬åªéœ€è¦å°†è„šæœ¬1.52(simulation-repeat.py)ä¸­çš„ä»£ç è¡Œn=100ä¸­çš„æ ·æœ¬å¤§å°æ›´æ”¹ä¸ºä¸åŒçš„æ•°å­—å¹¶é‡æ–°è¿è¡Œæ¨¡æ‹Ÿä»£ç ã€‚å›¾1.18ä¸­æ˜¾ç¤ºäº†n=10ã€50ã€100å’Œ1000çš„ç»“æœã€‚æ˜¾ç„¶ï¼Œ$\bar{\gamma}$çš„æ–¹å·®å®é™…ä¸Šç¡®å®åœ¨å‡å°ã€‚n=1000çš„å¯†åº¦å›¾å·²ç»éå¸¸ç‹­çª„å’Œé«˜ï¼Œè¡¨æ˜æ–¹å·®å¾ˆå°ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸å¯èƒ½åœ¨ä¸ä½¿è®¡ç®—æœºå´©æºƒçš„æƒ…å†µä¸‹å°†nå¢åŠ åˆ°æ— ç©·ï¼Œä½†ä¼¼ä¹æœ‰å¯èƒ½çš„æ˜¯ï¼Œå½“nâ†’âˆæ—¶ï¼Œå¯†åº¦æœ€ç»ˆä¼šåå¡Œä¸ºä¸€æ¡å‚ç›´çš„çº¿ï¼Œå¯¹åº”$Var(\bar{\gamma})\rightarrow0$â€‹ã€‚

![image-20240403163607607](imgs/image-20240403163607607.png)

åœ¨æˆ‘ä»¬çš„æ¨¡æ‹Ÿä¾‹å­ä¸­ï¼Œéšæœºå˜é‡$\bar{\gamma}$æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œå› æ­¤æ— è®ºæ ·æœ¬å¤§å°å¦‚ä½•ï¼Œæ ·æœ¬å¹³å‡$\bar{\gamma}$ä¹Ÿæ˜¯æ­£æ€çš„ã€‚è¿™ä¸€ç‚¹ä¹Ÿå¯ä»¥åœ¨å›¾1.18ä¸­å¾—åˆ°è¯å®ï¼Œå›¾ä¸­å„è‡ªçš„æ­£æ€å¯†åº¦ä»¥è™šçº¿çš„å½¢å¼æ·»åŠ åˆ°å›¾ä¸­ã€‚ä¸­å¿ƒæé™å®šç†(central limit theorem,CLT)è®¤ä¸ºï¼Œå½“$n\rightarrow âˆ$æ—¶ï¼Œéšæœºæ ·æœ¬çš„æ ·æœ¬å‡å€¼$\bar{\gamma}$æœ€ç»ˆæ€»æ˜¯æ­£æ€åˆ†å¸ƒï¼Œæ— è®º$\bar{\gamma}$çš„åˆ†å¸ƒæ˜¯ä»€ä¹ˆ(é™¤éå®ƒéå¸¸å¥‡æ€ªï¼Œæ–¹å·®æ— ç©·å¤§)ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„åˆ†å¸ƒæ”¶æ•›ã€‚

```python
[25] A motivated reader will already have ï¬gured out that this graph was generated by chi2.pdf(x, df) from the scipy module.
```

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªéå¸¸éæ­£æ€çš„åˆ†å¸ƒï¼Œå³ä¸€ä¸ªè‡ªç”±åº¦çš„$\chi^2$â€‹åˆ†å¸ƒæ¥æ£€éªŒè¿™ä¸€ç‚¹ã€‚å®ƒçš„å¯†åº¦å¦‚å›¾1.19.25æ‰€ç¤ºï¼Œå®ƒçœ‹èµ·æ¥ä¸æˆ‘ä»¬ç†Ÿæ‚‰çš„é’Ÿå½¢æ­£æ€å¯†åº¦éå¸¸ä¸åŒã€‚æˆ‘ä»¬å¿…é¡»åœ¨Script 1.52(simulation-repeat .py)ä¸­çš„æ¨¡æ‹Ÿä»£ç ä¸­å°†è¯­å¥`sample=stats.norm.rvs(10,2,size=n)`æ›´æ”¹ä¸º`sample =stats.chi2.rvs(1,size=n)`æ ¹æ®è¡¨1.6ã€‚

![image-20240403164249552](imgs/image-20240403164249552.png)

å›¾1.20ç»™å‡ºäº†ä¸åŒæ ·æœ¬é‡ä¸‹çš„æ¨¡æ‹Ÿå¯†åº¦ï¼Œå¹¶ä¸å…·æœ‰ç›¸åŒå‡å€¼$\mu=1$ã€æ ‡å‡†å·®$\frac{s}{\sqrt{n}}=\sqrt{\frac{2}{n}}$çš„æ­£æ€åˆ†å¸ƒè¿›è¡Œäº†æ¯”è¾ƒã€‚æ³¨æ„ï¼Œä¸ºäº†æ›´å¥½åœ°å‘ˆç°å¯†åº¦çš„å½¢çŠ¶ï¼Œå­å›¾ä¹‹é—´çš„è½´çš„åªåº¦ç°åœ¨æœ‰æ‰€ä¸åŒã€‚æ–¹å·®é€’å‡çš„å½±å“åœ¨è¿™é‡Œçš„å·¥ä½œæ–¹å¼ä¸æ­£å¸¸æ€»ä½“çš„å·¥ä½œæ–¹å¼å®Œå…¨ç›¸åŒã€‚æ¯«ä¸å¥‡æ€ªï¼Œåœ¨n=2è¿™æ ·çš„å°æ ·æœ¬ä¸­ï¼ŒYçš„åˆ†å¸ƒä¸æ­£æ€åˆ†å¸ƒæœ‰å¾ˆå¤§çš„ä¸åŒã€‚éšç€æ ·æœ¬é‡çš„å¢åŠ ï¼ŒCLIå‘æŒ¥äº†å®ƒçš„é­”åŠ›ï¼Œåˆ†å¸ƒè¶Šæ¥è¶Šæ¥è¿‘æ­£å¸¸çš„é’Ÿå½¢ã€‚å¯¹äºn=10000ã€å¯†åº¦å‡ ä¹æ²¡æœ‰ä»»ä½•å·®å¼‚ï¼Œæ‰€ä»¥å¾ˆå®¹æ˜“æƒ³è±¡å®ƒä»¬æœ€ç»ˆå°†ä¸$n\rightarrow âˆ$ç›¸åŒã€‚

![image-20240403164149381](imgs/image-20240403164149381.png)

#### ï¼ˆ3ï¼‰ç½®ä¿¡åŒºé—´å’Œtæ£€éªŒçš„æ¨¡æ‹Ÿ

é™¤äº†åå¤ä¼°è®¡æ€»ä½“å‚æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è®¡ç®—ç½®ä¿¡åŒºé—´ï¼Œå¹¶å¯¹æ¨¡æ‹Ÿæ ·æœ¬è¿›è¡Œæ£€éªŒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæœ‰ç‚¹é«˜çº§çš„æ¨¡æ‹Ÿè·¯å¾„ã€‚

é˜…è¯»è¿™äº›ææ–™çš„å›æŠ¥æ˜¯ï¼Œå®ƒå¯èƒ½ä¼šå¤§å¤§æé«˜æˆ‘ä»¬å¯¹ç»Ÿè®¡æ¨æ–­å·¥ä½œçš„ç†è§£ã€‚æˆ‘ä»¬ä»ç¬¬1.9.1èŠ‚ä¸­ç›¸åŒçš„ä¾‹å­å¼€å§‹ï¼šåœ¨æ€»ä½“$Y\sim Normal(10,4)$ä¸­ï¼Œæˆ‘ä»¬ä»è¿™ä¸ªæ€»ä½“ä¸­æŠ½å–äº†$10000$ä¸ªå¤§å°ä¸ºn=100çš„æ ·æœ¬ã€‚å¯¹äºæ¯ä¸€ä¸ªæ ·æœ¬æˆ‘ä»¬è®¡ç®—å¦‚ä¸‹å†…å®¹ï¼š

- 95% ç½®ä¿¡åŒºé—´å¹¶å­˜å‚¨`CIlower`å’Œ`CIupper`çš„`limit`
- æ­£ç¡®çš„é›¶å‡è®¾$H_0:\mu=10$çš„åŒä¾§æ£€éªŒ$p$å€¼â€”â€”æ•°ç»„`pvalue1`
- é”™è¯¯çš„é›¶å‡è®¾$H_0:\mu=9.5$çš„åŒä¾§æ£€éªŒpå€¼â€”â€”æ•°ç»„`pvalue2`

æœ€åï¼Œæˆ‘ä»¬ç”¨é€»è¾‘é¡¹è®¡ç®—æ•°ç»„`reject1`å’Œ`reject2`ï¼Œå¦‚æœæˆ‘ä»¬åœ¨$\alpha=5\%$æ—¶æ‹’ç»å„è‡ªçš„é›¶å‡è®¾ï¼Œå³å¦‚æœ`pvalue1`æˆ–`pvalue2`åˆ†åˆ«å°äº`0.05`ã€‚è„šæœ¬`1.55 (Simulation-Inference.py)`æ˜¾ç¤ºäº†è¿™äº›æ¨¡æ‹Ÿçš„Pythonä»£ç å’Œç»“æœ`reject1`å’Œ`reject2`çš„é¢‘ç‡è¡¨ã€‚

å¦‚æœç†è®ºå’ŒPyhonä¸­çš„å®ç°æ˜¯å‡†ç¡®çš„ï¼Œæ‹’ç»ä¸€ä¸ªæ­£ç¡®çš„é›¶å‡è®¾(å³çŠ¯ç¬¬Iç±»é”™è¯¯)çš„æ¦‚ç‡åº”è¯¥ç­‰äºæ‰€é€‰æ‹©çš„æ˜¾è‘—æ€§æ°´å¹³$\alpha$ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡æ‹Ÿä¸­ï¼Œæˆ‘ä»¬åœ¨10 000ä¸ªæ ·æœ¬ä¸­çš„504ä¸ªä¸­æ‹’ç»äº†æ­£ç¡®çš„å‡è®¾ï¼Œè¿™ç›¸å½“äº`5.04%`ã€‚

æ‹’ç»é”™è¯¯å‡è®¾çš„æ¦‚ç‡è¢«ç§°ä¸ºæ£€éªŒçš„å¨åŠ›ã€‚å®ƒå–å†³äºå¾ˆå¤šä¸œè¥¿ï¼Œæ¯”å¦‚æ ·æœ¬é‡å’Œ$H_0$çš„è¯¯å·®â€œæœ‰å¤šä¸¥é‡â€ï¼Œå³$\mu_0$ä¸çœŸå®$\mu$çš„è·ç¦»æœ‰å¤šè¿œã€‚ç†è®ºåªæ˜¯å‘Šè¯‰æˆ‘ä»¬ï¼Œå¨åŠ›æ¯”$\alpha$å¤§ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡æ‹Ÿä¸­ï¼Œé”™è¯¯çš„é›¶å‡è®¾$H_0:\mu=9.5$åœ¨$69.9%$çš„æ ·æœ¬ä¸­è¢«æ‹’ç»ã€‚å¼ºçƒˆå»ºè®®è¯»è€…ä¿®æ”¹ä»¿çœŸä»£ç ï¼Œä»¥éªŒè¯å½“$\mu_0$è¿œç¦»$10$å’Œæ ·æœ¬å¤§å°$n$å¢åŠ æ—¶ï¼Œè¿™ç§åŠŸç‡å°±ä¼šå¢åŠ çš„ç†è®ºç»“æœã€‚

å›¾1.21ä»¥å›¾å½¢çš„æ–¹å¼å±•ç¤ºäº†å‰100ä¸ªæ¨¡æ‹Ÿæ ·æœ¬çš„$95\%CI$ï¼Œæ¯æ¡æ°´å¹³çº¿ä»£è¡¨ä¸€ä¸ª$CI$ã€‚åœ¨è¿™å‰100ä¸ªæ ·æœ¬ä¸­ï¼Œæ­£ç¡®çš„é›¶å‡è®¾åœ¨4ä¸ªæ¡ˆä¾‹ä¸­è¢«æ‹’ç»[26]ã€‚è¿™ä¸€äº‹å®æ„å‘³ç€ï¼Œå¯¹äºè¿™4ä¸ªæ ·æœ¬ï¼Œ$CI$æ²¡æœ‰è¦†ç›–$\mu_0=10$ï¼Œå‚è§Wooldridge(2019ï¼Œé™„å½•C6)å…³äº$CI$ä¸æµ‹è¯•ä¹‹é—´çš„å…³ç³»ã€‚è¿™å››ç§æƒ…å†µåœ¨å›¾çš„å·¦è¾¹ç”¨é»‘è‰²æ ‡å‡ºï¼Œè€Œå…¶ä»–çš„åˆ™æ˜¯ç°è‰²ã€‚$t$æ£€éªŒåœ¨å‰100ä¸ªæ ·æœ¬ä¸­æœ‰72ä¸ªæ‹’ç»äº†é”™è¯¯çš„é›¶å‡è®¾ï¼Œ$H_0:\mu=9.5$ï¼Œåœ¨å›¾1.21çš„å³ä¾§ä»¥é»‘è‰²ç»˜åˆ¶ã€‚

```python
import numpy as np
import scipy.stats as stats

# set the random seed: 
np.random.seed(123456) 
# set sample size and MC simulations:
r = 10000 
n = 100 

# initialize arrays to later store results: 
CIlower = np.empty(r) 
CIupper = np.empty(r) 
pvalue1 = np.empty(r)
pvalue2 = np.empty(r) 

# repeat r times: 
for j in range(r): 
    # draw a sample: 
    sample = stats.norm.rvs(10, 2, size=n)
    sample_mean = np.mean(sample) 
    sample_sd = np.std(sample, ddof=1) 
    # test the (correct) null hypothesis mu=10: 
    testres1 = stats.ttest_1samp(sample, popmean=10)
    pvalue1[j] = testres1.pvalue
    cv = stats.t.ppf(0.975, df=n - 1)
    CIlower[j] = sample_mean - cv * sample_sd / np.sqrt(n)
    CIupper[j] = sample_mean + cv * sample_sd / np.sqrt(n) 
    
    # test the (incorrect) null hypothesis mu=9.5 & store the p value: 
    testres2 = stats.ttest_1samp(sample, popmean=9.5) 
    pvalue2[j] = testres2.pvalue 
    
# test results as logical value:
reject1 = pvalue1 <= 0.05 
count1_true = np.count_nonzero(reject1)  # counts true 
count1_false = r - count1_true 
print(fâ€™count1_true: {count1_true}\nâ€™) 
print(fâ€™count1_false: {count1_false}\nâ€™) 

reject2 = pvalue2 <= 0.05 
count2_true = np.count_nonzero(reject2)
count2_false = r - count2_true 
print(fâ€™count2_true: {count2_true}\nâ€™) 
print(fâ€™count2_false: {count2_false}\nâ€™)
```

![image-20240403164638784](imgs/image-20240403164638784.png)

```python
[26] For the sake of completeness, the code for generating these graphs is shown in Appendix IV, Script 1.54 (Simulation-Inference-Figure.py), but most readers will probably not ï¬nd it important to look at it at this point.
```

## ä¸‰ã€æœ¬é¡¹ç›®å‚è€ƒèµ„æ–™ï¼š

[1] Using Python for Introductory EconometricsÂ© Florian Heiss, Daniel Brunner 2020. All rights reserved

ä¹¦ç±åœ°å€ï¼šhttp://www.upfie.net/downloads/PDF/UPfIE_web.pdf

ä»£ç æ•°æ®ä¸‹è½½ï¼šhttp://www.upfie.net/code.html

[2] åº”ç”¨è®¡é‡ç»æµå­¦è®²ç¨¿æ•ˆå‚²æ±Ÿæ¹–:æ•ˆåº”è¯„ä¼°çš„ç‹¬å­¤ä¹å‰‘.è®¸æ–‡ç«‹ï¼Œ2022

[3] é«˜çº§è®¡é‡ç»æµå­¦åŠStataåº”ç”¨/é™ˆå¼ºç¼–è‘—.åŒ—äº¬ï¼šé«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾ï¼Œ2014.4

[4] Pythonæ•°æ®åˆ†æåŸºç¡€ï¼ˆç¬¬2ç‰ˆï¼‰/é˜®æ•¬ç¼–è‘—.åŒ—äº¬ï¼šä¸­å›½ç»Ÿè®¡å‡ºç‰ˆç¤¾ï¼Œ2018.9
