# 09.其他补充知识

## 一、命令行工具

### 1. scrapy.cfg

`scrapy`项目生成时将自动创建一个`ini`样式的`cfg`配置文件，其中`settings`下指示项目设置类的路径，该配置可以由多个项目共享。

```python
[settings]
default = project.settings
project1 = myproject1.settings
project2 = myproject2.settings

[deploy]
#url = http://localhost:6800/
project = project
```

### 2.命令行基本语法

- 基本语法

```shell
scrapy <command> [options] [args]
```

> 如果不记得相关指令，可以直接输如`scrapy`，后台会弹出相关帮助信息，对某些命令不太熟悉，则可以输如 `-h`指令查询帮助。

- `startproject`

创建一个名为 `project_name` 下 `project_dir` 目录。如果 `project_dir` 没有指定， `project_dir` 将与 `project_name` 

```shell
# scrapy startproject <project_name> [project_dir]
scrapy startproject myproject
```

- `genspider`

在当前文件夹或当前项目的 `spiders` 文件夹（如果从项目内部调用）。这个 `<name>` 参数设置为spider的 `name` ，同时 `<domain>` 用于生成 `allowed_domains` 和 `start_urls` 蜘蛛的属性。

```shell
 # scrapy genspider [-t template] <name> <domain>
 # template 可选 : [basic, crawl, csvfeed, xmlfeed]

 scrapy genspider example example.com
 scrapy genspider -t crawl scrapyorg scrapy.org
```

- `crawl`

开始用蜘蛛爬行。

```shell
# scrapy crawl <spider>
 scrapy crawl myspider
```

- `check`

运行检查，可以事先检测出框架是否缺少某些功能， `-l`列出所有可供检查的项。

```shell
 # scrapy check [-l] <spider>
```

- `list`

列出当前项目中所有可用的`spider`。每行输出一个蜘蛛。

```sehll
scrapy list
```

- `edit`

使用编辑器编辑给定的蜘蛛 `EDITOR` 环境变量或（如果未设置） `EDITOR  `设置。

```shell
# scrapy edit <spider>
scrapy edit spider1
```

- `fetch`

使用`ScrapyDownloader`下载给定的`URL`，并将内容写入标准输出，支持的选项：

```shell
 # scrapy fetch --args <url>
 # args可选 [ --spider=SPIDER ] : 绕过Spider自动检测并强制使用特定Spider
 #			[--headers] : 打印响应的HTTP头而不是响应的正文
 # 			[--no-redirect] : 不遵循HTTP 3xx重定向（默认为遵循）。

scrapy fetch --nolog http://www.example.com/some/page.html
```

- `view`

```shell
# scrapy view --args <url>
# args可选 [ --spider=SPIDER ] : 绕过Spider自动检测并强制使用特定Spider
# 		   [--no-redirect] : 不遵循HTTP 3xx重定向（默认为遵循）。
scrapy view http://www.example.com/some/page.html
```

- `shell`

为给定的URL（如果给定）启动`scrapy shell`；如果没有给定URL，则为空。还支持`Unix`风格的本地文件路径，无论是相对于 `./` 或 `../` 前缀或绝对文件路径。

```shell
#  scrapy shell --args [url]
# args可选  [ --spider=SPIDER ] : 绕过Spider自动检测并强制使用特定Spider
#		    [-c code] : 评估shell中的代码，打印结果并退出
# 		    [--no-redirect] : 不遵循HTTP 3xx重定向（默认为遵循）。

scrapy shell http://www.example.com/some/page.html
scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
shell follows HTTP redirects by default
scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
```

- `parse`

获取给定的URL，并使用处理它的spider，使用 `--callback` 选项，或 `parse` 如果没有给出。

```shell
# scrapy parse --args  <url> [options]
# --spider=SPIDER ：绕过Spider自动检测并强制使用特定Spider
# --a NAME=VALUE ：set spider参数（可以重复）
# --callback 或 -c ：用作分析响应的回调的spider方法
# --meta 或 -m ：将传递给回调请求的附加请求元。这必须是有效的JSON字符串。示例：--meta='“foo”：“bar”'
# --cbkwargs ：将传递给回调的其他关键字参数。这必须是有效的JSON字符串。示例：--cbkwargs='“foo”：“bar”'
# --pipelines ：通过管道处理项目
# --rules 或 -r 使用 CrawlSpider 发现用于解析响应的回调（即spider方法）的规则
# --noitems ：不显示爬取的项目
# --nolinks ：不显示提取的链接
# --nocolour ：避免使用Pygments对输出着色
# --depth 或 -d ：应递归执行请求的深度级别（默认值：1）
# --verbose 或 -v ：显示每个深度级别的信息
# --output 或 -o ：将刮取的项目转储到文件

scrapy parse http://www.example.com/ -c parse_item
```

- `settings`

获取 `Scrapy` 设置的值，如果在项目中使用，它将显示项目设置值，否则它将显示该设置的默认` Scrapy `值。

```shell
 # scrapy settings [options]
 scrapy settings --get BOT_NAME
 scrapy settings --get DOWNLOAD_DELAY
```

- `runspider`

运行一个包含在`python`文件中的`spider`，而不必创建一个项目。

```shell
 # scrapy runspider <spider_file.py>
scrapy runspider myspider.py
```

- `version`

打印`scrapy`版本。

```python
 # scrapy version [-v]
```

- `bench`

运行一个快速项目测试。

```python
scrapy bench
```

### 3.自定义命令

可以仿照 [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands)实现自定义的命令类：

```python
# mybot/commands.py
from scrapy.commands import ScrapyCommand

class MyCustomCommand(ScrapyCommand):
    requires_project = True
    def run(self, args, opts):
        """ args, opts 是 run 方法的通用逻辑"""
        # 执行自定义命令的逻辑
        self.stdout.write('Hello from custom command!\n')
```

实现完毕后，有两种方式激活命令：

- `COMMANDS_MODULE`

在`settings.py`中增加命令路径。

```python
# settings.py
COMMANDS_MODULE = 'mybot.commands'
```

- `setup.py`

在该库的`setup.py`文件中添加一个`scrapy.commands`入口点。

```python
# setup.py
from setuptools import setup, find_packages

setup(
    name="scrapy-mymodule",
    packages=find_packages(),
    entry_points={
        'scrapy.commands': [
            'my_command=my_scrapy_module.commands:MyCommand',
        ],
    },
)
```

## 二、Scrapy-Shell

`Scrapy-shell`是一个交互终端，可以在未启动spider的情况下尝试及调试代码，也可以用来测试XPath或CSS表达式，查看他们的工作方式，方便从爬取的网页中提取的数据，如果安装了 IPython ，Scrapy终端将使用 IPython (替代标准Python终端)。 IPython 终端与其他相比更为强大，提供智能的自动补全，高亮输出，及其他特性。（推荐安装IPython）

### 1.配置`Scrapy-Shell`

可以在`scrapy.cfg`文件中添加设置控制启动`shell`的标准`shell`，

```python
[settings]
shell = ipython  # 可选 ipython ， bpython 或 python
```

### 2.启动shell

```shell
scrapy shell <url>
```

https://www.osgeo.cn/scrapy/topics/shell.html

https://docs.scrapy.org/en/latest/topics/shell.html

## 三、`scrapy`中的协程

`Scrapy`支持部分协程语法（ `await` / `async for` / `async with`），目前仅仅支持部分可调用对象直接使用，主要包括：

- `Request `的`callbacks` ；
- `Pipelines` 中的`process_item()`；
- `Downloader middlewares` 中的`process_request()`、`process_response()`和`process_exception()`；
- `spider middlewares`中的`process_spider_output()`

> 协程与异步：异步编程常用在`IO`较频繁的系统中如文件下载、爬虫开发过程中广泛应用，协程能够在`IO`等待时间就去切换执行其他任务，当`IO`操作结束后再自动回调，从而大大节省资源并提供性能。
>
> 在`Python`中有多种方式可以实现协程，如`yield`生成器实现协程、`python3.4`引入的内置模块`asyncio`、第三方模块`greenlet`和`python3.5`引入的协程关键字`async` & `awiat`。

#### （1）`Pipelines` 中的`process_item()`协程案例

```python
from itemadapter import ItemAdapter


class DbPipeline:
    async def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        adapter["field"] = await db.get_some_data(adapter["id"])
        return item
```

当遇到耗时较大的任务时，将自动切换到其他任务，等待任务完成后再继续执行。

#### （2）`Request `的`callbacks` 

并行发送多个请求，实现同时下载。

```python
from scrapy import Spider, Request
from scrapy.utils.defer import maybe_deferred_to_future
# 将 Twisted 的 Deferred 转换为 Python 3.5+ 引入的 async/await 语法使用的 Future 对象。
from twisted.internet.defer import DeferredList
# 导入 Deferred 对象对应的列表


class MultipleRequestsSpider(Spider):
    name = "multiple"
    start_urls = ["https://example.com/product"]

    async def parse(self, response, **kwargs):
        additional_requests = [
            Request("https://example.com/price"),
            Request("https://example.com/color"),
        ]
        deferreds = []
        for r in additional_requests:
            deferred = self.crawler.engine.download(r)
            deferreds.append(deferred)
        responses = await maybe_deferred_to_future(DeferredList(deferreds))
        yield {
            "h1": response.css("h1::text").get(),
            "price": responses[0][1].css(".price::text").get(),
            "price2": responses[1][1].css(".color::text").get(),
        }
```

#### （3）混合同步和异步爬虫中间件

> 暂时看不懂先记录下来，等学完中间件再来看。

https://docs.scrapy.org/en/latest/topics/coroutines.html#mixing-synchronous-and-asynchronous-spider-middlewares





## [在浏览器中打开](https://www.osgeo.cn/scrapy/topics/debug.html#open-in-browser)

有时，您只想查看某个响应在浏览器中的外观，可以使用 `open_in_browser` 功能。以下是您将如何使用它的示例：

```
from scrapy.utils.response import open_in_browser

def parse_details(self, response):
    if "item name" not in response.body:
        open_in_browser(response)
```

`open_in_browser` 将打开一个浏览器，此时Scrapy接收到响应，调整 [base tag](https://www.w3schools.com/tags/tag_base.asp) 以便正确显示图像和样式。