# 03.基本特征伪造

## 一、伪造UA信息

一般情况下，服务器都会检查请求的头域，尤其是`UA`信息，但是，如果某一个固定的请求头多次访问，也有可能会被服务器标记为爬虫，所以有时需要配置随机请求头。

构建`UA`有很多方案，一种方案是构建本地的分类明确的请求头，该方案费时较长，但是在特定情况下非常有效，有些网站在开发时，为了兼顾移动用户和各类电脑用户，后端服务会根据用户浏览器的类型选择性的返回响应，如百度贴吧，遇到IE6或移动端浏览器时，其响应较为简单，数据非常容易提取而且很多反爬手段都失效了。

### 1.规则组装

该函数生成的`ua`不一定可用。

```python
import random
def get_ua():
    """Get some user-agent ,But not necessarily accepted by the website"""
    windows_versions = ('Windows NT 10.0', 'Windows NT 6.3', 'Windows NT 6.2', 'Windows NT 6.1')
    architectures = ('Win64; x64', 'Win32; x86', 'ARM', 'Win64; x64')
    processors = ('Intel i3', 'Intel i5', 'Intel i7', 'Intel i9', 'AMD Ryzen')
    browsers = (
        'Mozilla/5.0 ({os}; {arch}; {proc}) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{chrome_version} Safari/537.36',
        'Mozilla/5.0 ({os}; {arch}; {proc}) AppleWebKit/537.36 (KHTML, like Gecko) Edge/{edge_version} Safari/537.36',
        'Mozilla/5.0 ({os}; {arch}; rv:{firefox_version}) Gecko/20100101 Firefox/{firefox_version}'
    )
    chrome_version = f'{random.randint(80, 125)}.0.{random.randint(4000, 4999)}.{random.randint(100, 999)}'
    edge_version = f'{random.randint(85, 125)}.0.{random.randint(1000, 1999)}.{random.randint(100, 999)}'
    ua = random.choice(browsers).format(
        os=random.choice(windows_versions),
        arch=random.choice(architectures),
        proc=random.choice(processors),
        chrome_version=chrome_version,
        edge_version=edge_version,
        firefox_version=f'{random.randint(85, 125)}.0'
    )
    return ua    
# {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.2071.45 Safari/537.36'}
```

### 2.积累`UA`信息

可以利用随机函数库`random` 来实现随机的效果，通过调用`random.choice([user-agent]) `随机pick数组中一个请求头即可。

```python
ua_list = [
    "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
]
```

### 3.随机`UA`模块

`Faker`和`fake_useragent`都可以用于模拟生成随机请求头，安装命令。

```python
pip install Faker
pip install fake_useragent
```

具体使用可参考官方文档：[fake-useragent](https://fake-useragent.readthedocs.io/en/latest/)/[Faker](https://faker.readthedocs.io/en/master/)，更推荐使用`fake_useragent`，如下是其基本使用的代码。

```python
from fake_useragent import UserAgent

def generate_random_ua():
    ua = UserAgent(
        os=["windows", "macos"],
        min_version=100.0,
        platforms=["pc"],
        fallback=(
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0"
        )  # 网络异常时随机从中选一个。
    )
    return ua.random
```

使用`fake_useragent`时可能会因为网络波动导致请求头获取失败，从而导致整个请求异常，这时可以将该库中使用的所有请求头下载下来[地址](https://fake-useragent.herokuapp.com/browsers/0.1.11)并保存为`fake_useragent.json`调用使用即可。

```python
from fake_useragent import UserAgent
import os

ua_path = os.getcwd() + r'\fake_useragent.json'
ua = UserAgent(path=ua_path)
print(ua.chrome) # 谷歌
print(ua.ie)  # IE
print(ua.opera)  # 欧鹏
print(ua.firefox)  # 火狐
print(ua.safari)  # Safari
print(ua.random)  # 随机获取任意UA

print(UserAgent().random) # 也可以使用类方法
print(ua.random)
```

## 二、使用代理IP

网站可能会采取基于`IP`的反爬策略，通过检测某个 IP 在单位时间内的请求次数，如果超过了某个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封` IP`，解决的一种思路就是伪装`IP`，伪装`IP`的一种手段就是使用代理，**这里要注意尽量不要使自己原本的IP被标记。**

### 1.代理原理与作用

代理实际上指的就是代理服务器，英文叫作 `Proxy Server`，代理`ip`就是一个代理服务器的真实`ip`地址。正常请求网站资源时，本机向服务器发送一个请求，服务器返回了对应的响应。如果设置了代理服务器，本机发送的请求会由代理服务器转发给服务器，服务器返回的资源也由代理服务器转发给本机，在这个过程中，服务器识别的`IP`信息是代理服务器的`IP`地址而不是本机的真实`IP`。

| 代理应用场景   | 说明                                                         |
| -------------- | ------------------------------------------------------------ |
| 突破访问限制   | 使用代理`ip`绕过网站防止被目标网站封禁或限制访问。           |
| 提高网络安全性 | 使用代理`IP`隐藏真实`IP`地址防止黑客攻击和网络钓鱼等安全威胁。 |
| 访问内部资源   | 一些资源需要局域网内部地址才能访问，可以使用代理服务访问一些单位或团体内部资源。 |
| 提高访问速度   | 利用代理服务器提前缓存所需信息。                             |

### 2.代理IP的分类

对代理进行分类时，可以根据协议区分，也可以根据其匿名程度区分。

#### 1）根据转发请求分类

|   类别   | 说明                                                         |
| :------: | ------------------------------------------------------------ |
| 正向代理 | 为浏览器或客户端（发送请求的一方）转发请求的，叫做正向代理。浏览器知道最终处理请求的服务器的真实`ip`地址，例如`VPN` |
| 反向代理 | 为最终处理请求的服务器转发请求的，叫做反向代理。浏览器不知道服务器的真实地址，例如`nginx`。 |

> 代码中传入的代理都是正向代理，网站防止被攻击使用的代理都是反向代理。

#### 2）根据匿名程度分类

| 代理类别 | 说明                                                         |
| :------: | ------------------------------------------------------------ |
| 透明代理 | 在请求中显示客户端的真实 IP，不具备匿名作用，内网中的硬件防火墙就是透明代理。 |
| 匿名代理 | 在请求头中添加如`HTTP_VIA`等字段，告诉服务器本请求是转发的，但不显示客户端的真实 `IP`。 |
| 高匿代理 | 原封不动的转发数据包，服务器记录的 `IP` 是代理服务器的 `IP`。 |
| 间谍代理 | 组织或个人创建的，用于记录用户传输的数据，然后进行研究、监控等目的的代理服务器。 |

> 无论使用什么代理，都一定能找到客户机真实`IP`，因为代理服务器会记录客户机的真实`IP`，而所有代理厂商及其服务器`IP`都有备案，如果违反法律是可以查到的。

```python
# 透明代理(Transparent Proxy):目标服务器接收到的请求头
REMOTE_ADDR = Proxy IP
HTTP_VIA = Proxy IP
HTTP_X_FORWARDED_FOR = Your IP
# 匿名代理(Anonymous Proxy):目标服务器接收到的请求头
REMOTE_ADDR = proxy IP
HTTP_VIA = proxy IP
HTTP_X_FORWARDED_FOR = proxy IP
# 高匿代理(Elite proxy或High Anonymity Proxy):目标服务器接收到的请求头
REMOTE_ADDR = Proxy IP
HTTP_VIA = not determined
HTTP_X_FORWARDED_FOR = not determined
```

#### 3）根据代理的协议区分

|     代理类别     | 说明                                                         |
| :--------------: | ------------------------------------------------------------ |
|    `FTP 代理`    | 主要用于访问 FTP 服务器，一般有上传、下载以及缓存功能，端口一般为 21、2121 等。 |
|   `HTTP 代理`    | 主要用于访问网页，一般有内容过滤和缓存功能，端口一般为 80、8080、3128 等。 |
|  `SSL/TLS 代理`  | 主要用于访问加密网站，一般有` SSL` 或 `TLS `加密功能（最高支持 128 位加密强度），端口一般为 443。 |
|   `RTSP 代理`    | 主要用于` Realplayer` 访问` Real `流媒体服务器，一般有缓存功能，端口一般为 554。 |
|  `Telnet 代理`   | 主要用于 Telnet 远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为 23。 |
| `POP3/SMTP 代理` | 主要用于 `POP3/SMTP` 方式收发邮件，一般有缓存功能，端口一般为 110/25。 |
|   `SOCKS 代理`   | 只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为 1080。 |

`SOCKS` 代理协议又分为` SOCKS4` 和 `SOCKS5`，`SOCKS4 `协议只支持 `TCP`，而 `SOCKS5` 协议支持`TCP `和 `UDP`，还支持各种身份验证机制、服务器端域名解析等。

### 3.常见代理网站

#### 1）免费代理

如[快代理](https://free.kuaidaili.com/free/)，使用前抓取下来并筛选一下可用代理，并检测其代理类型，可以自行维护一个代理池，`github`也有很多维护的开源的代理池，可以在有需要的时候拉一个。

#### 2）付费代理

直接百度或谷歌搜索代理会有很多

##### 3）`ADSL` 拨号

链接一次网络换一个 `IP`，稳定性高。

##### 4）蜂窝代理

用 `4G` 或 `5G` 网卡等制作的代理。

### 4.代理的使用

代理的具体使用，一些收费网站上会给出各种语言、各种模块下的代理使用`demo`，各种请求模块如`urllib`、`requests`、`httpx`、`aiohttp`、`websocket`也会在文档中给出使用代理的`demo`，在使用时建议参考使用模块对应版本的官方文档，如下给出`requests`使用的`demo`：

```python
# 无需认证形
proxy_meta = 'http://%(ip)s:%(port)s' % {"ip":ip,"port":port}
# 账号密码认证
proxy_meta = "http://%(user)s:%(pwd)s@%(ip)s:%(port)s/" % {"user": username, "pwd": password, "ip":ip,"port":port}

response = requests.get(url, proxies={'https://':  proxy_meta, 'http://': proxy_meta})
```

### 5.搭建`IP`代理池

配置代理IP首先要从各种代理IP网站上爬取IP，爬取后应该测试该IP能否使用，将可以使用的IP放入内存或硬盘中，当使用的时候从中获取IP使用即可，如从网站https://free.kuaidaili.com/free/中提取代理并使用，测试网址使用http://httpbin.org/get。

https://cuiqingcai.com/7048.html

## 三、伪造认证信息

### 1.网站认证方式

`HTTP` 是一种无状态的协议，客户端每次发送请求时，首先要和服务器端建立一个连接，在请求完成后又会断开这个连接。这种方式可以节省传输时占用的连接资源，但同时也存在一个问题：每次请求都是独立的，服务器端无法判断本次请求和上一次请求是否来自同一个用户，进而也就无法判断用户的登录状态。

#### 1）`Cookie + Session`

> 为了解决 HTTP 无状态的问题，Lou Montulli 在 1994 年的时候，推出了 Cookie。

网站通过 Session 和 Cookie 的相互配合工作实现用户登陆状态的验证与维持，具体而言

- 用户首次登录验证成功之后，后端会创建一个 Session 对象然后保存到缓存或者数据库里
- 然后在响应登录接口的响应头里，设置 Set-Cookie 字段，并把 SessionId 等信息写入进去，并设置过期时间，这些信息就是 Cookie，然后浏览器会保存这些 Cookie 信息
- 然后之后再发送请求的时候，如果当前域名有保存 Cookie 信息的话，浏览器会自动在请求头上添加 Cookie 字段，并带上保存的 Cookie 信息
- 然后后端接收到请求后，会把请求头中的 Cookie 信息提取出来和存在服务器的对应的 Session 信息作对比，如果一致就说明登录验证成功了，不需要再重复登录。

**主要缺点**

主要是 `CSRF攻击`：因为 Session 是基于 Cookie 识别的，如果 Cookie 被截获，然后人家就可以利用截获的 Cookie 信息跳过登录，直接进入登录状态，发起跨站伪造请求。

另外如果把 Session 存在服务器内存，没有存到数据库的话，还会有以下问题：

- `多服务器无法共享`：比如我们登录的时候，，然后我们网站不同的资源可能存在不同的服务器，当我们请求另一台服务器时里面就没有 Session 信息，就会被认为没有登录过。所以导致这种方式的话，前端代码和后端代码都只能放在同一台服务器上
- `服务器压力大`：因为用户登录后的 Session 信息是存在内存里，如果用户量很大，服务器压力也会增大

#### 2） JWT（JSON Web Token）

`JWT`，英文全称为 `JSON Web Token`，是为了在网络应用环境间传递声明而执行的一种基于 `JSON` 的开放标准。实际上就是在每次登录的时候通过一个 `Token` 字符串来校验登录状态。`JWT` 的声明一般被用来在身份提供者和服务提供者之间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的业务逻辑所必须的声明信息，所以这个 `Token` 也可直接被用于认证，也可传递一些额外信息。

`JWT` 通常就是一个加密的字符串，它也有自己的标准，类似如下格式：

```python
eyJ0eXAxIjoiMTIzNCIsImFsZzIiOiJhZG1pbiIsInR5cCI6IkpXVCIsImFsZyI6IkhTMjU2In0.eyJVc2VySWQiOjEyMywiVXNlck5hbWUiOiJhZG1pbiIsImV4cCI6MTU1MjI4Njc0Ni44Nzc0MDE4fQ.pEgdmFAy73walFonEm2zbxg46Oth3dlT02HR9iVzXa8
```

可以发现中间有两个用来分割的 `.` ，因此可以把它看成是一个三段式的加密字符串，分别是` Header`、`Payload`、`Signature`。

- `Header`，声明了 `JWT` 的签名算法，如 `RSA、SHA256` 等，也可能包含 `JWT` 编号或类型等数据，然后对整个信息进行 `Base64` 编码即可。
- `Payload`，通常用来存放一些业务需要但不敏感的信息，如 `UserID` 等，另外它也有很多默认是字段，如` JWT `签发者、`JWT` 接受者、`JWT `过期时间等，`Base64` 编码即可。
- `Signature`，就是一个签名，是把 `Header`、`Payload` 的信息用秘钥` secret `加密后形成的，这个 `secret` 是保存在服务器端的，不能被轻易泄露。如此一来，即使一些 `Payload `的信息被篡改，服务器也能通过` Signature` 判断出非法请求，拒绝服务。

JWT认证的流程和 Session 认证差不多：

- 首次登录验证成功之后，后端一般用 `JWT` 将用户信息、签名等加密生成一串字符串，存到数据库并返回给前端
- 前端再存起来，存在 `Cookie`、`SessionStorage`、`LocalStorage` 都可以
- 之后再次请求时浏览器也不会默认携带，需要自己在请求拦截器里添加请求头，把` Token` 信息带上
- 然后后端就拿到 `Token` 信息来查找数据库里有没有，如果有再执行查询数据库操作等返回前端请求的数据，因为设置了过期时间的话，时间到了 `Redis` 会自动就删掉数据库里的` Token`，查询就不会有。当然也有人是开定时任务去删。

当然也有不把 `Token` 存在数据库，而是生成 `Token` 的时候，把用户信息，过期时间等一起加密发给前端，之后再请求时，后端拿到 `Token` 就先解密，再查询解密出来的用户信息，如果查询到了并且也没有过期，就查询返回前端请求的数据

主要特点：

- 服务器端不需要存放 Token，所以不会对服务器端造成压力，即使是服务器集群，也不需要增加维护成本。
- Token 可以存放在前端任何地方，可以不用保存在 Cookie 中，提升了页面的安全性。
- Token 下发之后，只要在生效时间之内，就一直有效，如果服务器端想收回此 Token 的权限，并不容易。

### 2.信息伪造思路

认证信息加密大致有两种方式：

- 浏览器向服务器发送请求，服务器在响应头带上认证信息，下次请求的时候需要再带上认证信息去进行请求；

- 浏览器向服务器发送请求，认证信息存放在响应体内部，通过`js`代码动态生成认证信息。

#### 1）直接复制认证信息

如果已经在浏览器中登录了自己的账号，要想用爬虫模拟，那么可以直接把 `Cookie` 复制过来交给爬虫。这是最省时省力的方式，相当于我们用浏览器手动操作登录了。把` Cookie` 放到代码里，爬虫每次请求的时候再将其放到` Request Headers` 中，完全模拟了浏览器的操作。之后服务器会通过 `Cookie` 校验登录状态，如果没问题，自然就可以执行某些操作或返回需要的响应。

> 假如`cookie`信息加密参数中传入了时间戳，服务器验证接收到请求的时间与`cookie`生成时间不同，则复制的方式就会失效。

#### 2）本地生成认证信息

整个爬虫逻辑无非是发送了一些请求并接收到了一些响应，如果`cookie`来源于某个响应，则可以像伪造参数一样分析`cookie`生成的逻辑，对所有内容进行算法还原，只是这种方式往往比较困难。

#### 3）使用自动化工具模拟登录

如果单纯是参数的复杂，往往可以使用 `Selenium`、`Pyppeteer` 或 `Playwright` 来驱动浏览器模拟执行一些操作，如填写用户名和密码、提交表单等，如果需要短信验证，则可以使用相关的自动化工具或者接码平台获取验证码。拿到认证信息后，可以通过构建服务的方式与下载程序协同，同样可以实现模拟网站认证。

### 3.搭建cookie池

如果爬虫要求爬取的数据量比较大或爬取速度比较快，而网站又有单凭证并发限制或者访问状态检测等反爬虫手段，那么认证凭证可能就会无法访问或者面临封禁的风险了。这时可以使用分流的方案来实现。假设某个网站设置一分钟之内检测到同一个账号访问 3 次或 3 次以上则封号，可以建立一个账号池，用多个账号来随机访问或爬取数据，这样就能大幅提高爬虫的并发量，降低被封号的风险了。

比如准备 100 个账号，然后 100 个账号都模拟认证，把对应的 Cookie 或 JWT 存下来，每次访问的时候随机取一个来，由于账号多，所以每个账号被取用的概率也就降下来了，这样就能避免单账号并发过大的问题，也降低封号风险。

https://cuiqingcai.com/8243.html

## 四、TLS指纹

