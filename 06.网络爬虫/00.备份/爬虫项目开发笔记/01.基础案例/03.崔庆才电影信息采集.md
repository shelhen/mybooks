## 03.电影列表详情模拟采集

### 一、直接爬取网站内容

#### 1.目标地址：

https://ssr1.scrape.center/

#### 2.需求分析：

爬取每一页电影列表，并且爬取详情页面

提取出电影名称，封面，类别，上映时间，评分，剧情简介

将结果保存为json文件

#### 3.抓包分析

#### 4.爬虫开发分析

##### 1）首先进入列表页面，主要目的就是抓取url，可以临时抓取电影题目

##### 2）进入url页面抓取[名称，封面，类别，上映时间，评分，剧情简介]内容

封面如何抓取？保存为图片并且返回路径。

##### 3）抓取完一页之后应该翻页继续，最后一页的标志是什么？什么时候停止？

#### 5.代码

```python
import requests
from parsel import Selector
import json
import re


class MoviesSpiders(object):

    def __init__(self):
        self.session = requests.session()
        self.headers={
            "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }
        self.url="https://ssr1.scrape.center/"

    def get_list(self, num):
        url = f"https://ssr1.scrape.center/page/{num + 1}"
        lists = self.session.get(url,headers=self.headers,timeout=10)
        listsel = Selector(lists.text)
        anodes = listsel.xpath('//a[@class="name"]')
        hrefs = (node.attrib['href'] for node in anodes)
        return hrefs

    def get_pages(self, href):
        movie = {}
        details = self.session.get(self.url + href[1:], headers=self.headers, timeout=10)
        detailsel =  Selector(details.text)
        divNode = detailsel.xpath('//div[@class="el-card__body"]/div/div')
        movie['cover'] = divNode.xpath('//img[@class="cover"]').attrib['src']
        movie['score'] = divNode.css('p[class*="score"]::text').getall()[0].replace('\n','').replace(' ','')
        movie['name'] = divNode.xpath('./a/h2/text()').get()
        movie['categories'] = divNode.xpath('./div[@class="categories"]/button/span/text()').getall()
        infos = ''.join(divNode.xpath('./div/span/text()').getall()).replace('/n','').replace(' ','')
        publish = re.search('(\d{4}-\d{2}-\d{2})',infos)
        movie['time'] = publish.group() if publish else '未知'
        movie['describe'] = divNode.xpath('./div[@class="drama"]/p/text()').get().replace('\n','').replace(' ','')
        print(movie)
        return movie

    def save(self,path, data):
        json.dump(data, open(path, 'w', encoding='utf-8'), indent=2, ensure_ascii=False)

    def main(self):
        path = './datas/doubanmovie.json'
        # 首先发起访问获取总页数
        pre_response = self.session.get(self.url,headers=self.headers)
        presel = Selector(pre_response.text)
        pageNum = presel.xpath('//span[@class="el-pagination__total"]/text()').getall()[0]
        pageNum = re.search(r'\d+', pageNum).group()
        num = int(pageNum)//10
        result=[]
        for i in range(num):
            hrefs = self.get_list(i)
            movies = [self.get_pages(href) for href in hrefs]
            result.extend(movies)
        self.save(path, result)


if __name__ == '__main__':
    ms = MoviesSpiders()
    ms.main()
```

### 二、先登录后采集

#### 目标地址：https://login2.scrape.center/login

#### 密码admin/admin

#### 1.具体需求

1）实现requests的模拟登录与状态保持

2）本次抓取结果仅能使用`re`正则表达式

3）抓取需求同一、即电影名称、封面、时间、类型即描述

4）结果保存在csv文件中

#### 2.抓包分析

#### 3.实现思路总结

#### 4.代码

```python
import requests
import re
from urllib.parse import urljoin


class MoviesSpiders(object):

    def __init__(self):
        self.session = requests.session()
        self.headers={
            "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }
        self.account = 'admin'
        self.pwd = 'admin'
        self.totalpages=10
        self.url = "https://login2.scrape.center/"

    def login(self, login_url):
        self.session.get(login_url,headers=self.headers,timeout=10)
        data={ 'username': self.account, 'password': self.pwd}
        response = self.session.post(login_url,headers=self.headers,data=data,timeout=10)
        print()
        if "https://login2.scrape.center/"==response.request.url:
            return True
        else:
            return False

    def parse_detail(self, html):
        cover_pattern = re.compile('class="item.*?<img.*?src="(.*?)".*?class="cover">', re.S)
        name_pattern = re.compile('<h2.*?>(.*?)</h2>')
        categories_pattern = re.compile('<button.*?category.*?<span>(.*?)</span>.*?</button>', re.S)
        published_at_pattern = re.compile('(\d{4}-\d{2}-\d{2})\s?上映')
        drama_pattern = re.compile('<div.*?drama.*?>.*?<p.*?>(.*?)</p>', re.S)
        score_pattern = re.compile('<p.*?score.*?>(.*?)</p>', re.S)
        cover = re.search(cover_pattern, html).group(1).strip() if re.search(cover_pattern, html) else None
        name = re.search(name_pattern, html).group(1).strip() if re.search(name_pattern, html) else None
        categories = re.findall(categories_pattern, html) if re.findall(categories_pattern, html) else []
        published_at = re.search(published_at_pattern, html).group(1) if re.search(published_at_pattern, html) else None
        drama = re.search(drama_pattern, html).group(1).strip() if re.search(drama_pattern, html) else None
        score = float(re.search(score_pattern, html).group(1).strip()) if re.search(score_pattern, html) else None
        return {
            'cover': cover,
            'name': name,
            'categories': '/'.join(categories),
            'published_at': published_at,
            'drama': drama,
            'score': score
        }

    def parse_index(self, html):
        pattern = re.compile('<a.*?href="(.*?)".*?class="name">')
        items = re.findall(pattern, html)
        if not items:
            return []
        for item in items:
            detail_url = urljoin(self.url, item)
            yield detail_url

    def scrape_page(self, url):
        response = self.session.get(url, headers=self.headers)
        if response.status_code == 200:
            return response.text

    def scrape_index(self, page):
        index_url = f'{self.url[:-1]}/page/{page}'
        return self.scrape_page(index_url)

    def scrape_detail(self, url):
        return self.scrape_page(url)

    def save_to_csv(self, path, datas, mode='w'):
        '''
        :param path:
        :param data: [{},{},{},...]
        :param mode:
        :return:
        '''
        import csv
        with open(path, mode, encoding='utf-8', newline="") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=['name', 'categories', 'published_at','score','cover','drama'])
            writer.writeheader()
            for data in datas:
            	writer.writerow(data)

    def main(self):
        login_url = 'https://login2.scrape.center/login'
        flag = self.login(login_url)
        if flag:
            result = []
            for page in range(1, self.totalpages + 1):
                index_html = self.scrape_index(page)
                detail_urls = self.parse_index(index_html)
                for detail_url in detail_urls:
                    detail_html = self.scrape_detail(detail_url)
                    data = self.parse_detail(detail_html)
                    print(data)
                    result.append(data)
            self.save_to_csv('./datas/doubanmovie.csv',result)
        else:
            print('模拟登陆失败，请重试！')


if __name__ == '__main__':
    ws = MoviesSpiders()
    ws.main()
```

#### 三、基于AJAX抓取网站

#### 地址：https://spa1.scrape.center/

#### 1.需求分析：

获取以ajax请求的电影数据，同样获取电影名称；封面；时间；类型；描述；

保存为txt文本即可

#### 2.抓包分析

列表页面数据缺少`描述`w文本，因此必须去详情页才能抓到想要的数据，

进一步抓包发现，可以从接口:'https://spa1.scrape.center/api/movie/num/'下直接访问详情数据，具体而言，num从1-100即可。

#### 3.代码实现

```python
import requests
import json

class MovieSpider(object):
    def __init__(self):
        self.url='https://spa1.scrape.center/api/movie/'
        self.session = requests.session()
        self.limit = 10
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }

    def get_details(self, num):
        response = self.session.get(self.url + f"{num}/", headers=self.headers, timeout=10).json()
        movie = {
            'cover': response['cover'],
            'name': response['name'] + '-' + response['alias'],
            'categories': '/'.join(response['categories']),
            'published_at': response['published_at'],
            'drama': response['drama'],
            'score': response['score']
        }
        print(movie)
        return movie

    def save_txt(self, path, data):
        with open(path, 'w', encoding='utf8') as f:
            f.write(json.dumps(data))

    def main(self):
        movies = [self.get_details(i+1) for i in range(100)]
        self.save_txt('./datas/movies.txt', {'movies': movies})



if __name__ == '__main__':
    ms = MovieSpider()
    ms.main()
```

