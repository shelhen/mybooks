# 07.信号与核心Api

## 一、组件

`Scrapy`组件是使用`scrapy.utils.misc.create_instance()`函数创建的任何类，听起来有些复杂，其实大部分内置组件我们都已经使用过，这些组件包括：

```python
SCHEDULER
SCHEDULER_MEMORY_QUEUE
SCHEDULER_PRIORITY_QUEUE
ITEM_PIPELINES
SPIDER_MIDDLEWARES
DOWNLOADER_MIDDLEWARES
SCHEDULER_DISK_QUEUE
FEED_EXPORTERS
FEED_STORAGES
EXTENSIONS
DUPEFILTER_CLASS
DNS_RESOLVER
DOWNLOAD_HANDLERS
DOWNLOADER_CLIENTCONTEXTFACTORY
```

`scrapy`同样也支持开发第三方自定义组件，通常可以使用设置进行配置以修改其行为，这里补充重要的一点，由于组件往往需要运行于不同的环境直线，因此在声明组件时建议在类对象中写清楚类和接口的详细功能，并在实例化时在`__init__`函数中对系统环境检测。

比如，对于下载器中间件、扩展、项目管道和爬虫中间件，如果不满足环境要求，应该抛出`scrapy.exceptions.NotConfigured`，将问题的描述作为参数传递给异常，以便在日志中打印出来，供用户查看。对于其他组件，也应该输出合适其他异常，如`RuntimeError`用以说明`Scrapy`版本不匹配，`ValueError`用以说明某些值设置的有问题。

```python
import scrapy
from packaging.version import parse as parse_version

class MyComponent:
    def __init__(self):
        if parse_version(scrapy.__version__) < parse_version("2.7"):
            raise RuntimeError(
                f"{MyComponent.__qualname__} requires Scrapy 2.7 or "
                f"later, which allow defining the process_spider_output "
                f"method of spider middlewares as an asynchronous "
                f"generator."
            )
```

> 使用`scrapy.__version__`获取当前`scrapy`的版本号。

## 二、设置类

`Scrapy`设置(`settings`)提供了定制`Scrapy`组件的方法，可以控制包括核心(`core`)，插件(`extension`)，`pipeline`及`spider`组件。设置提供了一个提供键值映射的全局明明空间，代码可以从该明明空间中提取配置值，这些值可以通过不同的机制填充。

### 1.指定设置项

当使用`scrapy`时，必须通过环境变量`SCRAPY_SETTINGS_MMODULE`指定当前的设置文件，`SCRAPY_SETTINGS_MMODULE`因该使用`python`路径的语法，例如`myproject.SETTINGS`。同时注意，`settings`模块应该位于`Python`导入搜索路径。

### 2.设置方法

`scrapy`提供了多种机制配置设置项，包括全局默认设置、设置文件项、命令行选项、爬虫类属性设置、`meta`参数设置，其中每种机制都具有不同的优先级，命令行选项设置优先级最高，其次是`meta`参数设置，其对应每个请求最高的优先级，再其次是爬虫类属性，其后是设置文件项目，最后才是全局默认设置，后者的优先级低主要是因为前者定义会覆盖后者的定义情况。

> 所有设置项的设置值都必须是`picklable`。

#### （1）命令行设置

```shell
scrapy crawl myspider -s LOG_FILE=scrapy.log
```

注意有的命令行参数中并没有增加参数，这并不代表没有设置，而是使用了配置项默认设置。

#### （2）请求级meta设置

```python
Request(meta={'dont_merge_cookies':True})
```

#### （3）爬虫级设置

- 类属性设置信息

```python
import scrapy


class MySpider(scrapy.Spider):
    name = "myspider"

    custom_settings = {
        "SOME_SETTING": "some value",
    }
```

- 类方法

基于`update_settings`配置设置。

```python
import scrapy


class MySpider(scrapy.Spider):
    name = "myspider"

    @classmethod
    def update_settings(cls, settings):
        super().update_settings(settings)
        settings.set("SOME_SETTING", "some value", priority="spider")
```

基于` from_crawler()`基于爬虫参数配置设置。

```python
import scrapy


class MySpider(scrapy.Spider):
    name = "myspider"

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        if "some_argument" in kwargs:
            spider.settings.set(
                "SOME_SETTING", kwargs["some_argument"], priority="spider"
            )
        return spider
```

#### （4）设置文件项

```python
BOT_NAME = "spider_tansoole_category"
# Disable cookies (enabled by default)
COOKIES_ENABLED = True
```

#### （5）附加组件设置

[附加组件](https://doc.scrapy.org/en/latest/topics/addons.html#topics-addons)可以修改设置。

#### （6）全局默认设置

参考全局默认设置，可参数文件`scrapy.settings.default_settings`模块，其中的详细参数记录在[内置设置参考](https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref)部分。

### 3.文件项设置的导包路径

当设置引用要由`Scrapy`导入的可调用对象（如类或函数）时，有两种不同的方式可以指定该对象：

```python
from mybot.pipelines.validate import ValidateMyItem

ITEM_PIPELINES = {
    # passing the classname...
    ValidateMyItem: 300,
    # ...equals passing the class path
    "mybot.pipelines.validate.ValidateMyItem": 300,
}
```

### 4.在爬虫中使用设置项

在爬虫项目中，可以通过`self.settings`进行设置：

```python
class MySpider(scrapy.Spider):
    name = "myspider"
    start_urls = ["http://example.com"]

    def parse(self, response):
        print(f"Existing settings: {self.settings.attributes.keys()}")
```

> 如上设置只能在`spider`初始化后才能读取，如果想要在`__init__`中使用设置项，则需要重写`from_crawler`方法。

```python
class MyExtension:
    def __init__(self, log_is_enabled=False):
        if log_is_enabled:
            print("log is enabled!")

    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(settings.getbool("LOG_ENABLED"))
```

### 5.常用的内置设置

| 名称                     | 说明                                                         | 默认值      |
| ------------------------ | ------------------------------------------------------------ | ----------- |
| `BOT_NAME`               | 当使用 `startproject` 命令创建项目时其也被自动赋值。         | `scrapybot` |
| `CONCURRENT_ITEMS`       | `Item Pipeline`同时处理(每个`response`的)`item`的最大值。    | `100`       |
| `CONCURRENT_REQUESTS`    | `Scrapy downloader `并发请求的最大值。                       | `16`        |
| `DEFAULT_REQUEST_HEADERS | `HTTP Request`使用的默认`header`。                           | `...`       |
| `DEPTH_LIMIT`            | 爬取网站最大允许的深度(depth)值，如果为0，则没有限制。       | `0`         |
| `DOWNLOAD_DELAY`         | 下载器在下载同一个网站下一个页面前需要等待的时间（支持小数）。该选项可以用来限制爬取速度， 减轻服务器压力，`0.25`表示250毫秒。默认情况下，Scrapy在两个请求间不等待一个固定的值， 而是使用0.5到1.5之间的一个随机值 * DOWNLOAD_DELAY 的结果作为等待间隔。 | ` 0`        |
| `DOWNLOAD_TIMEOUT`       | 下载器最大等待时间。                                         | ` 180`      |
| `ITEM_PIPELINES`         | 保存项目中启用的pipeline及其顺序的字典，值(value)习惯设置在0-1000范围内，值越小优先级越高。 | ` {}`       |
| `LOG_ENABLED`            | 是否启用logging。                                            | `True`      |
| `LOG_ENCODING`           | logging使用的编码。                                          | `utf-8`     |
| `LOG_LEVEL`              | log的最低级别。可选的级别有: CRITICAL、 ERROR、WARNING、INFO、DEBUG 。 | `DEBUG`     |
| `LOG_FILE`               | log日志文件的保存路径，如果设置该参数，日志信息将写入文件，终端将不再显示，且受到LOG_LEVEL日志等级的限制。 |             |
| `USER_AGENT`             | 爬取的默认User-Agent，除非被覆盖。                           | `...`       |
| `PROXIES`                | 代理设置                                                     | ` [...]`    |
| `COOKIES_ENABLED`        | 是否启用`cookiemiddleware`。                                 | `True`      |
| `COOKIES_DEBUG`          | 表示日志中是否显示cookie的传递过程。                         | `False`     |

设置中包含太多的内容，因此文档将很多可能使用到的配置项和其他项目写在了一起，并在此处留下[系统参考文档](https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref)/[中文版文档](https://www.jianshu.com/p/df9c0d1e9087)。

```python
# proxies 使用案例
PROXIES=[
  {'ip_port': '120.198.243.22:80', 'password': ''},
  {'ip_port': '111.8.60.9:8123', 'password': ''},
  {'ip_port': '101.71.27.120:80', 'password': ''},
  {'ip_port': '122.96.59.104:80', 'password': ''},
  {'ip_port': '122.224.249.122:8088', 'password':''},
]
```

### 6.设置对象

#### （1）设置基类

一个类似于`python`字典对象的实例，该实例可以存储键值对，同时设置器优先级并可以设置是否可变（可将其值标记为不可变）。

```python
class scrapy.settings.BaseSettings(values, priority='project'):
    pass
```

当直接创建一个`BaseSettings` 实例时，可以通过 `values` 参数传递键值对，这些键值对的优先级将由 `priority` 参数决定，除非 `values` 已经是一个 `BaseSettings` 实例，这时，现有的优先级将被保留。

如果 `priority` 参数是一个整数，那么这个整数将被直接用作优先级，如果 `priority` 参数是一个字符串，那么这个字符串将被用作在 `SETTINGS_PRIORITIES`（一个预定义的优先级映射）中查找的键。

```python
from scrapy.settings import BaseSettings

# 假设 SETTINGS_PRIORITIES 定义如下
SETTINGS_PRIORITIES = {
    'default': 0,
    'project': 10,
    'cmdline': 20
}

# 创建一个 BaseSettings 实例，并传递键值对
settings = BaseSettings({'download_delay': 2}, priority='project')
# 使用 set() 方法更新设置
settings.set('download_delay', 5, priority='cmdline')  # 更新下载延迟并设置更高的优先级
# 使用方括号表示法获取设置
print(settings['download_delay'])  # 输出: 5，因为 'cmdline' 优先级高于 'project'
# 假设再次更新设置，但使用较低的优先级
settings.set('download_delay', 3, priority='default')
# 使用 get() 方法获取设置
print(settings.get('download_delay'))  # 输出: 5，因为 'cmdline' 优先级仍然是最高的
```

**内置方法**

| 方法                                     | 说明                                                |
| ---------------------------------------- | --------------------------------------------------- |
| `copy()`                                 | 返回当前设置实例的深拷贝。                          |
| `copy_to_dict()`                         | 将当前设置实例转换为字典并返回。                    |
| `freeze()`                               | 禁止对当前设置实例进行进一步更改。                  |
| `frozencopy()`                           | 返回当前设置的不可变副本。                          |
| `get(name, default=None)`                | 获取设置值，保持其原始类型。                        |
| `getbool(name, default=False)`           | 获取设置值，并将其转换为布尔值。                    |
| `getdict(name, default=None)`            | 获取设置值，并将其转换为字典。                      |
| `getdictorlist(name, default=None)`      | 获取设置值，并将其转换为字典或列表。                |
| `getfloat(name, default=0.0)`            | 获取设置值，并将其转换为浮点数。                    |
| `getint(name, default=0)`                | 获取设置值，并将其转换为整数。                      |
| `getlist(name, default=None)`            | 获取设置值，并将其转换为列表。                      |
| `getpriority(name)`                      | 返回设置的当前数值优先级，如果不存在则返回 `None`。 |
| `getwithbase(name)`                      | 获取字典类设置的组合及其 `_BASE` 对应项。           |
| `maxpriority()`                          | 返回所有设置中的最高优先级数值。                    |
| `pop(k, d)`                              | 移除指定的键并返回对应的值。                        |
| `set(name, value, priority=‘project’)`   | 存储键/值属性及其优先级。                           |
| `setdefault(k[, d])`                     | 获取一个键的值，如果不存在，则设置一个键的默认值。  |
| `setmodule(module, priority=‘project’) ` | 从一个模块中存储设置，并给这些设置指定一个优先级。  |
| `update(values, priority=‘project’)`     | 存储键值对，并给这些键值对指定一个优先级。          |

```python
# setmodule 使用案例

from scrapy.settings import BaseSettings
import myproject.settings as my_settings

settings = BaseSettings()

# 使用 setmodule 方法将 myproject.settings 中的设置存储到 BaseSettings 实例中
settings.setmodule(my_settings, priority='project')

# 现在我们可以获取 'DOWNLOAD_DELAY' 的值
print(settings.get('download_delay'))  # 输出: 5

# update 案例
# 使用 update 方法存储新的设置
settings.update({'download_delay': 10,....}, priority='cmdline')
```

> `update` 方法中的 `values` 如果是字符串，它将被假设为 JSON 编码的，并使用 `json.loads()` 解析为字典。如果 `values` 是 `BaseSettings` 实例，则每个键的优先级将被使用，并忽略 `priority` 参数。

#### （2）设置子类

此对象存储用于配置内部组件的 配置参数，并可在项目中进行进一步自定义。

```python
class scrapy.settings.Settings
```

它是一个直接的子类，支持`BaseSettings`的所有方法。在实例化此类后，新对象将已填充内置设置引用中描述的全局默认设置。

#### （3）设置接口

- 设置优先级配置项

```python
scrapy.settings.SETTINGS_PRIORITIES
```

该对象是一个字典映射，用于配置`Scrapy`中使用的默认设置优先级的键名和优先级。每个项目都定义了一个设置入口点，为其提供了一个用于标识的代码名和一个整数优先级。在Settings类中设置和检索值时，优先级越高，优先级越低。

```python
SETTINGS_PRIORITIES = {
    "default": 0,
    "command": 10,
    "addon": 15,
    "project": 20,
    "spider": 30,
    "cmdline": 40,
}
```

- 获取优先级接口

在`SETTINGS_PRIORITIES`字典中查找给定的字符串优先级并返回其数值，或直接返回给定的数值优先级。

```python
from scrapy.settings import get_settings_priority

def get_settings_priority(priority)->int:
    pass
```



## 三、`scrapy`调度器

调度器组件负责接收并存储来自引擎（从其他组件处）接收到的请求，并在请求时将其反馈给引擎，请求的可能来源是：

- 爬虫类：`start_requests`或`yield`或`return`产生的回调请求；
- 爬虫中间件：` process_spider_output`或`process_spider_exception`方法；
- 下载器中间件：`process_request`、`process_response`或`process_exception`方法。

> 调度器根据`next_request`方法决定返回其存储的请求的顺序。

### 1.调度器基类

此类中定义的方法构成了调度器与引擎之交互的最小接口。

```python
class scrapy.core.scheduler.BaseScheduler
```

#### （1）close方法

```python
def close(reason: str):
    """当引擎关闭spider时调用，它接收爬虫关闭的原因作为参数，执行清理代码。"""
    pass
```

#### （2）open方法

```python
def open(self, spider: Spider):
    """当引擎打开spider时调用，他接受spider作为参数，执行初始化代码。"""
    pass
```

#### （3）enqueue_request方法

```python
@abstractmethod
def enqueue_request(self, request: Request) -> bool:
    """处理引擎收到的请求。"""
    raise NotImplementedError()
```

如果请求存储正确，则返回`True`，否则返回`False`，如果返回`False`，引擎将发出`request_dropped`信号，并且不会在以后再次尝试调度该请求，比如请求被`dupefilter`拦截时，调度器就会返回`False`。

#### （4）next_request方法

```python
def next_request(self) -> Optional[Request]:
    """返回下一个要处理的请求，或返回None，表示目前没有可以视为就绪的请求。"""
    pass
```

如果返回`None`意味着，调度器没有向下载器传递任何请求，引擎将继续调用`next_request`，直到`has_pending_requests`为`False`。

#### （5）has_pending_requests方法

```python
 def has_pending_requests(self) -> bool:
    """如果调度程序有排队请求，则为True，否则为False"""
    return len(self) > 0
```

#### （6）from_crawler方法

```python
@classmethod
def from_crawler(cls, crawler: Crawler) -> Self:
    """接收当前Crawler对象作为参数的工厂方法。"""
    return cls()
```

### 2.默认 Scheduler程序

默认的调度器，其内部实现了请求的重复过滤，此调度器将请求根据优先级存储在多个队列（这些队列可以存储在内存或硬盘中）中，此调度器优先使用基于磁盘的优先级队列，如果发生序列化错误或磁盘队列不存在，则回退到基于内存的队列。

```python
class scrapy.core.scheduler.Scheduler(
    dupefilter: BaseDupeFilter,  # `scrapy.dupefilters.BaseDupeFilter`类，或实现BaseDupeFilter接口的任何类，负责检查和过滤重复请求的对象。其值由 DUPEFILTER_CLASS 设置
    jobdir: Optional[str] = None,  # 存储请求的磁盘路径，默认为  JOBDIR 路径。
    dqclass=None,  # 控制请求持久化队列的类，默认由 SCHEDULER_DISK_QUEUE 配置。
    mqclass=None,  # 控制请求内存队列的类，默认由 SCHEDULER_MEMORY_QUEUE 配置。
    logunser: bool = False,  # 指示是否应记录不可序列化的请求。默认使用SCHEDULER_DEBUG设置的值。
    stats: Optional[StatsCollector] = None,  # scrapy.statscollectors.StatsCollector（统计收集器）类或或实现 StatsCollector 接口的任何类，用于记录请求调度过程的统计信息。默认由STATS_CLASS设置。
    pqclass=None,  # 用作请求优先级队列的类。默认由 SCHEDULER_PRIORITY_QUEUE设置。
    crawler: Optional[Crawler] = None  # scrapy.crawler.crawler–当前爬网相对应的crawl对象。
):
```

内置方法包括：

#### （1）\_\_len\_\_魔术方法

```python
def __len__(self) -> int:
    """
    返回已排队请求的总数
    """
    return len(self.dqs) + len(self.mqs) if self.dqs is not None else len(self.mqs)
```

#### （2）close方法

```python
def close(self, reason: str) -> Optional[Deferred]:
    """
    1.如果定义了磁盘存储，则将请求队列挂起到磁盘
    2.返回 dupefilter’s close 方法的结果
    """
    ...
```

#### （3）open方法

```python
 def open(self, spider: Spider) -> Optional[Deferred]:
    """
    (1) 初始化内存队列
    (2) 如果`jobdir`属性是一个有效的目录，则初始化磁盘队列
    (3) 返回 dupefilter’s open 方法的结果
    """
    ...
```

#### （4）enqueue_request方法

```python
 def enqueue_request(self, request: Request) -> bool:
    """  除非收到的请求被Dupefilter过滤掉，否则尝试将其推入磁盘队列，然后退回到将其推入内存队列。"""
    pass
```

增加适当的统计数据如：`scheduler/dequeued`、`scheduler/dequeued/disk`、`scheduler/dequeued/memory`。

#### （5）has_pending_requests方法

```python
def has_pending_requests(self) -> bool:
    """如果调度程序有排队请求，则为True，否则为False"""
    return len(self) > 0
```

#### （6）next_request方法

```python
def next_request(self) -> Optional[Request]:
    """从内存队列返回一个Request对象，如果内存队列为空，则返回磁盘队列。如果没有更多排队请求，则返回None。"""
    pass
```

#### （7）from_crawler方法

```python
@classmethod
def from_crawler(cls, crawler: Crawler) -> Self:
    """接收当前Crawler对象作为参数的工厂方法。"""
    return cls()
```

## 四、核心Api

核心`api`的使用主要面向重写扩展`extensions`和中间件`middlewares`。

### 1.Crawler API

核心`api`中最重要的就是`Crawler`实例对象，该对象提供对所有Scrapy核心组件的访问， 也是扩展访问Scrapy核心组件和挂载功能到Scrapy的唯一途径。该对象通过类方法`from_crawler`创建，并将其传递给扩展(`extensions`)。



Extension Manager负责加载和跟踪已经安装的扩展， 它通过 [`EXTENSIONS`](https://scrapy-chs.readthedocs.io/zh-cn/latest/topics/settings.html#std:setting-EXTENSIONS) 配置，包含一个所有可用扩展的字典， 字典的顺序跟你在 [configure the downloader middlewares](https://scrapy-chs.readthedocs.io/zh-cn/latest/topics/downloader-middleware.html#topics-downloader-middleware-setting) 配置的顺序一致。

#### （1）Crawler类

`Crawler`对象必须传入一个` spiders`类和一个 `settings`类 进行实例化，该对象中包含了`scrapy`核心组件的访问接口。

```python
class scrapy.crawler.Crawler(spidercls, settings):
    """
    该类提供的接口
    settings: crawler的配置管理器,扩展(extensions)和中间件(middlewares)使用它用来访问Scrapy的配置。
    signals: crawler的信号管理器，扩展和中间件使用它将自己的功能挂载到Scrapy。
    stats：crawler的统计信息收集器，扩展和中间件使用它记录操作的统计信息，或者访问由其他扩展收集的统计信息。
    extensions：扩展管理器，跟踪所有开启的扩展。大多数扩展不需要访问该属性。
    engine：执行引擎，协调crawler的核心逻辑，包括调度，下载和spider， 某些扩展可能需要访问Scrapy的引擎属性，以修改检查(modify inspect)或修改下载器和调度器的行为， 这是该API的高级使用，但还不稳定。
    spider：正在爬取的spider，该spider类的实例由创建crawler时所提供。
    function crawl(*args, **kwargs):根据给定的 args , kwargs 的参数来初始化spider类，启动执行引擎，启动crawler。返回一个延迟deferred对象，当爬取结束时触发它。
    """
    def __init__(
        self,
        spidercls: Type[Spider],
        settings: Union[None, Dict[str, Any], Settings] = None,
        init_reactor: bool = False,
    ):
        # ...
        self.spidercls: Type[Spider] = spidercls
        self.settings: Settings = settings.copy()
        self.spidercls.update_settings(self.settings)

        self.addons: AddonManager = AddonManager(self)
        self.signals: SignalManager = SignalManager(self)
        
        self._init_reactor: bool = init_reactor
        self.crawling: bool = False
        self._started: bool = False
		self._update_root_log_handler()
        self.extensions: Optional[ExtensionManager] = None
        self.stats: Optional[StatsCollector] = None
        self.logformatter: Optional[LogFormatter] = None
        self.request_fingerprinter: Optional[RequestFingerprinter] = None
        self.spider: Optional[Spider] = None
        self.engine: Optional[ExecutionEngine] = None
    
    @inlineCallbacks
    def crawl(self, *args: Any, **kwargs: Any) -> Generator[Deferred, Any, None]:
        """执行爬取"""
        if self.crawling:
            raise RuntimeError("Crawling already taking place")
        if self._started:
            warnings.warn(
                "Running Crawler.crawl() more than once is deprecated.",
                ScrapyDeprecationWarning,
                stacklevel=2,
            )
        self.crawling = self._started = True

        try:
             # return self.spidercls.from_crawler(self, *args, **kwargs)
            self.spider = self._create_spider(*args, **kwargs)
            self._apply_settings()
            self._update_root_log_handler()
            # 当前的引擎
            self.engine = self._create_engine()  # return ExecutionEngine(self, lambda _: self.stop())
            start_requests = iter(self.spider.start_requests())
            yield self.engine.open_spider(self.spider, start_requests)
            yield maybeDeferred(self.engine.start)
        except Exception:
            self.crawling = False
            if self.engine is not None:
                yield self.engine.close()
            raise
    
    def _apply_settings(self) -> None:
        """从设置中读取信息，用于配置加载到相应的addons、stats、signals等"""
        pass
    
    def _update_root_log_handler(self) -> None:
        """基于setting更新root handler信息"""
        ...
    
    @inlineCallbacks
    def stop(self) -> Generator[Deferred, Any, None]:
        """优雅的停止crawled，返回一个延迟，当爬虫停止时触发该延迟 """
        if self.crawling:
            self.crawling = False
            assert self.engine
            yield maybeDeferred(self.engine.stop)
```

#### （2）CrawlerRunner类

一个方便的辅助类，用于跟踪、管理和运行已经设置好的爬虫程序，除非想要从代码中运行`spider`，否则应该用不要这个类。

```python
class scrapy.crawler.CrawlerRunner(settings= None):
    """
    CrawlerRunner对象必须用Settings对象实例化，该类提供的接口：
    function crawl(crawler_or_spidercls, *args, **kwargs): 使用提供的参数运行爬虫,它将调用给定的Crawler的crawl()方法，同时跟踪它，以便以后可以停止它。
如果crawler_or_spidercls不是crawler实例，则此方法将尝试使用此参数作为给定的spider类来创建一个。返回一个延迟，当爬行完成时触发。
	function join():返回一个延迟，当所有托管爬虫完成执行时触发。
	function stop():同时停止所有正在进行的爬虫任务，返回一个延迟，当它们都结束时被触发。
	crawlers: 由crawl()启动并由该类管理的一组爬虫。
    function create_crawler(crawler_or_spidercls)->Crawler:
    - 如果crawler_or_spidercls是crawler，则按原样返回。
    - 如果crawler_or_spidercls是Spider子类，则为其构造一个新的crawler。
    - 如果crawler_or_spidercls是一个字符串，则此函数在Scrapy项目中查找具有此名称的蜘蛛（使用蜘蛛加载器），然后为其创建一个crawler实例。
    """
    pass
```

#### （3）CrawlerProcess类

一个可以在一个进程中同时运行多个爬虫的类。此类扩展了CrawlerRunner，增加了对启动反应堆和处理停堆信号的支持，如键盘中断命令Ctrl-C。它还配置了顶级日志记录。

```python
class scrapy.crawler.CrawlerProcess(CrawlerRunner):
    """
    CrawlerProcess对象必须用Settings对象实例化，并且比CrawlerRunner更适合从脚本中启动 该类提供的接口：
    function stop()/join()/create_crawler()/crawlers/crawl:...
    
    start(stop_after_crawl=True, install_signal_handlers=True):此方法启动反应器，将其池大小调整为REATOR_THREADBOOL_MAXSIZE，并基于DNSCACHE_ABLED和DNSCACHE_size安装DNS缓存。如果stop_after_crawl为True，则反应器将在所有爬虫完成后使用join（）停止。
    """
    def __init__(self, install_root_handler,...):
        # 参数install_root_handler：是否安装根日志处理程序（默认值：True）
        pass
```

### 2.SpiderLoader API

此类负责检索和处理整个项目中定义的蜘蛛类，可以在设置中指定`spider_LOADER_CLASS`参数指定项目的`SpiderLoader`。如果想要自定义`SpiderLoader`，必须要完全实现` scrapy.interfaces.ISpiderLoader`接口。

```python
@implementer(ISpiderLoader)
class scrapy.spiderloader.SpiderLoader:
    """该类的关键接口：
    from_settings(settings)：它使用当前项目设置调用，并加载在SPIDER_modules设置的模块中递归找到的蜘蛛
    load(spider_name)：获取具有给定名称的Spider类，如果找不到，将引发KeyError
    list()：获取项目中可用蜘蛛的名称列表。
    find_by_request(request)：列出可以处理给定请求的蜘蛛名称。将尝试将请求的url与蜘蛛的域名进行匹配。
    """
    def __init__(self, settings: BaseSettings):
        self.spider_modules: List[str] = settings.getlist("SPIDER_MODULES")
        self.warn_only: bool = settings.getbool("SPIDER_LOADER_WARN_ONLY")
        self._spiders: Dict[str, Type[Spider]] = {}
        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)
        self._load_all_spiders()
        # ...
    
```

还有更多核心`api`包括信号、设置和统计信息，都列在其对应的部分以便于学习。

## 五、信号（Signals）

`Scrapy`使用信号来通知事情发生。可以在Scrapy项目使用扩展 [extension](https://scrapy-chs.readthedocs.io/zh-cn/latest/topics/extensions.html#topics-extensions)中捕捉一些信号来完成额外的工作或添加额外的功能。信号分发机制(`singal dispatching mechanism`)仅仅处理处理器(`handler`)接收的参数，因此即使提供了多个参数，捕获程序并不一定都进行处理。

### 1.信号`api`

```python
class scrapy.signalmanager.SignalManager(sender):
    """信号管理基类,其提供了如下官方推荐使用的接口：
    connect(receiver,signal,**kwargs):将接收器功能连接到信号(信号可以是任何对象,receiver应该是一个函数对象)。
    disconnect(receiver,signal,**kwargs): 从信号上断开接收器功能。与connect参数相同，但效果相反。
    disconnect_all((signal):断开所有接收器与给定信号的连接。
    send_catch_log(signal, **kwargs): 发送信号、捕获异常并通过日志记录，其关键字参数将被传递给connect链接的receiver。
    send_catch_log_deferred(signal, **kwargs):与send_catch_log类似，但支持从信号处理程序返回Deferred对象。一旦所有信号处理程序Deferred都被触发，它就会被触发。
    """
    pass
```

### 2.爬虫信号案例

```python
from scrapy import signals
from scrapy import Spider


class DmozSpider(Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/",
    ]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)
        # 调用 signals.connect 链接方法，向 接收器函数为spider.spider_closed 发送信号，该函数用于接收信号实现某些功能，信号为 signal=signals.spider_closed，意味着当触发signals.spider_closed时调用 spider.spider_closed。
        # signals.spider_closed是一个内置信号
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    def spider_closed(self, spider):
        spider.logger.info("Spider closed: %s", spider.name)
```

### 3.异步信号

信号支持从其处理程序返回异步信号，以避免阻塞，在`scrapy`中使用`async`定义的接收器将会自动对应异步信号。

```python
import scrapy


class SignalSpider(scrapy.Spider):
    name = "signals"
    start_urls = ["https://quotes.toscrape.com/page/1/"]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)
        return spider

    async def item_scraped(self, item):
        # Send the scraped item to the server
        response = await treq.post(
            "http://example.com/post",
            json.dumps(item).encode("ascii"),
            headers={b"Content-Type": [b"application/json"]},
        )

        return response

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }
```

### 4.内置信号

内置信号是存储在`scrapy.signals`模块中的一系列函数。

#### （1）`engine`信号

- `engine_started()`：当Scrapy引擎启动时发送信号。
- `engine_stopped()`：当Scrapy引擎关闭爬行时发送信号。

> 根据蜘蛛的启动方式，此信号会在蜘蛛打开信号后发出，这些信号均支持从其处理程序返回异步信号。

#### （2）`item`信号

- `item_scraped(item, response, spider)`：在`item`通过所有`pipline`（且未被删除）时发送该信号。
- `item_dropped(item, response, exception, spider)`：当`pipline`触发`DropItem`异常，将`item`删除时发送该信号。
- `item_error(item, response, spider, failure)`：当`pipline`触发其他异常时，发送该信号。

> 由于`item`自然支持并行处理，因此其异步信号依赖`DeferredList`而不是`Deferred`。

#### （3）`spider`信号

- `spider_closed(spider, reason)`：在`spider`被关闭后发送，这用于释放`spider_opened`上保留的资源，`reason`一般被设置为`finished`（爬虫正常关闭）、`cancelled`（爬虫被调用`close_spider`函数取消）和`shutdown`（强制关闭）。

- `spider_opened(spider)`：`spider`被开启时发送该信号。

- `spider_idle(spider)`：当蜘蛛空闲时发送（此信号不支持异步），这往往意味着`spider`可能处于如下状态：

  - 等待下载新请求
  - 等待请求队列中的请求
  - 等待管道处理`item`

  如果如上处理程序都已经完成，空闲状态仍然存在，此时引擎将尝试关闭蜘蛛。

  如果不希望`spider`被立刻关闭，可以手动抛出`DontCloseSpider`异常，或者抛出`CloseSpider`并提供自定义的`reason`。

  该信号时放置自定义代码的理想场所，一般在这里实现评估最终的蜘蛛结果并相应的更新爬虫关闭的原因。

> 注意：在`spider_idle  handler`中添加一些请求有时候并不能保证爬虫不被关闭，因为有些请求可能因为重复被过滤。

- `spider_error(failure, response, spider)`：当`spider`的`callback`函数引发异常时发送信号（此信号不支持异步），参数`failure`传递引发异常的失败对象。

- `feed_slot_closed(slot)`：当`FeedSlot`关闭时发送信号，参数`solt`是`FeedSlot`对象。
- `feed_exporter_closed()`：当`feed`导出完毕时发送信号。

> 以上信号，如没在括号中声明，均支持异步信号。

#### （4）`Request`信号

- `request_scheduled(request, spider)`:当要求引擎存储请求时发送信号，该信号发生在请求被发出之前且被存入`schedule`之前（该信号不支持异步）。
  - 该信号可以通过抛出一个`IgnoreRequest`异常删除一个请求。

- `request_dropped(request, spider)`：当引擎计划拒绝存储某请求时发出信号（该信号不支持异步）。

- `request_reached_downloader(request, spider)`：当请求到达下载器时发送信号（该信号不支持异步）。

-  `request_left_downloader(request, spider)`：当请求离开下载器时发出信号，即使请求失败（该信号不支持异步）。

- `bytes_received(data, request, spider)`：当收到特定字节的响应时发出信号，对于同一个请求，此信号可能会被多次触发，因为部分数据发生了重复（该信号不支持异步）————一个响应可能由多个响应组装而成，参数`data`应该传入一个`bytes`类的数据。
  - 此信号的处理程序可以通过抛出一个`StopDownload`异常来停止正在进行的响应下载，阅读[停止下载响应](https://docs.scrapy.org/en/latest/topics/request-response.html#topics-stop-response-download)了解更多。
- `headers_received(headers, body_length, request, spider)`：在下载相应之前，如果给定请求的响应标头可用时，发出信号（该信号不支持异步）。
  - 此信号的处理程序可以通过抛出一个`StopDownload`异常来停止正在进行的响应下载，阅读[停止下载响应](https://docs.scrapy.org/en/latest/topics/request-response.html#topics-stop-response-download)了解更多。

#### （5）`Response`信号

- `response_received(response, request, spider)`：当引擎从下载器收到新的响应时发送信号（该信号不支持异步）。
- `response_downloaded(response, request, spider)`：下载器在下载`HTTPResponse`后立即发送。

> 如果 下载器中间件 修改了`Response`对象，并设置了特定的请求属性，这里的接收到的请求可能不包含哪些设置的新参数。

## 六、基本常见异常（Exceptions）

### 1.CloseSpider

此异常可以在`spider`的回调函数`parse`中引发，请求关闭爬虫，支持参数`reason`。

```python
scrapy.exceptions.CloseSpider(reason='cancelled')
```

用例：

```python
def parse_page(self, response):
    if "Bandwidth exceeded" in response.body:
        raise CloseSpider("bandwidth_exceeded")
```

### 2.DontCloseSpider

可以在`spider_idle`信号中抛出，用于防止`spider`被关闭。

```python
scrapy.exceptions.DontCloseSpider
```

### 3.DropItem

`Item Pipline`类必须引发此异常才能停止处理项目。

```python
scrapy.exceptions.DropItem
```

### 4.IgnoreRequest

调度器或任何下载器中间件都可以引发此异常，以指示应忽略该请求。

```python
scrapy.exceptions.IgnoreRequest
```

### 5.NotConfigured

```python
scrapy.exceptions.NotConfigured
```

通常由`scrapy`组件引发此异常，表示其将保持禁用状态，包括：

```python
Extensions/Item pipelines/Downloader middlewares/Spider middlewares
```

> 该异常必须在组件的\_\_init\_\_方法中引发。

### 6.NotSupported

引发此异常表示不支持的功能。

```python
scrapy.exceptions.NotSupported
```

### 7.StopDownload

```python
scrapy.exceptions.StopDownload(fail=True)
```

从`bytes_received`或`headers_received`信号处理程序中发出，表示不应为响应下载更多字节。

其中的`fail`参数用于控制处理结果响应的方法：

- 若`fail=True`（默认），调用请求`errback`，此时`response`可以作为异常对象`StopDownload`的属性，被存储在传递给`errback`函数的参数`failure`的属性`value`中，可以在`errback(self, failure)`中直接使用`failure.value.response`获取。
- 若`fail=False`，调用请求的`callback`。

无论以上那种请情形，`response`的正文(`body`)可能被截断，其正文(`body`)包含在引发异常之前接收到的所有字节，包括在引发异常信号处理程序中接收到的字节。另外，将会设置`response.flag`为`"download_stoped"`。

> 了解更多请参阅，`bytes_received`和`headers_received`信号和[停止下载响应](https://www.osgeo.cn/scrapy/topics/request-response.html#topics-stop-response-download)主题。
