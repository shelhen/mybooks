# 05.管道类与数据导出

## 一、管道类

`spider`爬取到的数据最终会被发送到`Item Pipline`，管道类本质上是实现一些通用接口的`python`类，管道类通过执行几个函数来对其进行处理，条目管道的典型用途是清理数据，验证数据(检查条目是否包含某些字段) ，检查重复项（并删除它们）和将爬取的项目存储在数据库中。

### 1.自定义管道类

项目管道`Pipeline`是一个简单的`python`类，其应当实现如下方法：

```python
import openpyxl


class CustomPipeline:
    def __init__(self):
        self.wb = openpyxl.Workbook()
        self.sheet = self.wb.active
        self.sheet.title = 'Top250'
        self.sheet.append(('名称', '评分', '名言'))

    def process_item(self, item, spider):
        """
        - 管道类中必须有的函数
   		- 实现对item数据的处理
   		- 必须return item : 被低优先级的 ItemPipline process_item 处理，直至经过所有的pipline
   		- 或者抛出一个 DropItem异常，该Item 将会被 丢弃，不再被处理。
   		接受item和spider，其中spider表示当前传递item过来的spider
        """
        self.sheet.append((item['title'], item['score'], item['motto']))
        return item

    def open_spider(self, spider):
        # 当spider打开时调用此方法，在爬虫开启的时候仅执行一次
        pass

    def close_spider(self, spider):
        # 当spider关闭时自动执行的函数，在爬虫关闭的时候仅执行一次
        self.wb.save('豆瓣电影数据.xlsx')

    @classmethod
    def from_crawler(cls, crawler):
        """
        该方法可选被设置，允许从 Crawler 对象的配置文件中获取参数，定制Pipline行为。
        """
        # 访问爬虫的设置
        settings = crawler.settings
        # 根据设置做一些初始化
        custom_path = settings.get('CUSTOM_PATH', 'default_path')
        return cls(crawler)
```

> `process_item`方法中`item` 应该是一个项目`Item`对象，且其必须返回项目`Item`对象。

若要激活项管道组件，必须将其类添加到`settings.ITEM_PIPELINES`设置：

```python
ITEM_PIPELINES = {
    # 引擎：值表示距离引擎的远近，越近数据会越先经过
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,  # 权重值越小，越优先执行！
    # 存储 
}
```

**为什么需要开启多个`PIPELINES`？**

- 不同的`pipeline`可以处理不同爬虫的数据，通过`spider.name`属性来区分。
- 不同的`pipeline`能够对多个爬虫进行不同的数据处理的操作，比如一个进行数据清洗，一个进行数据的保存。
- 同一个管道类也可以处理不同爬虫的数据，通过`spider.name`属性来区分

### 2.常见的管道类案例

#### （1）数据验证管道

如下管道实现了调整不包含增值税`item`的价格属性，同时删除了价格为空的条目。

```python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class PricePipeline:
    vat_factor = 1.15

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter.get("price"):
            if adapter.get("price_excludes_vat"):
                adapter["price"] = adapter["price"] * self.vat_factor
            return item
        else:
            raise DropItem(f"Missing price in {item}")
```

#### （2）数据去重

```python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem


class DuplicatesPipeline:
    """这里基于内存去重，同理可以根据缓存去重"""
    def __init__(self):
        self.ids_seen = set()
    
    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter["id"] in self.ids_seen:
            raise DropItem(f"Duplicate item found: {item!r}")
        else:
            self.ids_seen.add(adapter["id"])
            return item
```

基于`redis`进行缓存去重的增强版本

```python
class RedisDeduplicatorPipeline:
    """
    使用redis实现重复数据删除逻辑。
    """
    def __init__(self):
        self.is_restart = True
        self.redis_con = None
        self.pipe = None

    @classmethod
    def from_crawler(cls, crawler):
        cls.DB = crawler.settings.get("REDIS_DB")
        cls.HOST = crawler.settings.get("REDIS_HOST")
        cls.PASSWD = crawler.settings.get("REDIS_PASS")
        cls.PORT = crawler.settings.get("REDIS_PORT")
        return cls()

    def open_spider(self, spider):
        self.redis_key = '{}:item_duplicate'.format(spider.name)
        if spider.name == '****':
            self.redis_con = redis.Redis(host=self.HOST, port=self.PORT, db=self.DB, password=self.PASSWD)
            if self.is_restart:
                self.redis_con.delete(self.redis_key)
            self.pipe = self.redis_con.pipeline()

    def process_item(self, item, spider):
        if spider.name == 'tx_work_info':
            # 将传递过来的item数据转为字符串并加密成md5数据
            item_str = json.dumps(item)
            md5_hash = hashlib.md5()
            md5_hash.update(item_str.encode())
            hash_value = md5_hash.hexdigest()
            # redis_con.sadd(redis_key, str(info))
            if self.redis_con.sadd(self.redis_key, hash_value):
                # 判断hash值是否存在于redis中,如果不存在则将hash保存到redis中
                # 如果存在则抛出异常停止管道传递数据
                raise DropItem("Duplicate item found: %s" % item_str)
            # 将数据返给下一个pipline处理
            return item

    def close_spider(self, spider):
        if spider.name == '****':
            self.redis_con.close()
```

#### （3）数据导出为`json/text/csv/excel`文件

- 数据导出为excel

```python

import os
from openpyxl import Workbook
from scrapy.exceptions import DropItem
from openpyxl.utils import get_column_letter


class ExcelFormatPipeline:
    def __init__(self):
        self.filepath = None
        self.workbook = None
        self.sheet = None
        self.columns = None

    @classmethod
    def from_crawler(cls, crawler):
        cls.save_dir = crawler.settings.get("SAVE_DIR")
        return cls()

    def open_spider(self, spider):
        self.filepath = os.path.join(self.save_dir, f'{spider.name}.xlsx')
        self.workbook = Workbook()
        self.sheet = self.workbook.active
        self.sheet.title = spider.name

    def close_spider(self, spider):
        # 在第一行插入空行
        self.sheet.insert_rows(0)
        # 插入表头
        for index, key in enumerate(self.columns, start=1):
            self.sheet[f"{get_column_letter(index)}1"] = key
        self.workbook.save(self.filepath)

    def process_item(self, item, spider):
        if self.columns is None:
            self.columns = item.keys()
        else:
            if self.columns!=item.keys():
                # 注意，如果第一个错了，后续的都错了！
                raise DropItem('An exception occurred due to a key'
                               ' mismatch with the previously saved '
                               'keys, ignoring the entry...')
        if self.workbook and self.sheet:
            self.sheet.append(list(item.values()))
        return item
```

- 导出为json

```python

import json
import os


class JsonFormatPipeline:

    def __init__(self):
        self.filepath = None
        self.file = None

    @classmethod
    def from_crawler(cls, crawler):
        cls.save_dir = crawler.settings.get("SAVE_DIR")
        cls.encoding = crawler.settings.get("FEED_EXPORT_ENCODING")
        return cls()

    def open_spider(self, spider):
        self.filepath = os.path.join(self.save_dir, f'{spider.name}.json')
        self.file = open(self.filepath, 'a', encoding='utf-8')
        self.file.write("[\n")


    def close_spider(self, spider):
        self.file.write("\n]")
        self.file.close()


    def process_item(self, item, spider):
        if not self.file.closed:
            json.dump(dict(item), self.file, ensure_ascii=False)
            self.file.write(",\n")
        return item

```

- 导出为`txt`

```python
import os
from scrapy.exceptions import DropItem


class TxtFormatPipeline:

    def __init__(self):
        self.filepath = None
        self.file = None
        self.columns = None

    @classmethod
    def from_crawler(cls, crawler):
        cls.save_dir = crawler.settings.get("SAVE_DIR")
        cls.sep = crawler.settings.get("EXPORT_ENCODING_SEP")
        cls.encoding = crawler.settings.get("EXPORT_ENCODING")
        return cls()

    def open_spider(self, spider):
        self.filepath = os.path.join(self.save_dir, f'{spider.name}.txt')
        self.file = open(self.filepath, 'w', encoding=self.encoding)

    def close_spider(self, spider):
        # 指针移动至开头
        self.file.seek(0)
        # 增加新行
        self.file.write(self.sep.join(self.columns)+'\n')
        self.file.close()

    def process_item(self, item, spider):
        if self.columns is None:
            self.columns = item.keys()
        else:
            if self.columns!=item.keys():
                # 注意，如果第一个错了，后续的都错了！
                raise DropItem('An exception occurred due to a key'
                               ' mismatch with the previously saved '
                               'keys, ignoring the entry...')
        if not self.file.closed:
            self.file.write(self.sep.join(item.values())+'\n')
        return item
```

- 导出为`csv`

```python
import csv
import os
from scrapy.exceptions import DropItem


class CSVFormatPipeline:

    def __init__(self):
        self.filepath = None
        self.file = None
        self.writer = None
        self.columns = None

    @classmethod
    def from_crawler(cls, crawler):
        cls.save_dir = crawler.settings.get("SAVE_DIR")
        cls.sep = crawler.settings.get("EXPORT_ENCODING_SEP")
        cls.encoding = crawler.settings.get("EXPORT_ENCODING")
        return cls()

    def open_spider(self, spider):
        self.filepath = os.path.join(self.save_dir, f'{spider.name}.csv')
        self.file = open(self.filepath, 'w', newline='', encoding=self.encoding)
        self.writer = csv.writer(self.file, delimiter=self.sep)

    def close_spider(self, spider):
        # 指针移动至开头
        self.file.seek(0)
        self.writer.writerow(list(self.columns))
        self.file.close()

    def process_item(self, item, spider):
        if self.columns is None:
            self.columns = item.keys()
        else:
            if self.columns!=item.keys():
                # 注意，如果第一个错了，后续的都错了！
                raise DropItem('An exception occurred due to a key'
                               ' mismatch with the previously saved '
                               'keys, ignoring the entry...')
        if not self.file.closed:
            self.writer.writerow(list(item.values()))
        return item
```

#### （4）将数据导出到`mysql`数据库

```python
class MySQLTwistedPipeline(object):
    def __init__(self):
        self.insert_sql=''

    @classmethod
    def from_crawler(cls, crawler):
        cls.MYSQL_DB_NAME = crawler.settings.get("MYSQL_DB")
        cls.HOST = crawler.settings.get("MYSQL_HOST")
        cls.USER = crawler.settings.get("MYSQL_USER")
        cls.PASSWD = crawler.settings.get("MYSQL_PASSWD")
        cls.PORT = crawler.settings.get("MYSQL_PORT")
        return cls()

    def open_spider(self, spider):
        # 创建一个线程池对象
        self.pool = adbapi.ConnectionPool(
            'pymysql',
            host=self.HOST,
            port=self.PORT,
            passwd=self.PASSWD,
            user=self.USER,
            db=self.MYSQL_DB_NAME
        )

    def process_item(self, item, spider):
        # 在process_item()方法中使用数据库连接池对象进行数据库操作，自动传递cursor对象到数据库操作方法
        # runInteraction()的第一个参数（自定义方法）
        query = self.pool.runInteraction(self.insert, item)
        # 设置出错时的回调方法，自动传递出错消息对象error到错误处理方法的第一个参数（自定义方法）
        query.addErrback(self.handle_error, item)
        return item

    def handle_error(self, error, item):
        logging.error(f'插入数据失败 原因：{error} 错误对象：{item}')

    def insert(self, cursor, item):
        conn = cursor.connection
        try:
            conn.ping()
        except:
            self.pool.close()
            self.pool =self.pool = adbapi.ConnectionPool(
                'pymysql',
                host=self.HOST,
                port=self.PORT,
                passwd=self.PASSWD,
                user=self.USER,
                db=self.MYSQL_DB_NAME
            )
        try:
            cursor.execute(self.insert_sql.format(**item))
            logging.info(item)
        except Exception as e:
            logging.info("插入数据库失败 {0} {1}".format(
                e, 
                self.insert_sql.format(**item))
            )
```

#### （5）将数据导出到`MongoDB`数据库

```python
import pymongo
from itemadapter import ItemAdapter

class MongoPipeline:

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())
        return item
```

#### （6）连接`Elasticsearch`

```python
from elasticsearch import Elasticsearch


class ElasticPipeline(object):
    def __init__(self):
        self.conn =None
    
    @classmethod
    def from_crawler(cls, crawler):
        cls.connection_string = crawler.settings.get('ELASTICSEARCH_CONNECTION_STRING')
        cls.index = crawler.settings.get('ELASTICSEARCH_INDEX')
        return cls
    
    def open_spider(self, spider):
        self.conn = Elasticsearch([self.connection_string])
        if not self.conn.indices.exists(self.index):
            self.conn.indices.create(index=self.index)
    
    def close_spider(self, spider):
        self.conn.transport.close()
    
    def process_item(self, item, spider):
        # 调用index 方法 对数据进行索引：
        # index 代表索引名称；body代表数据对象；id为索引数据的id
        self.conn.index(index=self.index,body=dict(item),id=hash(item['name']))
        return item
```

#### （7）项目截图

```python
import hashlib
from urllib.parse import quote

import scrapy
from itemadapter import ItemAdapter
from scrapy.utils.defer import maybe_deferred_to_future


class ScreenshotPipeline:
    """一个基于Splash渲染页面并进行截图操作的Pipeline类"""
    
    SPLASH_URL = "http://localhost:8050/render.png?url={}"
    
    async def process_item(self, item, spider):
        # 协程语法
        adapter = ItemAdapter(item)
        encoded_item_url = quote(adapter["url"])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url)
        response = await maybe_deferred_to_future(spider.crawler.engine.download(request, spider))

        if response.status != 200:
            # Error happened, return item.
            return item

        # 将截图结果保存到文件，文件名称一般是url的hash值
        url = adapter["url"]
        url_hash = hashlib.md5(url.encode("utf8")).hexdigest()
        filename = f"{url_hash}.png"
        with open(filename, "wb") as f:
            f.write(response.body)
        # 将文件名称保存在item中.
        adapter["screenshot_filename"] = filename
        return item
```

### 3.管道扩展：文件下载和图片存储

`Scrapy`提供了内置的可重用的`item pipelines`为某个特定的`Item`去下载文件，包括`Files Pipeline`或`Images Pipeline`。这两个管道都实现了：

- 避免重复下载
- 指定下载后保存的地方

此外，`Images Pipeline`额外实现了：

- 将所有下载的图片格式转换成普通的JPG并使用RGB颜色模式
- 生成缩略图
- 检查图片的宽度和高度确保它们满足最小的尺寸限制

`scrapy`增加了对 文件系统的支持，如需要，可进一步阅读[支持文件系统](https://doc.scrapy.org/en/1.0/topics/media-pipeline.html#supported-storage)文档。

#### （1）`FilesPipeline`基本使用

`FilesPipeline`典型工作流程大概是，首先在`spider`爬取的`item`字段中直接增加`file_urls`和`file`字段，包含如上属性的`item`将会被`FilesPipeline`解析出来并使用标准的`Scrapy`调度器发送请求，在文件下载完毕之前，`item`在特定的管道阶段保持“锁定”状态，直到文件下载完成（或因某种原因失败）。

下载完成后，`FilesPipeline`将会返回一个下载是否成功的信号和一个包含下载文件信息的字典，并将下载文件的信息添加到`item`的`file`字段，例如下载路径、原始抓取url、文件校验和和文件状态。

`FilesPipeline`启用也需要在设置中启用该管道：

```python
ITEM_PIPELINES = {
    'scrapy.pipelines.files.FilesPipeline': 2
}
# 文件存储路径
FILES_STORE = '/path/to/valid/dir' 
# 文件过期时间
FILES_EXPIRES = 90

# 是否允许媒体下载请求的重定向
MEDIA_ALLOW_REDIRECTS = True
```

#### （2）`Images Pipeline`基本使用

`ImagesPipeline`使用和`FilesPipeline`基本一致，其区别在于`item`应增加`image_urls`和`image`字段，另外`ImagesPipeline`还可以通过配置`settings`增加额外功能，如基于`Pillow`生成生成缩略图、转化图像格式和图像大小过滤图像等。

`FilesPipeline`启用也需要在设置中启用该管道：

```python
# 图片存储路径
IMAGES_STORE = '/path/to/valid/dir' 
# 过期时间 30 天
IMAGES_EXPIRES = 30

# 图片缩略图配置
IMAGES_THUMBS = {
    'small': (50, 50),
    'big': (270, 270),
}
# 图片过滤器，最小高度和宽度
IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110

# 如果以上设置了两个，则需要存储的图片二者都需要满足，否则只满足其一即可。 
# 是否允许媒体下载请求的重定向
MEDIA_ALLOW_REDIRECTS = True
```

> 文件存储路径为`<IMAGES_STORE>/full/<FILE_NAME>`，这里的`<IMAGES_STORE>`直接等价于`IMAGES_STORE`或`FILES_STORE`，`<FILE_NAME>`则来自于函数`file_path`的返回结果，如果在设置中启用`IMAGES_THUMBS`，则将会将图片存储在地址`<IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg`中。

#### （3）媒体管道继承拓展

如果还需要更加复杂的功能，想自定义下载其他媒体逻辑，请参考[扩展媒体管道](http://doc.scrapy.org/en/1.0/topics/media-pipeline.html#topics-media-pipeline-override)，不管是扩展`FilesPipeline`还是`ImagesPipeline`，一般只需重写如下三个方法。

- 文件名称定义

默认情况下，该函数将返回基于文件内容进行`sha-1()`哈希结果。

```python
import hashlib
from pathlib import PurePosixPath

def file_path(self, request, response=None, info=None, *, item=None):
    """该方法负责处理生成file/img 对象的 name,可以通过重写该方法 实现自定义文件名"""
    image_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)
    image_perspective = request.url.split("/")[-2]
    image_filename =  PurePosixPath(f"{image_url_hash}_{image_perspective}.jpg").name 
    return f"full/" + image_filename
```

> 自定义媒体名称应考虑文件名称是否包含`/`及网站文件名的动态变化等问题。

- 自定义请求生成

```python
from itemadapter import ItemAdapter

def get_media_requests(item, info):
    """
    将管道从项目中获取到的文件URL转化为Request，当完成下载后，
    结果将作为2元元组列表 [(success，file_info_or_error),...]，
    发送到item_completed()
    """
    adapter = ItemAdapter(item)
    for file_url in adapter["file_urls"]:
        yield scrapy.Request(file_url)
```

`get_media_requests`发出的特殊请求在文件下载完毕后，其结果将会作为一个包含二元组的列表返回到`item_completed`，返回信息中大概包含如下信息：

```python
[
	    (
	        True,
	        {
	            "checksum": "2b00042f7481c7b056c4b410d28f33cf",  # 为图片或文件内容的 MD5 hash-1 结果，将作为存储文件名。
	            "path": "full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg", # `path`为保存路径
	            "url": "http://www.example.com/files/product1.pdf",  # 请求url
	            "status": "downloaded",  # 文件状态指示，可选 ['downloaded','uptodate','cached']
                # 分别代表 文件下载成功， 文件下载失败，文件已经被下载过了。
	        },
	    ),
	    (False, Failure(...)),
	]
```

- `item_completed`函数

```python
from scrapy.exceptions import DropItem


def item_completed(self, results, item, info):
    """
    类似于正常管道中的process_item方法，必须向其一样返回（或删除）该项目。
    """
    file_paths = [x['path'] for ok, x in results if ok]
    if not file_paths:
        raise DropItem("Item contains no files")
    item['file_paths'] = file_paths
    return item
```

- 继承扩展` Images pipeline`案例

```python
import scrapy
from scrapy.pipelines.images import ImagesPipeline
from scrapy.exceptions import DropItem

class MyImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        for image_url in item['image_urls']:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem("Item contains no images")
        item['image_paths'] = image_paths
        return item
```

#### （4）使用案例

```python
class ImagesPipeline(ImagesPipeline):
    IMAGES_STORE = get_project_settings().get("IMAGES_STORE")

    def get_media_requests(self, item, info):
        image_url = item["imagesUrls"]
        yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        # 固定写法，获取图片路径，同时判断这个路径是否正确，如果正确，就放到 image_path里，ImagesPipeline源码剖析可见
        image_path = [x["path"] for ok, x in results if ok]

        os.rename(self.IMAGES_STORE + "/" + image_path[0], self.IMAGES_STORE + "/" + item["name"] + ".jpg")
        item["imagesPath"] = self.IMAGES_STORE + "/" + item["name"]

        return item
```

## 二、对接驱动浏览器



## 三、`Item`导出

### 1.feed导出

`Feed`导出提供了许多内置功能，允许将爬取的结果快速导出为多种格式的文件中，例如`JSON`、`CSV`、`XML` 等，此功能通过设置`Scrapy ` 项目的 `FEEDS` 配置来实现，允许指定多个导出目标、格式以及是否要覆盖原始文件等选项。

#### （1）导出到文件配置

在 `settings.py` 文件中，可以像下方配置 `FEEDS` 以实现数据的自动导出，进一步执行如下命令即可按照要求导出数据：

```shell
scrapy crawl myspider -o items.json
```

其他导出格式：

| 格式         | `format`    | `itemExporter`          | 备注                                                     |
| ------------ | ----------- | ----------------------- | -------------------------------------------------------- |
| `JSON`       | `json`      | `JsonItemExporter`      |                                                          |
| `JSON lines` | `jsonlines` | `JsonLinesItemExporter` |                                                          |
| `CSV`        | `csv`       | `CsvItemExporter`       | 需要使用`FEED_EXPORT_FIELDS`指定要导出的列名称及其顺序。 |
| `XML`        | `xml`       | `XmlItemExporter`       |                                                          |
| `Pickle`     | `pickle`    | `PickleItemExporter`    |                                                          |
| `Marshal`    | `marshal`   | `MarshalItemExporter`   |                                                          |

#### （2）导出到存储后端

- `scrapy`支持将数据直接导出到`Storages backend `，大概支持如下文件系统：

| 文件系统                                                     | `scheme` | `Example URI`                                                |
| ------------------------------------------------------------ | -------- | ------------------------------------------------------------ |
| [Local filesystem](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-storage-fs) | `file`   | `file:///tmp/export.csv`                                     |
| [FTP](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-storage-ftp) | `ftp`    | `ftp://user:pass@ftp.example.com/path/to/export.csv`         |
| [Amazon S3](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-storage-s3) | `s3`     | `s3://mybucket/path/to/export.csv`<br>`s3://aws_key:aws_secret@mybucket/path/to/export.csv` |
| [Google Cloud Storage (GCS)](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-storage-gcs) | `gs`     | `gs://mybucket/path/to/export.csv`                           |
| [Standard output](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-storage-stdout) | `stdout` | `stdout:`                                                    |

> 部分文件系统的存储需要用到一些设置参数，具体可点击文件名参考官方文档。部分文件系统需要第三方模块，如`s3`需要[boto3](https://github.com/boto/boto3)模块，`Google Cloud Storage (GCS)`需要[google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python)模块。

- `storage URI `存在一些`URI`参数：

| 参数           | 说明                                       |
| -------------- | ------------------------------------------ |
| `%(time)s`     | 使用`feed`创建时的时间戳替换该参数         |
| `%(name)s`     | 使用`spider-name`替换该参数                |
| `%(site_id)s ` | 使用爬虫类属性`spider.site_id`替换该参数； |

```python
# example
# 存储在FTP
ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json
# 寸尺在s3
s3://mybucket/scraping/feeds/%(name)s/%(time)s.json
```

#### （3）条目过滤

可以使用`ItemFilterClass`定义一些规则来决定是否将条目导出，可以通过重写`ItemFilter`类来自定义筛选规则，可以在该类中修改`feeds_option`来实现。

```python
class scrapy.extensions.feedexport.ItemFilter(object):
    """FeedExporter 使用它来决定是否将某个项目导出到feed。"""
    def __init__(self, feed_options):
        # feed_options应该是一个类似于上文中 FEEDS 的 字典，用于传递一些特殊选项。
        self.feed_options = feed_options

    def accepts(self, item):
        """如果某项应该导出，则返回 True，否则返回 False。"""
        if "field1" in item and item["field1"] == "expected_data":
            return True
        return False
```

#### （4）后期处理

`Scrapy`内置了一些在导出之前先对数据进行处理的插件类，下面将一一介绍，也可以自定义插件类。

这些插件类都封装在模块`scrapy.extensions.postprocessing`中。

| 插件名称                                                     | 说明                   | `feed_options`                                          |
| ------------------------------------------------------------ | ---------------------- | ------------------------------------------------------- |
| [GzipPlugin](https://docs.python.org/3/library/gzip.html#gzip.GzipFile) | 使用 gzip 压缩数据     | `gzip_compresslevel`/`gzip_mtime`/`gzip_filename`       |
| [LZMAPlugin](https://docs.python.org/3/library/lzma.html#lzma.LZMAFile) | 使用 lzma 压缩的数据。 | `lzma_format`/`lzma_check`/`lzma_preset`/`lzma_filters` |
| [Bz2Plugin](https://docs.python.org/3/library/bz2.html#bz2.BZ2File) | 使用 bz2压缩数据。     | `bz2_compresslevel`                                     |

自定义插件：每个自定义插件必须实现如下方法：

```python
class Custom Plugins:
    def __init__(self, file, feed_options):
        """使用 file 和 feed_options 初始化插件对象。"""
        # 不要再此函数内关闭文件
        pass
    
    def write(self, data):
        """处理和写入数据(字节或内存视图)到插件的目标文件。它必须返回写入的字节数。"""
        pass
    
    def close(self):
        """清理插件：例如关闭打开的 用于压缩数据的类。"""
        pass
```

> 这些插件可以通过`feed_options`来激活和使用，

#### （5）`feed`配置

- `FEEDS`参数

一个字典，其中每个键都是路径字符串(或 `pathlib`对象)，每个值都应该是包含特定提要的配置参数的嵌套字典

```python
FEEDS = {
    # items.json 应该是一个文件路径或者一个pathlib.Path('items.csv.gz')对象
    'items.json': {  # 文件名称
        'format': 'json',  # 格式
        'encoding': 'utf8',  # 编码
        'store_empty': False,  # 
        'item_classes': [MyItemClass1, 'myproject.items.MyItemClass2'],
        'fields': ['field1', 'field2'],  # 只导出 field1 和 field2 字段
        'indent': 4, 
        'item_export_kwargs': {
           'export_empty_fields': True,
        },
    },
    'items.csv': {
        'format': 'csv',
        'item_filter': MyCustomFilter1,  # itemFilter类
        'fields': ['field1', 'field2'],
        
    },
    pathlib.Path('items.csv.gz'):{
        format': 'csv',
        'fields': ['price', 'name'],
        'item_filter': 'myproject.filters.MyCustomFilter2',  # itemFilter类
        # 用于后期处理的插件列表。
        'postprocessing': [MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],
        'gzip_compresslevel': 5,
        'overwrite':True, # ,当导入到文件系统时，
        
    }
}
```

| 可接受的键           | 说明                                                         |
| -------------------- | ------------------------------------------------------------ |
| `format`             | 上文中的**导出格式**。                                       |
| `batch_item_count`   | 文件传递延迟，参考`FEED_EXPORT_BATCH_ITEM_COUNT`。           |
| `encoding`           | 文件导出编码，参考`FEED_EXPORT_ENCODING`。                   |
| `fields`             | 只导出` field1` 和` field2` 字段，参考`FEED_EXPORT_FIELDS`。 |
| `item_classes`       | 需要导出的`Item`类列表，未定义或为空，则导出所有Item。       |
| `Item_filter`        | 对应上文中的过滤器类`ItemFilter`，默认使用` ItemFilter`。    |
| `indent`             | `json`或`lxm`使用的格式化缩进字符，参考`FEED_EXPORT_INDENT`。 |
| `store_empty`        | 是否导出为空的`feed`，参考`FEED_STORE_EMPTY`。               |
| `uri_params`         | 设置应用于 `URI` 的` printf `样式的参数，参考`FEED_URI_PARAMS`。 |
| `postprocessing`     | 使用的`post-processing`插件列表，将按照列表顺序调用。        |
| `Item_export_kwargs` | `ItemExporter`类可能使用到的参数。                           |
| `overwrite`          | 如果文件存在是重写(`w`)还是追加(`a`)，不同的文件系统该项目默认值不同。其中`Local filesystem`和`Standard output`为`False`，其余为`True`。 |

> 没有定义的键将使用`settings`的值作为默认值。

- `FEED_EXPORT_ENCODING`：用于`feed`导出数据的编码，默认为`utf-8`。

- `FEED_STORE_EMPTY`：是否导出为空的`feed`，默认为`True`，即使`feed`中没有数据，也会创建文件。

- `FEED_EXPORT_FIELDS`：设置需要导出的字段，默认为`None`，对于结构化数据而言，有时候标签是必须的。

- `FEED_EXPORT_INDENT`：用于`json`和`xml`文件的级别缩进，默认为`0`，若该值为负，等价于`0`。

- `FEED_STORAGES`：默认为`{}`，包含项目支持的所有文件系统的`dict`，键是`URI`，值是存储类的路径。

- `FEED_STORAGE_FTP_ACTIVE`：默认为`False`，数据导出到`FTP`服务时是使用主动连接模式(`True`)还是被动连接模式(`False`)，具体可参考[FTP 连接模式](https://stackoverflow.com/a/1699163)。

- `FEED_STORAGE_S3_ACL`：包含由项目导出到 AmazonS3的自定义`ACL`的字符串，具体参考[Canned ACL](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl)。

- `FEED_EXPORTERS`：包含`Item`支持的导出信息的字典，其中键为序列化`format`，值为导出器类的路径。

- `FEED_STORAGES_BASE`：包含`feed`支持的内置文件系统的`dict`，可以在`FEED_EXPORTERS`中将其值设置为`None`来禁用设置。

  ```python
  # 默认
  {
      "": "scrapy.extensions.feedexport.FileFeedStorage",
      "file": "scrapy.extensions.feedexport.FileFeedStorage",
      "stdout": "scrapy.extensions.feedexport.StdoutFeedStorage",
      "s3": "scrapy.extensions.feedexport.S3FeedStorage",
      "ftp": "scrapy.extensions.feedexport.FTPFeedStorage",
  }
  ```

- `FEED_EXPORT_BATCH_ITEM_COUNT`：存储后端不会再下载`Item`时直接将数据上传到指定的`URI`，而是先将`Item`写入一个临时的本地文件，当所有条目爬取完毕时，才开始导入数据。可使用该参数将输出项拆分到多个文件中，并为每个文件指定最大项目计数，一旦达到该数，该文件按就会被导入到文件系统中。

- `FEED_URI_PARAMS`：包含函数导入路径的字符串，用于设置应用于 `URI` 的` printf `样式的参数，具体可参考`scrapy.extensions.feedexport.uri_params`。

导出到后端文件系统等：https://docs.scrapy.org/en/latest/topics/feed-exports.html#storages

### 2.ItemExport类

`feed`导出不同类型的数据本质上是使用了不同的`itemExporter`类，这里可以进一步了解`itemExporter`类的工作原理以及自定义项目导出器类。

#### （1）基本使用

要使用`ItemExport`程序，首先根据其需要使用的参数将其实例化（需要不同的参数），其次调用`start_exporting()`发出数据导出信号，再其次对要导出的每个`Item`调用`export_item()`，最后，调用`finish_exporting()`方法表示导出过程的结束。

```python
from itemadapter import ItemAdapter
from scrapy.exporters import XmlItemExporter


class PerYearXmlExportPipeline:
    
    """根据“年份”字段跨多个 XML导出"""
    def open_spider(self, spider):
        self.year_to_exporter = {}

    def close_spider(self, spider):
        for exporter, xml_file in self.year_to_exporter.values():
            exporter.finish_exporting()
            xml_file.close()

    def _exporter_for_item(self, item):
        adapter = ItemAdapter(item)
        year = adapter["year"]
        if year not in self.year_to_exporter:
            xml_file = open(f"{year}.xml", "wb")
            exporter = XmlItemExporter(xml_file)
            exporter.start_exporting()
            self.year_to_exporter[year] = (exporter, xml_file)
        return self.year_to_exporter[year][0]

    def process_item(self, item, spider):
        exporter = self._exporter_for_item(item)
        exporter.export_item(item)
        return item
```

#### （2）内置的Item Exporters

内置的`ItemExporter`应该都位于`scrapy.exporters.`文件中，可以在使用时按需调用。

- 基类：`BaseItemExporter`

所有`Item Exporter`的抽象基类，支持所有(具体的)项导出程序使用的公共特性，比如定义导出什么字段，是否导出空字段，或者使用哪种编码。

```python
class BaseItemExporter:
    
    def __init__(self, *, dont_fail=False, **kwargs):
        # 定义导出字段、编码、是否导出空字段，缩进等
        self._kwargs = kwargs
        self._configure(kwargs, dont_fail=dont_fail)
        
    def _configure(self, options, dont_fail=False):
        """与init协同，利用“options”字典配置导出器，dont_fail表示不对意外选项引发异常。"""
        # 输出字符编码。
        self.encoding = options.pop("encoding", None)
        # None 、fields List 或 传入一个 keys 为 field values 为 输出fields值
        self.fields_to_export = options.pop("fields_to_export", None)
        # 是否在导出的数据中包含空/未填充的项目字段，默认为False。
        self.export_empty_fields = options.pop("export_empty_fields", False)
        # 参考 indent 设置
        self.indent = options.pop("indent", None)
        if not dont_fail and options:
            raise TypeError(f"Unexpected options: {', '.join(options.keys())}")
   	
    def export_item(self, item):
        """导出主函数，该方法必须在子类中实现"""
        raise NotImplementedError
    
    def serialize_field(self, field, name, value):
        """返回给定字段的序列化值, 如果要控制特定字段或值的序列化/导出方式，可以重写此方法，默认情况下，该方法会首先调用`Item`中定义的序列化程序。
        field 应该是 Field 对象 或 dict.
        """
        serializer = field.get("serializer", lambda x: x)
        return serializer(value)
    
    def start_exporting(self):
        pass

	def finish_exporting(self):
        pass
```

- [PythonItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#PythonItemExporter)

```python
class PythonItemExporter(BaseItemExporter):
    """将Item序列化为内置的Python类型，可以在其上使用任何序列化库（例如json或msgpack）."""
	pass
```

- [XmlItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#XmlItemExporter)

```python
class XmlItemExporter(BaseItemExporter):
    """将XML格式的项目导出到指定的文件对象。"""
    def __init__(self, file, **kwargs):
        # file:用于导出数据的类文件对象，其write 方法应该以 b模式打开
        # 导出XML中每个项目元素的名称。
        self.item_element = kwargs.pop("item_element", "item")
        # 导出XML中根元素的名称。
        self.root_element = kwargs.pop("root_element", "items")
        super().__init__(**kwargs)
		...
```

- [CsvItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#CsvItemExporter)

```python
class CsvItemExporter(BaseItemExporter):
    def __init__(
        self,
        file,
        include_headers_line=True,
        join_multivalued=",",
        errors=None,
        **kwargs,
    ):
        ...
        # file:用于导出数据的类文件对象，其write 方法应该以 b模式打开
        # 应传入一个字符串，用于指定csv 的标题项
        self.include_headers_line = include_headers_line
        self._join_multivalued = join_multivalued
        # join_multivalued: 用于连接多值字段的一个或多个字符。
        # errors（str）–指定如何处理编码和解码错误的可选字符串。
```

- [PickleItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#PickleItemExporter)

```python
class PickleItemExporter(BaseItemExporter):
    def __init__(self, file, protocol=4, **kwargs):
        super().__init__(**kwargs)
        self.file = file
        # (int) - pickle protocol 
        self.protocol = protocol
```

- [PprintItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#PprintItemExporter)

```python
class PprintItemExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        super().__init__(**kwargs)
        self.file = file
```

- [JsonItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#JsonItemExporter)

```python
class JsonItemExporter(BaseItemExporter):
    """将JSON格式的项目导出到指定的类文件对象，将所有对象写入对象列表。"""
    def __init__(self, file, **kwargs):
        super().__init__(dont_fail=True, **kwargs)
        self.file = file
        json_indent = (
            self.indent if self.indent is not None and self.indent > 0 else None
        )
        self._kwargs.setdefault("indent", json_indent)
        self._kwargs.setdefault("ensure_ascii", not self.encoding)
        self.encoder = ScrapyJSONEncoder(**self._kwargs)
        self.first_item = True
```

> `JSON`是一种非常简单灵活的序列化格式，但它不能很好地扩展到大量数据，因为`JSON`解析器（在任何语言上）都不支持增量（又名流模式）解析（如果有的话），而且它们中的大多数只是解析内存中的整个对象。

- [JsonLinesItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#JsonLinesItemExporter)

```python
class JsonLinesItemExporter(BaseItemExporter):
    """将JSON格式的项目导出到指定的类文件对象，每行写入一个JSON编码的项目。"""
    def __init__(self, file, **kwargs):
        super().__init__(dont_fail=True, **kwargs)
        self.file = file
        self._kwargs.setdefault("ensure_ascii", not self.encoding)
        self.encoder = ScrapyJSONEncoder(**self._kwargs)
```

> 与`JsonItemExporter`生成的格式不同，此导出器生成的格式非常适合序列化大量数据。

- [MarshalItemExporter](https://docs.scrapy.org/en/latest/_modules/scrapy/exporters.html#MarshalItemExporter)

```python
class MarshalItemExporter(BaseItemExporter):
	"""以Python特定的二进制格式导出项目。"""
    def __init__(self, file, **kwargs):
        super().__init__(**kwargs)
        self.file = file
```

#### （3）`Item`字段格式化

- 在`Item`中定义字段的格式化

```python
import scrapy


def serialize_price(value):
    return f"$ {str(value)}"


class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field(serializer=serialize_price)
```

- 重写`serialize_field()`方法

仅适用于导出数据的格式化。

```python
from scrapy.exporters import XmlItemExporter


class ProductXmlExporter(XmlItemExporter):
    def serialize_field(self, field, name, value):
        if name == "price":
            return f"$ {str(value)}"
        return super().serialize_field(field, name, value)
```

