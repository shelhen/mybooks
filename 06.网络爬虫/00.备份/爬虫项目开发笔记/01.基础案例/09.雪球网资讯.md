## 09.雪球网资讯

### 一、需求分析

爬取雪球网中的资讯数据，即爬取热帖内容

地址：https://xueqiu.com/

#### 1.抓包分析

分析发现帖子的内容是通过ajax动态加载出来的，因此通过抓包工具，定位到ajax请求的数据包，从数据包中提取

接口api:https://xueqiu.com/statuses/hot/listV2.json

请求方式：get

请求参数：

```
since_id=-1
max_id=311519
size=15
```

请求时携带了cookie，最好用session存储cookie

#### 2.爬虫设计

### 二、具体实现

```python
import requests
import json
import random
import time


class XueSpider(object):
    def __init__(self):
        self.session = requests.session()
        self.headers = {
            "Accept":"application/json, text/plain, */*",
            "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        }
        self.base_url ="https://xueqiu.com/statuses/hot/listV2.json"

    def get_xhr(self, max_id):
        params={ "since_id": "-1", "max_id": max_id, "size":"15"}
        response = self.session.get(url=self.base_url, params=params, timeout=10, headers=self.headers)
        return response.json()

    def parse(self, datas):
        return [{
            'id':item['id'],
            'description':item['original_status']['description'],
            'title':item['original_status']['title']
        } for item in datas['items']]

    def main(self):
        path = r"./datas/雪球资讯.json"
        max_id = 517045
        results = []
        self.session.get('https://xueqiu.com/', headers=self.headers, timeout=10)
        for i in range(20):
            time.sleep(3 + 5*random.random())
            max_id = max_id-15
            datas = self.get_xhr(max_id)
            result = self.parse(datas)
            for r in result:
                print(r)
            results.extend(result)
        json.dump(results, open(path, 'w', encoding='utf-8'), indent=2, ensure_ascii=False)


if __name__ == '__main__':
    xs = XueSpider()
    xs.main()

```

