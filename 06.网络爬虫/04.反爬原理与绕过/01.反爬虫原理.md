# 01.反爬虫原理

## 一、反爬虫原因

> 限制爬虫程序访问服务器资源和获取数据的行为称为反爬虫。

网站开发的目的更多是是为了给用户使用而不是爬虫，对于企业自身而言，大量的自动化程序可以极大的降低人工成本，但是对于其他企业而言往往百害而无一利，带来极大的负面影响，因此大多数企业希望自身能最大限度的使用爬虫而又希望竞商尽量无法使用爬虫技术，因此相关企业做出了一些爬虫约定，但这些约定并没有法律效益，即使违反相关规定的网络爬虫也仅仅是涉及法律风险，状告爬虫胜诉的几率并不高，因此，企业需要自行开发反爬虫技术来提升网络安全方面。

> 对企业自身有利的爬虫，比如谷歌、百度等搜索引擎，企业希望自身获取更大的流量，更容易被其爬取。

### 1.反爬虫的目的

但鉴于`http`协议自身的局限性，爬虫目前无法被彻底禁止，爬虫本质上是模拟人类互联网行为的`web`程序，除非抛弃用户，否则不可能禁止爬虫。爬虫即无法从法律上禁止又无法从技术上禁止，因此只能增加一些反爬虫手段，增加爬虫开发的技术难度，根据摩尔定律，机器成本要小于人工成本，因此，通常服务器反爬就是让爬虫工程师加班增加竞争商的开发成本，用有限的投入机器成本换取友商更多投入的人工成本。

企业一般会选择对如下爬虫进行反爬：

| 初创公司       |      | 很多初创公司需要获取大量的数据，往往选择爬虫方式获取。       |
| -------------- | ---- | ------------------------------------------------------------ |
| 低级的应届生   |      | 爬虫通常简单粗暴，根本不管服务器压力，且人数不可预测，影响服务器性能。 |
| 失控/错误爬虫  |      | 有些爬虫无法拿到数据，但可以继续工作，影响服务器性能。       |
| 商业对手       |      | 商业对手为增强竞争能力，对友商实施各种自动化爬虫。           |
| 抽风的搜索引擎 |      | 开发的不好搜索引擎请求量相当于网络攻击，会极大的占用服务器资源，导致服务器性能下降。 |

### 2.反爬虫领域常见的概念

|  名词  | 概念                                                       |
| :----: | ---------------------------------------------------------- |
|  爬虫  | 使用任何技术手段，批量获取网站信息的一种方式。             |
| 反爬虫 | 使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。 |
|  误伤  | 在反爬虫的过程中，错误的将普通用户识别为爬虫。             |
|  拦截  | 成功地阻止爬虫访问。                                       |
|  资源  | 机器成本与人力成本的总和。                                 |

## 二、常见的反爬虫手段

爬虫本质上是模拟人类互联网行为的`web`程序，因此反爬虫的过程就是分析用户与爬虫程序特征差异，反爬虫技术的开发也往往是依赖某几项程序与人类的关键区别来实现。具体而言大致可以分为：信息校验型、行为识别型、数据混淆型、验证码校验型。其中主要的反爬手段是信息校验和行为识别，数据混淆和污染是加大解析难度，并不会减轻服务器负担（当然也有可能别人觉得解析不了就走了），验证码校验则是对反爬结果的进一步补充，可以用于避免误伤。无论如何，**实施反反爬虫的关键其实是分析如何才能更相似的模拟用户行为。**

### 1.信息特征反爬

信息校验中的“信息”指的是客户端发起网络请求中的一些特征，比如常说的请求头、请求正文等，“校验”指的是服务器端通过对信息的正确性、完整性或唯一性进行验证或判断，服务器后端程序可以通过对一些字段的检验来识别爬虫。

#### 1）`headers`字段

`http`协议要求`headers`的存在，并规定了一些基本的头域，这些头域字段都有可能被服务器用于判断是否为爬虫。

比如`requests`默认请求头中除了`User-Agent="python-requests/2.28.1"`外，还规定了`Accept`、`Accept-Encoding`、`Connection`等字段，如果不加修改，就很容易的被服务器检测出。有些网站图片中增加了防盗链，如果请求本网站的图片资源，需要在请求头中增加`referer`字段。有些`ajax`请求限制该请求只能包含`User-Agent`，若请求中携带了其他请求头则会被服务器限制。一些网站会在请求头域中增加`x-***-token`并对其参数进行加密来实施反爬。

```python
# 默认情况下requests发送的get请求携带的信息/下方是请求头
{
  "args": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.31.0", 
    "X-Amzn-Trace-Id": "Root=1-64b8e6e8-04bfd1a539e4247f352b7d81"
  }, 
  "origin": "58.218.24.160", 
  "url": "https://httpbin.org/get"
}
# 默认情况下requests发送的post请求携带的信息
{
  "args": {}, 
  "data": "", 
  "files": {}, 
  "form": {}, 
  "headers": {
    "Accept": "*/*", 
    "Accept-Encoding": "gzip, deflate", 
    "Content-Length": "0", 
    "Host": "httpbin.org", 
    "User-Agent": "python-requests/2.31.0", 
    "X-Amzn-Trace-Id": "Root=1-64b8e763-6ee20fc3143c0e8945249143"
  }, 
  "json": null, 
  "origin": "58.218.24.160", 
  "url": "https://httpbin.org/post"
}
```

请求头中有一个重要字段`cookie`，所有的发送给服务器`cookie`信息就是请求头中对应的`cookie`字符串，过期时间和域名信息只是存在于客户端的信息，不会发送给服务器。很多网站实施了`cookie`反爬，即在`cookie`参数中自定义一些加密的校验信息，该信息有的来自于响应头、有的来自于响应体，还有的来自于前端生成，具体可以参考**参数反爬内容**。

反爬网站：[企知道-请求头加密](https://www.qizhidao.com/check?searchKey=%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&tagNum=1&fromRoutePage=check)/[网上房地产-cookie验证](http://www.fangdi.com.cn/new_house/new_house_jjswlpgs.html)/[麻章区人民政府办公室-cookie验证](http://www.zjmazhang.gov.cn/hdjlpt/published?via=pc)

#### 2）请求参数

通常服务器可以通过检查请求参数是否正确来判断是否为爬虫，请求参数的获取方法有很多，由于请求分为`post`请求和`get`请求，其参数位置和格式规范都不相同。

比如说`get`请求的`url`在老版本`ie`中限制最大长度不超过`2048`字符，有些网站默认设置了当`get`请求参数过多导致`url`长度超出限制时，则会将请求自动转化为`post`，还有就是`get`请求的`url`转义协议有很多标准，不同的网站也许使用不同的标准，有些网站将空格转化为`80%`有些则转译为其他字符，具体而言还要具体分析。

常见的有：

- 请求参数置于响应`HTML`或`Json`文档中

  可以通过分析各个请求的顺序和联系来寻找，也可以借助浏览器搜索工具来寻找，这些参数可能经过加密和混淆，变的难以查找和还原。

  > 加密参数常位于`HTML`的`<input>`或`<script>`标签中，有的处于`cookie`中，也有会返回不符合规范的网页，该网页中存放加密参数。

- `js`动态生成请求参数

  逆向解析`js`加密的实现过程，通过`js2py`等库获取`js`的执行结果。

- 请求参数置于`cookie`中

  通过操作`cookie`，从中提取请求参数。

> 以上方法都可以使用`selenium`、`splash`等动态加载网页，构建服务返回加密参数。

请求参数加密示例网站：[极志愿](https://www.jizhy.com/44/rank/school)

#### （3）`websoceket`反爬

按照` WebSocket `规范，客户端生成握手信息并向服务器端发送握手请求，然后读取服务器端推送的消息，最后验证握手结果。但是`WebSocket `协议规范只作为参考，服务器端和客户端实际上可以不遵守这些约定。比如服务器端可以在校验握手信息时增加对客户端`User-Agent` 或 `Referer `的验证，如果客户端发送的握手请求中并没有对应的信息，则拒绝连接。或者在服务器端新增一个校验逻辑，握手结束后要求客户端发送特定的消息，服务器端对该消息进行校验，校验通过则将服务器端的数据推送给客户端，否则不作处理。在消息互发阶阶段实现对一些参数的校验。还有，服务器端可以向客户端发送 `Ping `帧，当客户端收到 `Ping`帧时应当回复` Pong` 帧。如果客户端不回复或者回复的并不是 `Pong` ，那么服务器端就可以认为客户端异常，主动关闭该连接。

#### （4）SSL指纹反爬



#### （5）浏览器指纹



### 2.行为特征反爬

除了请求信息特征以外，爬虫行为与人类行为特征，也有很大差异，另外爬虫开发人员在开发爬虫时也会进行调试和检查，代码调试的过程也是异常的行为，也可以被检查。

#### （1）请求频率或请求总量

爬虫的请求频率与请求次数一般要远高于普通用户。因此可以通过检测同一`ip/账号`单位时间内总请求数量进行反爬。也可以通过检测同一`ip/账号`一定时间内的请求间隔进行反爬。或者通过对请求`ip/账号`每天请求次数设置阈值进行反爬。

常用解决方法包括伪造`UA`、使用代理`IP`、伪造`cookie`来解决，也可以更友好设置请求之间的随机等待，设置账号的随机休眠。

#### （2）基于请求步骤或隐藏链接

爬虫一般不会复原全部的`http`请求，只会选择关键的请求来复现，如果某个请求需要前面的请求进行激活，未激活的请求则限制访问，则可以有效对爬虫进行反爬。也有的网站会采用 请求使用`js`构造连接、设置隐藏的蜜罐链接、混入垃圾链接（在新页面设置一个响应超级慢的响应占用爬虫进程和阻塞队列）等方式。

常用的解决方法包括在爬虫运行时或运行后及时监控，仔细分析爬虫得到的响应数据与实际页面中数据对应情况，如果存在问题/仔细分析请求、URL、响应时间及响应数据，根据服务器具体的规则实施反反爬。

> **蜜罐(陷阱)**：爬虫在根据正则、`xpath`、`css`选择器等方式提取后续链接时，提取到了陷阱`url`，该陷阱`URL`会被提取规则获取，但是正常用户无法获取。

#### （3）页面行为反爬

用户点击事件、鼠标轨迹、浏览速度等。

#### （4）反调试反爬



### 3.数据混淆反爬

爬虫拿到的数据一般是网站原始响应数据，浏览器拿到数据后一般还要经过后期的渲染处理，服务器可以通过对数据加密、数据偏移、图片转化、改变数据的响应规则等方式增大数据的解析难度。

#### （1）动态渲染反爬

一些响应数据有可能会被服务器直接写在`<script>`中或者来自于异步`AJAX`请求，后期经过`js`框架的处理后才正常显示在浏览器中。

如果是`ajax`响应，则可以设法模拟该请求获取相同的响应进一步解析数据，如果数据直接写入`js`代码中，一方面可以使用`re`正则进行匹配获取，令一方面可以使用`python`调用`js`引擎执行`js`代码获取，当然也可以用驱动浏览器。

#### （2）字体反爬

服务器可能对关键数据进行特殊化处理，比如将商品的价格信息统一转化为图片或`svg`格式显示，某些小说网站会对数据字体改变，使用多种或自定义字体，前端通过`css`偏移等方法显示字体。

常用的解决方法包括解析字体文件、还原`css`位移、编写提取`svg`程序、借助图片解析引擎（`ocr`）解析数据等。

案例网站:[雷速体育](https://data.leisu.com/zuqiu-8433)/[开源资讯](https://www.oschina.net/tweets)

#### （3）数据污染反爬

服务器可以在规则/特征的数据下植入一些伪造数据，或是按照一定的规则打乱数据的顺序使不同`IP/ID`下拿到的数据出现重复和混乱。

（4）编码加密反爬

服务器可以对数据源进行自定义编码加密方法、或是融合使用多种编码方式混合加密、或是使用一些小众编码进行数据编码提高数据解码难度。

响应数据验证：[数位观察](https://www.swguancha.com/home/ranking-list?code=td&tabName=city)/[小鸟报告](http://www.birdreport.cn/home/search/report.html?search=eyJ0YXhvbmlkIjoiIiwic3RhcnRUaW1lIjoiIiwiZW5kVGltZSI6IiIsInByb3ZpbmNlIjoi6Z2S5rW355yBIiwiY2l0eSI6IiIsImRpc3RyaWN0IjoiIiwicG9pbnRuYW1lIjoiIiwidXNlcm5hbWUiOiIiLCJzZXJpYWxfaWQiOiIiLCJjdGltZSI6IiIsInRheG9ubmFtZSI6IiIsInN0YXRlIjoiIiwibW9kZSI6IjAiLCJvdXRzaWRlX3R5cGUiOjB9)

### 4.验证码反爬

```python
1）字符验证码
2）计算验证码
3）滑动验证码
4）拼图验证码
5）文字点选验证码
6）其他web端验证码
7）微信公众号密钥
8）短信或邮箱验证码
```

